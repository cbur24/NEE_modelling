{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate and fit a ML model on the EC flux tower data \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shap\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import seaborn as sb\n",
    "from joblib import dump\n",
    "import multiprocessing\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV#, ShuffleSplit, KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "import sys\n",
    "sys.path.append('/g/data/os22/chad_tmp/dea-notebooks/Tools')\n",
    "from dea_tools.classification import spatial_clusters, SKCV\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'AUS'\n",
    "model_var = 'GPP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncpus=multiprocessing.cpu_count()\n",
    "print('ncpus = '+str(ncpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = '/g/data/os22/chad_tmp/NEE_modelling/results/training_data/'\n",
    "sites = os.listdir('/g/data/os22/chad_tmp/NEE_modelling/results/training_data/')\n",
    "\n",
    "td = []\n",
    "for site in sites:\n",
    "    if '.csv' in site:\n",
    "        xx = pd.read_csv(base+site, index_col='time', parse_dates=True)\n",
    "        td.append(xx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = []\n",
    "yy = []\n",
    "cc = []\n",
    "for t in td:\n",
    "    t = t.dropna()  # remove NaNS\n",
    "    coords = t[['x_coord','y_coord']] #extract coords\n",
    "    t = t.drop(['x_coord','y_coord','PFT_RS'] , axis=1) # drop coords and Landcover\n",
    "    df = t.drop(['NEE_SOLO_EC','GPP_SOLO_EC','ER_SOLO_EC'], axis=1) # seperate carbon fluxes\n",
    "    \n",
    "    df = df.filter(regex='RS') # only use remote sensing variables   \n",
    "    \n",
    "    # Write out predictior variables to text file\n",
    "    textfile = open(\"/g/data/os22/chad_tmp/NEE_modelling/results/variables.txt\", \"w\")\n",
    "    for element in df.columns:\n",
    "        textfile.write(element + \",\")\n",
    "    textfile.close()\n",
    "    \n",
    "    df_var=t[model_var+'_SOLO_EC'] # seperate out the variable we're modelling\n",
    "\n",
    "    x = df.reset_index(drop=True)#.to_numpy()\n",
    "    y = df_var.reset_index(drop=True)#.to_numpy()\n",
    "    c = coords.reset_index(drop=True)\n",
    "    xx.append(x)\n",
    "    yy.append(y)\n",
    "    cc.append(c)\n",
    "    \n",
    "x = np.concatenate([x for x in xx])\n",
    "y = np.concatenate([y for y in yy])\n",
    "c = np.concatenate([c for c in cc])\n",
    "\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model robustness with nested spatial k-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_cv_splits = 5\n",
    "\n",
    "outer_cv_splits = 7\n",
    "\n",
    "test_size = 0.20\n",
    "\n",
    "cluster_method = 'Hierarchical'\n",
    "\n",
    "max_distance = 1 # map degrees\n",
    "\n",
    "n_clusters=25\n",
    "\n",
    "kfold_method = 'SpatialShuffleSplit'\n",
    "\n",
    "balance = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising spatial groupings\n",
    "\n",
    "After setting the parameters, lets first generate spatial clusters using the DE Africa function spatial_clusters to visualize how our data will be grouped when running SKCV in the next step. You may want to refine the parameters to achieve a grouping that works for your dataset by resetting the parameters above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aus=gpd.read_file('/g/data/os22/chad_tmp/NEE_modelling/data/aus_outline/AUS_2021_AUST_GDA2020.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create clustes\n",
    "spatial_groups = spatial_clusters(coordinates=c,\n",
    "                                  method=cluster_method,\n",
    "                                  max_distance=max_distance,\n",
    "                                  n_groups=n_clusters,\n",
    "                                  verbose=True)\n",
    "\n",
    "#plot\n",
    "fig, ax = plt.subplots(1,1, figsize=(10,7))\n",
    "aus[aus.AUS_CODE21=='AUS'].plot(ax=ax, color='lightgrey')\n",
    "ax.scatter(c[:, 0], c[:, 1], c=spatial_groups,\n",
    "            s=50, cmap='viridis');\n",
    "ax.set_xlim(110,155)\n",
    "plt.title('Spatial clusters of training data')\n",
    "plt.ylabel('y')\n",
    "plt.xlabel('x');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the parameter grid based on the results of random search \n",
    "param_grid = {\n",
    "    'criterion': [\"squared_error\", \"absolute_error\"],\n",
    "    'max_features': [1.0, 'log2', None],\n",
    "    'n_estimators': [100,200,300,400]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create outer k-fold splits\n",
    "outer_cv = SKCV(\n",
    "    coordinates=c,\n",
    "    max_distance=max_distance,\n",
    "    n_splits=outer_cv_splits,\n",
    "    cluster_method=cluster_method,\n",
    "    kfold_method=kfold_method,\n",
    "    test_size=test_size,\n",
    "    balance=balance,\n",
    ")\n",
    "\n",
    "# lists to store results of CV testing\n",
    "acc = []\n",
    "rmse=[]\n",
    "r2=[]\n",
    "i = 1\n",
    "for train_index, test_index in outer_cv.split(c):\n",
    "    print(f\"Working on {i}/{outer_cv_splits} outer cv split\", end='\\r')\n",
    "    model = RandomForestRegressor(random_state=1, n_jobs=ncpus)\n",
    "\n",
    "    # index training, testing, and coordinate data\n",
    "    X_tr, X_tt = x[train_index, :], x[test_index, :]\n",
    "    y_tr, y_tt = y[train_index], y[test_index]\n",
    "    coords = c[train_index]\n",
    "    \n",
    "    # inner split on data within outer split\n",
    "    inner_cv = SKCV(\n",
    "        coordinates=coords,\n",
    "        max_distance=max_distance,\n",
    "        n_splits=inner_cv_splits,\n",
    "        cluster_method=cluster_method,\n",
    "        kfold_method=kfold_method,\n",
    "        test_size=test_size,\n",
    "        balance=balance,\n",
    "    )\n",
    "    \n",
    "    clf = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grid,\n",
    "        scoring='r2',\n",
    "        n_jobs=ncpus,\n",
    "        refit=True,\n",
    "        cv=inner_cv.split(coords),\n",
    "    )\n",
    "\n",
    "    clf.fit(X_tr, y_tr)\n",
    "    # predict using the best model\n",
    "    best_model = clf.best_estimator_\n",
    "    pred = best_model.predict(X_tt)\n",
    "\n",
    "    # evaluate model w/ multiple metrics\n",
    "    # r2\n",
    "    r2_ = r2_score(y_tt, pred)\n",
    "    r2.append(r2_)\n",
    "    # Overall accuracy\n",
    "    ac = mean_absolute_error(y_tt, pred)\n",
    "    acc.append(ac)\n",
    "    # F1 scores\n",
    "    rmse_ = np.sqrt(mean_squared_error(y_tt, pred))\n",
    "    rmse.append(rmse_)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean MAE accuracy: \"+ str(round(np.mean(acc), 2)))\n",
    "print(\"Std dev of MAE accuracy: \"+ str(round(np.std(acc), 2)))\n",
    "print('\\n')\n",
    "print(\"Mean RMSE: \"+ str(round(np.mean(rmse), 2)))\n",
    "print(\"Std dev RMSE: \"+ str(round(np.std(rmse), 2)))\n",
    "print('\\n')\n",
    "print(\"Mean r2: \"+ str(round(np.mean(r2), 2)))\n",
    "print(\"Std dev r2: \"+ str(round(np.std(r2), 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate n_splits of train-test_split\n",
    "ss = SKCV(\n",
    "        coordinates=c,\n",
    "        max_distance=max_distance,\n",
    "        n_groups=n_clusters,\n",
    "        n_splits=outer_cv_splits,\n",
    "        cluster_method=cluster_method,\n",
    "        kfold_method=kfold_method,\n",
    "        test_size=test_size,\n",
    "        balance=balance\n",
    "        )\n",
    "\n",
    "#instatiate a gridsearchCV\n",
    "clf = GridSearchCV(RandomForestRegressor(),\n",
    "                   param_grid,\n",
    "                   scoring='r2',\n",
    "                   verbose=1,\n",
    "                   cv=ss.split(c),\n",
    "                   n_jobs=ncpus)\n",
    "\n",
    "clf.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The most accurate combination of tested parameters is: \")\n",
    "pprint(clf.best_params_)\n",
    "print('\\n')\n",
    "print(\"The r2 score using these parameters is: \")\n",
    "print(round(clf.best_score_, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit on all data using best params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestRegressor(**clf.best_params_, random_state=1, n_jobs=ncpus)\n",
    "model.fit(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine feature importance\n",
    "\n",
    "SHAP (SHapley Additive exPlanations) is a game theoretic approach to explain the output of any machine learning model\n",
    "\n",
    "https://github.com/slundberg/shap\n",
    "\n",
    "https://towardsdatascience.com/explaining-scikit-learn-models-with-shap-61daff21b12a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explain the model's predictions using SHAP\n",
    "explainer = shap.Explainer(model)\n",
    "shap_values = explainer(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add names of features\n",
    "shap_values.feature_names = df.columns.values\n",
    "\n",
    "# visualize the importances\n",
    "# shap.plots.waterfall(shap_values[0])\n",
    "# shap.plots.beeswarm(shap_values)\n",
    "shap.plots.bar(shap_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(model, '/g/data/os22/chad_tmp/NEE_modelling/results/models/'+model_name+'_'+model_var+'_RF_SKCV_model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x)\n",
    "\n",
    "rmse = float(format(np.sqrt(mean_squared_error(y, y_pred)), '.3f'))\n",
    "print(\"RMSE:\", rmse);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.scatterplot(x=y,y=y_pred,color=\"#338844\", edgecolor=\"white\", s=50, lw=1, alpha=0.5)\n",
    "sb.regplot(x=y, y=y_pred, scatter=False, color='m')\n",
    "sb.regplot(x=y, y=y, color='black', scatter=False, line_kws={'linestyle':'dashed'});\n",
    "plt.xlabel('True')\n",
    "plt.ylabel('Prediction');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
