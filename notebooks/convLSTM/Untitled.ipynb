{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7469c9fc-316a-4fba-9f67-8f3f73a8e619",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install cupy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7aee86e-0bec-4c63-ab17-7fac994f9806",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eed659f4-c2e2-4e9d-bbb9-19001725cd2f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cupy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mchainer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcopy\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcupy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mcp\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cupy'"
     ]
    }
   ],
   "source": [
    "# This model was used in the Google Science Fair \"Annual Gate ConvLSTM\"\n",
    "# model. It utilizes a modification known as an \"Annual Gate\" that allows\n",
    "# the LSTM to more strongly learn the annual repititions in the crop data.\n",
    "# Furthermore, the encoder-decoder model means it can predict the next 32\n",
    "# days of NDVI vegetation images. \n",
    "\n",
    "import math\n",
    "import chainer\n",
    "import numpy as np\n",
    "import chainer.functions as F\n",
    "import copy\n",
    "import cupy as cp\n",
    "import sys\n",
    "import random\n",
    "import matplotlib.pyplot as plt        \n",
    "import urllib\n",
    "import zipfile\n",
    "import os\n",
    "from scipy.interpolate import UnivariateSpline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fd6589-ef9b-4f3a-a29c-6b00d5de2bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "learning_rate = 0.001\n",
    "\n",
    "# number of sets of images\n",
    "S = 1\n",
    "\n",
    "# number of images per sequence/set\n",
    "T = 410\n",
    "\n",
    "# dimensions of the image\n",
    "M = 100\n",
    "N = 100\n",
    "\n",
    "# how far to look back at x tau\n",
    "distance = 39\n",
    "distance_forward = 14\n",
    "learning_window = 15\n",
    "prev_validate = 100\n",
    "tau_len = distance_forward\n",
    "\n",
    "channels_img_Decode = 1 \n",
    "channels_img = 2 # antecedent NDVI and rain\n",
    "channels_hidden = 24\n",
    "channels_hidden_initial = channels_hidden\n",
    "kernel_dimension = 5\n",
    "kernel_dimension_g = 5\n",
    "kernel_dimension_p = 5\n",
    "channels_p = 1\n",
    "pad_constant = 2\n",
    "channels_img_output = 1\n",
    "\n",
    "steps_ahead = 4\n",
    "\n",
    "#this values are saved for producing the final output that must be displayed to the user\n",
    "stdev = 0\n",
    "mean = 0\n",
    "\n",
    "satellite_images = np.empty([S, 711, channels_img, M, N])\n",
    "\n",
    "#---------------------------------------He Normal Initialization-------------------------------------\n",
    "#---------------------------------------------DECODER------------------------------------------------\n",
    "r_v_connected_weights = 2*math.sqrt(6/(channels_hidden_initial*M*N + 1))\n",
    "r_e_kernel = 2*math.sqrt(6/(channels_hidden_initial + (channels_img + channels_hidden_initial)*(kernel_dimension)*(kernel_dimension)))\n",
    "\n",
    "v_connected_weights = cp.random.uniform(-r_v_connected_weights, r_v_connected_weights,(channels_hidden*M*N))\n",
    "e_kernel = cp.random.uniform(-r_e_kernel, r_e_kernel, (channels_hidden, channels_img + channels_hidden, kernel_dimension, kernel_dimension))\n",
    "\n",
    "\n",
    "r_kernel_tanh = math.sqrt(6/((channels_hidden_initial+channels_img_Decode)*(kernel_dimension)*(kernel_dimension) + channels_hidden_initial))\n",
    "r_kernel_sigmoid = math.sqrt(6/((channels_hidden_initial+channels_img_Decode)*(kernel_dimension)*(kernel_dimension) + channels_hidden_initial))\n",
    "r_kernel_g = math.sqrt(6/((channels_hidden_initial + channels_p)*(kernel_dimension_g)*(kernel_dimension_g) + channels_hidden_initial))\n",
    "r_connected_weights =  math.sqrt(6/(channels_hidden_initial + 1))\n",
    "r_connected_weights_tau = math.sqrt(1/(tau_len + M*N*channels_hidden_initial))\n",
    "\n",
    "increaseRate = 1.8\n",
    "i_kernel = cp.random.uniform(-r_kernel_sigmoid, r_kernel_sigmoid, (channels_hidden, channels_img_Decode + channels_hidden, kernel_dimension, kernel_dimension))\n",
    "f_kernel = cp.random.uniform(-r_kernel_sigmoid, r_kernel_sigmoid, (channels_hidden, channels_img_Decode + channels_hidden, kernel_dimension, kernel_dimension))\n",
    "a_kernel = increaseRate*cp.random.uniform(-r_kernel_tanh, r_kernel_tanh, (channels_hidden, channels_img_Decode + channels_hidden, kernel_dimension, kernel_dimension))\n",
    "o_kernel = cp.random.uniform(-r_kernel_sigmoid, r_kernel_sigmoid, (channels_hidden, channels_img_Decode + channels_hidden, kernel_dimension, kernel_dimension))\n",
    "n_kernel = cp.random.uniform(-r_kernel_sigmoid, r_kernel_sigmoid, (channels_hidden, channels_img_Decode + channels_hidden, kernel_dimension, kernel_dimension))\n",
    "main_kernel = cp.concatenate((i_kernel, f_kernel, a_kernel, o_kernel, n_kernel))\n",
    "connected_weights = 0.8*cp.random.normal(-r_connected_weights, r_connected_weights, (1, channels_hidden))\n",
    "g_kernel = increaseRate*cp.random.uniform(-r_kernel_g, r_kernel_g, (channels_hidden, channels_p + channels_hidden, kernel_dimension_g, kernel_dimension_g))\n",
    "\n",
    "#--------------------------------------ENCODER--------------------------------------\n",
    "r_kernel_tanh = math.sqrt(6/((channels_hidden_initial+channels_img)*(kernel_dimension)*(kernel_dimension) + channels_hidden_initial))\n",
    "r_kernel_sigmoid = math.sqrt(6/((channels_hidden_initial+channels_img)*(kernel_dimension)*(kernel_dimension) + channels_hidden_initial))\n",
    "r_connected_weights = math.sqrt(6/(channels_hidden + 1))\n",
    "\n",
    "a_kernel = cp.random.uniform(-r_kernel_tanh, r_kernel_tanh, (channels_hidden, channels_img + channels_hidden, kernel_dimension, kernel_dimension))\n",
    "i_kernel = cp.random.uniform(-r_kernel_sigmoid, r_kernel_sigmoid, (channels_hidden, channels_img + channels_hidden, kernel_dimension, kernel_dimension))\n",
    "f_kernel = cp.random.uniform(-r_kernel_sigmoid, r_kernel_sigmoid, (channels_hidden, channels_img + channels_hidden, kernel_dimension, kernel_dimension))\n",
    "o_kernel = cp.random.uniform(-r_kernel_sigmoid, r_kernel_sigmoid, (channels_hidden, channels_img + channels_hidden, kernel_dimension, kernel_dimension))\n",
    "connected_weightsEncode = cp.random.normal(-r_connected_weights, r_connected_weights, (1, channels_hidden))\n",
    "main_kernelEncode = cp.concatenate((i_kernel, f_kernel, a_kernel, o_kernel))\n",
    "\n",
    "\n",
    "#--------------------------------------BIAS INITIALIZATION-----------------------------\n",
    "bias_cEncode = cp.zeros([channels_hidden, M, N])\n",
    "bias_iEncode = cp.zeros([channels_hidden, M, N])\n",
    "bias_fEncode = cp.ones([channels_hidden, M, N])\n",
    "bias_oEncode = cp.zeros([channels_hidden, M, N])\n",
    "bias_yEncode = cp.zeros([channels_img, M, N])\n",
    "\n",
    "ndviMean = 0.66673688145266\n",
    "ndviStdDev = 0.16560766237944935\n",
    "rainMean = 0.19636724781555773\n",
    "rainStdDev = 0.16560766237944935\n",
    "\n",
    "del i_kernel\n",
    "del f_kernel\n",
    "del a_kernel\n",
    "del o_kernel\n",
    "del n_kernel\n",
    "\n",
    "learning_rate = 0.0005\n",
    "clip_threshold = 5\n",
    "\n",
    "bias_c = cp.zeros([channels_hidden, M, N])\n",
    "bias_i = cp.full([channels_hidden, M, N], 0)\n",
    "bias_f = cp.full([channels_hidden, M, N], 1)\n",
    "bias_o = cp.zeros([channels_hidden, M, N])\n",
    "bias_y = cp.full([channels_img, M, N], 0)\n",
    "bias_n = cp.zeros([channels_hidden, M, N])\n",
    "bias_g = cp.zeros([channels_hidden, M, N])\n",
    "bias_e =  cp.zeros([channels_hidden, M, N])\n",
    "bias_v = cp.zeros([distance_forward])\n",
    "\n",
    "#--------------data structure used allowing to us to process data from both Terra and Aqua satellites--------------\n",
    "class ImageSat(object):\n",
    "    index = 0\n",
    "    satellite = \"SAME\"\n",
    "\n",
    "    def __init__(self, index, satellite):\n",
    "        self.index = index\n",
    "        self.satellite = satellite\n",
    "\n",
    "def make_ImageSat(index, satellite):\n",
    "    imageSat = ImageSat(index, satellite)\n",
    "    return imageSat\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = cp.exp(x - cp.max(x))\n",
    "    return e_x / cp.sum(e_x)\n",
    "\n",
    "def softmax_derivative(x):\n",
    "    return softmax(x)*(1 - softmax(x))\n",
    "\n",
    "#-----------helper functions for foreward prop and computing gradients in backprop----------  \n",
    "def sigmoid(k):\n",
    "    return 1 / (1 + cp.exp(-k))\n",
    "\n",
    "def sigmoid_derivative(k):\n",
    "    return sigmoid(k) * (1 - sigmoid(k))\n",
    "\n",
    "def bipolar_sigmoid(k):\n",
    "    return 2 / (1 + cp.exp(-k)) - cp.ones(k.shape)\n",
    "\n",
    "def bipolar_derivative(k):\n",
    "    return (1 - (bipolar_sigmoid(k)**2))/2\n",
    "\n",
    "def tanh(k):\n",
    "    return cp.tanh(k)\n",
    "\n",
    "def expdecay(x):\n",
    "    return distance/(1+cp.exp(-0.01*x))\n",
    "\n",
    "def rect_linear_exponential(arr):\n",
    "    arr2 = copy.deepcopy(arr)\n",
    "    arr2 = expdecay(arr2)\n",
    "    return arr2\n",
    "\n",
    "def rect_linear_exponential_derivative(arr):\n",
    "    arr2 = copy.deepcopy(arr)\n",
    "    derivatives = 0.01*115*cp.exp(-0.01*arr2)/((1+cp.exp(-0.01*arr2))**2)\n",
    "    return derivatives\n",
    "\n",
    "def normalize_np(arr, mean, stddev):\n",
    "    return (arr+0.3)/1.3\n",
    "    #return 1/(1+np.exp(-(arr-mean)/stddev))\n",
    "\n",
    "def unnormalize_np(arr, mean, stddev):\n",
    "    return arr*1.3 - 0.3\n",
    "    #return mean - stddev*np.log((1-arr)/arr)\n",
    "\n",
    "def normalize_cp(arr, mean, stddev):\n",
    "    return (arr+0.3)/1.3\n",
    "    #return 1/(1+cp.exp(-(arr-mean)/stddev))\n",
    "\n",
    "def unnormalize_cp(arr, mean, stddev):\n",
    "    return arr*1.3 - 0.3\n",
    "    #return mean - stddev*cp.log((1-arr)/arr)\n",
    "\n",
    "def rect_linear(arr):\n",
    "    newArr = copy.deepcopy(arr)\n",
    "    newArr[arr<0] = 0\n",
    "    return newArr\n",
    "\n",
    "def rect_linear_derivative(arr):\n",
    "    newArr = cp.zeros(arr.shape)\n",
    "    newArr[arr>0] = 1\n",
    "    return newArr\n",
    "\n",
    "# x[t] is the input\n",
    "def encode(x,local_time2):\n",
    "\n",
    "    # Input Gate\n",
    "    i = cp.empty([local_time2, channels_hidden, M, N])\n",
    "\n",
    "    # Forget Gate\n",
    "    f = cp.empty([local_time2, channels_hidden, M, N])\n",
    "\n",
    "    # Memory\n",
    "    a = cp.empty([local_time2, channels_hidden, M, N])\n",
    "\n",
    "    # Cell Gate\n",
    "    c = cp.empty([local_time2 + 1, channels_hidden, M, N])\n",
    "    c[-1] = cp.zeros([channels_hidden, M, N])\n",
    "    # Output Gate\n",
    "    o = cp.empty([local_time2, channels_hidden, M, N])\n",
    "\n",
    "    # Hidden Unit\n",
    "    h = cp.empty([local_time2 + 1, channels_hidden, M, N])\n",
    "    h[-1] = cp.zeros([channels_hidden, M, N])\n",
    "    # LSTM FORWARD PROPAGATION\n",
    "    for t in range(local_time2):\n",
    "\n",
    "        temporary = cp.concatenate((x[t], h[t - 1]), axis=0)\n",
    "        temporary = temporary.reshape(1, channels_img + channels_hidden, M, N)\n",
    "\n",
    "        i[t] = sigmoid(cp.asarray(F.convolution_2d(temporary, main_kernelEncode[0:channels_hidden], b=None, pad=pad_constant)[0].data) + bias_iEncode)\n",
    "        f[t] = sigmoid(cp.asarray(F.convolution_2d(temporary, main_kernelEncode[channels_hidden:2*channels_hidden], b=None, pad=pad_constant)[0].data) + bias_fEncode)\n",
    "        a[t] = tanh(cp.asarray(F.convolution_2d(temporary, main_kernelEncode[2*channels_hidden:3*channels_hidden], b=None, pad=pad_constant)[0].data) + bias_cEncode)\n",
    "        c[t] = cp.multiply(f[t], c[t - 1]) + cp.multiply(i[t], a[t])\n",
    "        o[t] = sigmoid(cp.asarray(F.convolution_2d(temporary, main_kernelEncode[3*channels_hidden:4*channels_hidden], b=None, pad=pad_constant)[0].data) + bias_oEncode)\n",
    "        h[t] = cp.multiply(o[t], tanh(c[t]))\n",
    "\n",
    "    # 1 x 1 convolution\n",
    "    #output = cp.matmul(connected_weights, h[local_time-1].reshape(channels_hidden, M * N)).reshape(M, N) + bias_y[0]\n",
    "    print(\"CONNECTED_WEIGHTS NORM: \" + str(cp.linalg.norm(connected_weights)))\n",
    "    print(\"HIDDEN_PREDICTION NORM: \" + str(cp.linalg.norm(h[local_time2-1])))\n",
    "    return cp.reshape(h[local_time2-1], (channels_hidden, M*N)), i, f, a, c, o, h\n",
    "\n",
    "# x[t] is the input\n",
    "def decode(local_time2, sequence, isFirst, timestamp, satellite_name, cEncode, hEncode, initial_input_Image):\n",
    "    global bias_tau\n",
    "\n",
    "    # Input Gate\n",
    "    i = cp.empty([local_time2, channels_hidden, M, N])\n",
    "\n",
    "    # Forget Gate\n",
    "    f = cp.empty([local_time2, channels_hidden, M, N])\n",
    "\n",
    "    # Memory\n",
    "    a = cp.empty([local_time2, channels_hidden, M, N])\n",
    "\n",
    "    # Output Gate\n",
    "    o = cp.empty([local_time2, channels_hidden, M, N])\n",
    "\n",
    "    n = cp.empty([local_time2, channels_hidden, M, N])\n",
    "    p = cp.empty([local_time2, channels_p, M, N])\n",
    "    g = cp.empty([local_time2, channels_hidden, M, N])\n",
    "    s = cp.empty([local_time2, distance_forward, channels_hidden, M, N])\n",
    "    e = cp.empty([local_time2, distance_forward])\n",
    "\n",
    "    alpha = cp.empty([local_time2, distance_forward])\n",
    "    initial_Image = initial_input_Image\n",
    "    xDecode = cp.zeros([local_time2, M, N])\n",
    "    \n",
    "    # Cell Gate\n",
    "    c = cp.empty([local_time2 + 1, channels_hidden, M, N])\n",
    "    c[-1] = cEncode\n",
    "\n",
    "    # Hidden Unit\n",
    "    h = cp.empty([local_time2 + 1, channels_hidden, M, N])\n",
    "    h[-1] = hEncode\n",
    "\n",
    "    hidden_prediction = cp.empty([local_time2, channels_hidden, M, N])\n",
    "\n",
    "    actual_output = cp.zeros([local_time2, M, N])\n",
    "    actual_output_pre_sigmoid = cp.zeros([local_time2, M, N]) \n",
    "    # LSTM FORWARD PROPAGATION\n",
    "    for t in range(local_time2):\n",
    "\n",
    "        temporary = cp.concatenate((initial_Image.reshape(1, M, N), h[t - 1]), axis=0)\n",
    "        temporary = temporary.reshape(1, channels_img_output + channels_hidden, M, N)\n",
    "        xDecode[t] = initial_Image[0]\n",
    "        print(\"HIDDEN PREDICTION INPUT: \" + str(cp.mean(h[t - 1])))\n",
    "        print(\"IMAGE INPUT: \" + str(cp.mean(initial_Image)))\n",
    "        i[t] = sigmoid(cp.asarray(F.convolution_2d(temporary, main_kernel[0:channels_hidden], b=None, pad=pad_constant)[0].data) + bias_i)\n",
    "\n",
    "        f[t] = sigmoid(cp.asarray(F.convolution_2d(temporary, main_kernel[channels_hidden:2*channels_hidden], b=None, pad=pad_constant)[0].data) + bias_f)\n",
    "\n",
    "        a[t] = tanh(cp.asarray(F.convolution_2d(temporary, main_kernel[2*channels_hidden:3*channels_hidden], b=None, pad=pad_constant)[0].data) + bias_c)\n",
    "\n",
    "        n[t] = sigmoid(cp.asarray(F.convolution_2d(temporary, main_kernel[4*channels_hidden:5*channels_hidden], b=None, pad=pad_constant)[0].data) + bias_n)\n",
    "\n",
    "        # Attention Network\n",
    "        for z in range(timestamp + t - (distance + learning_window), timestamp + distance_forward + t - (distance + learning_window)):\n",
    "            temp = cp.concatenate((cp.asarray(satellite_images[sequence][z]), h[t - 1]), axis = 0)\n",
    "            s[t][z - (timestamp + t - (distance + learning_window))] = tanh(cp.asarray(F.convolution_2d(temp.reshape(1, channels_img + channels_hidden, M, N), e_kernel, b=None, pad=pad_constant)[0].data) + bias_e)\n",
    "            s_temp = s[t][z - (timestamp + t - (distance + learning_window))].reshape(M*N*channels_hidden)\n",
    "            e[t][z - (timestamp + t - (distance + learning_window))] = cp.dot(v_connected_weights, s_temp) + bias_v[z - (timestamp + t - (distance + learning_window))]\n",
    "\n",
    "        xtemp = satellite_images[sequence][timestamp - distance:timestamp-distance+distance_forward, 0]\n",
    "\n",
    "        alpha[t] = softmax(e[t])\n",
    "        p[t] = cp.tensordot(alpha[t], cp.asarray(xtemp), axes = 1).reshape(1, M, N) # Sum all x arrays up, weighted array\n",
    "\n",
    "        temporary2 = cp.concatenate((p[t], h[t-1]), axis = 0)\n",
    "        temporary2 = temporary2.reshape(1, channels_p + channels_hidden, M, N)\n",
    "\n",
    "        g[t] = tanh(cp.asarray(F.convolution_2d(temporary2, g_kernel, b=None, pad=pad_constant)[0].data) + bias_g)\n",
    "\n",
    "        c[t] = cp.multiply(f[t], c[t - 1]) + cp.multiply(i[t], a[t]) + cp.multiply(n[t], g[t])\n",
    "\n",
    "        o[t] = sigmoid(cp.asarray(F.convolution_2d(temporary, main_kernel[3*channels_hidden:4*channels_hidden], b=None, pad=pad_constant)[0].data) + bias_o)\n",
    "\n",
    "        h[t] = cp.multiply(o[t], tanh(c[t]))\n",
    "\n",
    "        output = cp.matmul(connected_weights, h[t].reshape(channels_hidden, M * N)).reshape(M, N) + bias_y[0]\n",
    "        true_output = sigmoid(output)\n",
    "        actual_output[t] = true_output\n",
    "        actual_output_pre_sigmoid[t] = output  \n",
    "        initial_Image = true_output.reshape(1, M, N)\n",
    "\n",
    "      \n",
    "    return actual_output, actual_output_pre_sigmoid, cp.reshape(h[local_time2-1], (channels_hidden, M*N)), i, f, a, c, o, h, n, g, p, s, e, alpha, xtemp, xDecode\n",
    "\n",
    "def calculate_loss2(prediction, y):\n",
    "    return -np.sum(np.multiply(y, np.log(prediction)) + np.multiply(np.ones(y.shape) - y, np.log(np.ones(y.shape) - prediction)))\n",
    "\n",
    "#root mean square error\n",
    "def rootmeansquare(prediction, y):\n",
    "    return cp.sqrt(cp.sum((prediction - y)**2)/(10000))\n",
    "\n",
    "# Calculate loss\n",
    "def calculate_loss(prediction, y):\n",
    "    lossExpression = 0.5*cp.sum((prediction - y)**2)\n",
    "    return lossExpression\n",
    "\n",
    "def calculate_loss_modified(prediction, y):\n",
    "    prediction[prediction == 0] = 0.00000001\n",
    "    y[y == 0] = 0.00000001\n",
    "    lossExpression = -cp.sum(cp.multiply(y, cp.log(prediction)) + cp.multiply(cp.ones(y.shape) - y, cp.log(cp.ones(y.shape) - prediction)))\n",
    "    return lossExpression\n",
    "\n",
    "def return_forecast(x, local_time, timestamp):\n",
    "    hidden_predictionEncode, iEncode, fEncode, aEncode, cEncode, oEncode, hEncode = encode(x, local_time-1)\n",
    "    prediction, pre_sigmoid_prediction, hidden_prediction, i, f, a, c, o, h, n, g, p, s, e, alpha, xtemp, xDecode = decode(steps_ahead, 0, False, timestamp + local_time - 1, \"SAME\", cEncode[-2], hEncode[-2],  x[len(x)-1][0])\n",
    "\n",
    "    return cp.asnumpy(prediction)\n",
    "\n",
    "def loss_derivative(x, y):\n",
    "    return (x-y)\n",
    "    \n",
    "def bptt(x2, y2, iteration, local_time, region, isFirst, timestamp, satellite_name):\n",
    "    #--------------------------------------------DECODER BACKPROP CODE---------------------------------------------\n",
    "    x = cp.asarray(x2)\n",
    "    y = cp.asarray(y2)\n",
    "\n",
    "    global connected_weights\n",
    "    global main_kernel\n",
    "    global bias_i\n",
    "    global bias_f\n",
    "    global bias_c\n",
    "    global bias_o\n",
    "    global bias_y\n",
    "    global bias_n\n",
    "    global bias_g\n",
    "    global e_kernel\n",
    "    global bias_e\n",
    "    global learning_rate\n",
    "    global g_kernel\n",
    "    global v_connected_weights\n",
    "    global bias_v\n",
    "\n",
    "\n",
    "    # Perform forward prop\n",
    "    hidden_predictionEncode, iEncode, fEncode, aEncode, cEncode, oEncode, hEncode = encode(x, local_time-1)\n",
    "    prediction, pre_sigmoid_prediction, hidden_prediction, i, f, a, c, o, h, n, g, p, s, e, alpha, xtemp, xDecode = decode(steps_ahead, region, isFirst, timestamp + local_time - 1, satellite_name, cEncode[-2], hEncode[-2],  x[len(x)-1][0])\n",
    "\n",
    "    sumLossInitial = 0\n",
    "    for counter in range(0, steps_ahead):\n",
    "        loss = calculate_loss(prediction[counter], y[counter:counter+1, 0][0])\n",
    "        sumLossInitial += loss\n",
    "        print(\"LOSS \" + str(counter+1) + \": \" + str(loss))\n",
    "        if loss > 100:\n",
    "            f2 = open(\"wrong.txt\", \"a\")\n",
    "            f2.write(str(timestamp) + \"\\n\")\n",
    "\n",
    "    #---------------------------------------------DECODER DERIVATIVE COMPUTATION--------------------------------------------\n",
    "    \n",
    "    dLdW = cp.zeros([5*channels_hidden, channels_img_Decode + channels_hidden, kernel_dimension, kernel_dimension])\n",
    "\n",
    "    # Initialize other weight matrices\n",
    "    dLdW_g = cp.zeros([channels_hidden, channels_p + channels_hidden, kernel_dimension_g, kernel_dimension_g])\n",
    "    dLdW_v = cp.zeros([channels_hidden*M*N])\n",
    "    dLdW_e = cp.zeros([channels_hidden, channels_img + channels_hidden, kernel_dimension, kernel_dimension])\n",
    "    dLdw_0 = cp.zeros(connected_weights.shape)\n",
    "\n",
    "    # initialize biases\n",
    "    dLdb_c = cp.zeros([channels_hidden, M, N])\n",
    "    dLdb_i = cp.zeros([channels_hidden, M, N])\n",
    "    dLdb_f = cp.zeros([channels_hidden, M, N])\n",
    "    dLdb_o = cp.zeros([channels_hidden, M, N])\n",
    "    dLdb_n = cp.zeros([channels_hidden, M, N])\n",
    "    dLdb_g = cp.zeros([channels_hidden, M, N])\n",
    "    dLdb_e = cp.zeros([channels_hidden, M, N])\n",
    "    dLdb_v = cp.zeros([distance_forward])\n",
    "    dLdb_y = cp.zeros(bias_y.shape)\n",
    "\n",
    "    # Initialize cell matrix\n",
    "    dLdc_current = cp.zeros([channels_hidden, M, N])\n",
    "    dLdx = cp.zeros([M, N])\n",
    "    dLdh = cp.zeros([channels_hidden, M, N])\n",
    "    for t in range(steps_ahead - 1, -1, -1):\n",
    "        dLdy_2 = dLdx + loss_derivative(prediction[t], y[t:t+1, 0])\n",
    "        # Might NEED TO CHANGE pre_sigmoid_prediction, note that there is \n",
    "        # bracket [t] to indicate timestep\n",
    "        dLdy_1 = cp.multiply(sigmoid_derivative(pre_sigmoid_prediction[t]), dLdy_2)\n",
    "        testArr = cp.reshape(cp.matmul(cp.transpose(connected_weights), dLdy_1.reshape(1, M * N)), (channels_hidden, M, N))\n",
    "\n",
    "        dLdh = testArr + dLdh\n",
    "        # bracket [t] to indicate timestep\n",
    "        dLdw_0 += cp.matmul(dLdy_1.reshape(1, M*N), h[t].reshape(channels_hidden, M*N).transpose(1,0))\n",
    "        dLdb_y += dLdy_1\n",
    "\n",
    "        dLdo = cp.multiply(dLdh, tanh(c[t]))\n",
    "        dLdc_current += cp.multiply(cp.multiply(dLdh, o[t]), (cp.ones((channels_hidden, M, N)) - cp.multiply(tanh(c[t]), tanh(c[t]))))\n",
    "        dLdi = cp.multiply(dLdc_current, a[t])\n",
    "        dLda = cp.multiply(dLdc_current, i[t])\n",
    "        dLdf = cp.multiply(dLdc_current, c[t - 1])\n",
    "\n",
    "        dLdg = cp.multiply(dLdc_current, n[t])\n",
    "        dLdn = cp.multiply(dLdc_current, g[t])\n",
    "\n",
    "        dLdc_previous = cp.multiply(dLdc_current, f[t])\n",
    "        dLda = cp.multiply(dLda, (cp.ones((channels_hidden, M, N)) - cp.multiply(a[t], a[t]))) #dLda_hat\n",
    "        dLdi = cp.multiply(cp.multiply(dLdi, i[t]), cp.ones((channels_hidden, M, N)) - i[t]) #dLdi_hat\n",
    "        dLdf = cp.multiply(cp.multiply(dLdf, f[t]), cp.ones((channels_hidden, M, N)) - f[t]) #dLdf_hat\n",
    "        dLdo = cp.multiply(cp.multiply(dLdo, o[t]), cp.ones((channels_hidden, M, N)) - o[t]) #dLdo_hat\n",
    "        dLdg = cp.multiply(dLdg, (cp.ones((channels_hidden, M, N)) - cp.multiply(g[t], g[t]))) #dLdg_hat\n",
    "        dLdn = cp.multiply(cp.multiply(dLdn, n[t]), cp.ones((channels_hidden, M, N)) - n[t]) #dLdn_hat\n",
    "\n",
    "        temporary_p = cp.concatenate((p[t], h[t - 1]), axis=0).reshape(channels_hidden + channels_p, 1, M, N)\n",
    "\n",
    "        #Convolve to continue backpropagation along the annual gate path\n",
    "        dLdI_g = cp.asarray(F.convolution_2d(dLdg.reshape(1, channels_hidden, M, N), g_kernel.transpose(1, 0, 2, 3), b=None, pad=pad_constant)[0].data)\n",
    "        dLdW_g_temp = cp.asarray((F.convolution_2d(temporary_p, dLdg.reshape(channels_hidden, 1, M, N), b=None, pad=pad_constant).data).transpose(1,0,2,3))\n",
    "        #create dLdp, which is the derivative of loss with respect to p\n",
    "        dLdp = dLdI_g[0: channels_p]\n",
    "        dLdAlpha = cp.zeros(distance_forward)\n",
    "        for k in range(0, distance_forward):\n",
    "            dLdAlpha[k] = cp.sum(dLdp*cp.asarray(xtemp[k]))\n",
    "\n",
    "        dLde = cp.zeros(distance_forward)\n",
    "        for z in range(distance_forward):\n",
    "            for j in range(distance_forward):\n",
    "                if z == j:\n",
    "                    dLde[z] += dLdAlpha[j] * alpha[t][j] * (1 - alpha[t][z])\n",
    "                else:\n",
    "                    dLde[z] += -dLdAlpha[j] * alpha[t][j] * alpha[t][z]\n",
    "\n",
    "        dLdW_v_temp = cp.zeros([channels_hidden*M*N])\n",
    "        dLdW_e_temp = cp.zeros([channels_hidden, channels_img + channels_hidden, kernel_dimension, kernel_dimension])\n",
    "        dLdh_temp = cp.zeros([channels_hidden, M, N])\n",
    "\n",
    "        for k in range(0, distance_forward):\n",
    "            dLdW_v_temp += dLde[k]*s[t][k].reshape((M*N*channels_hidden))\n",
    "            dLds = dLde[k]*v_connected_weights # Changes each iteration of nested loop\n",
    "            dLds = dLds.reshape((channels_hidden, M, N))\n",
    "            dLds = cp.multiply(dLds, (cp.ones((channels_hidden, M, N)) - cp.multiply(s[t][k], s[t][k])))\n",
    "            temp3 = cp.concatenate((cp.asarray(satellite_images[region][int(k - (timestamp + t - (distance + learning_window)))]), h[t - 1]), axis = 0)\n",
    "\n",
    "\n",
    "            dLdI_e = cp.asarray(F.convolution_2d(dLds.reshape(1, channels_hidden, M, N), e_kernel.transpose(1, 0, 2, 3), b=None, pad=pad_constant)[0].data) # reshape into flipped kernel dimensions\n",
    "            dLdW_e_temp += cp.asarray((F.convolution_2d(temp3.reshape(channels_hidden+channels_img, 1, M, N), dLds.reshape(channels_hidden, 1, M, N), b=None, pad=pad_constant).data).transpose(1,0,2,3)) #reshape into kernel dimensions\n",
    "            dLdh_temp += dLdI_e[channels_img: channels_img + channels_hidden]\n",
    "            # if cp.amax(dLds) > 1 or cp.amin(dLds) < -1:\n",
    "            #     dLds = dLds/cp.linalg.norm(dLds)\n",
    "            dLdb_e += dLds\n",
    "\n",
    "        dLdb_c += dLda\n",
    "        dLdb_i += dLdi\n",
    "        dLdb_f += dLdf\n",
    "        dLdb_o += dLdo\n",
    "        dLdb_n += dLdn\n",
    "        dLdb_g += dLdg\n",
    "        \n",
    "\n",
    "        # CONCATENATE Z IN THE RIGHT ORDER SAME ORDER AS THE WEIGHTS\n",
    "        dLdz_hat = cp.concatenate((dLdi, dLdf, dLda, dLdo, dLdn), axis = 0) \n",
    "        del dLdi\n",
    "        del dLdf\n",
    "        del dLda \n",
    "        del dLdo\n",
    "        del dLdn\n",
    "        del dLds\n",
    "        del dLdg\n",
    "\n",
    "        #determine convolution derivatives (main convolution)\n",
    "        #here we will use the fact that in z = w * I, dLdW = dLdz * I\n",
    "        temporary = cp.concatenate((xDecode[t].reshape(1, M, N), h[t - 1]), axis=0).reshape(channels_hidden + channels_img_Decode, 1, M, N)\n",
    "        dLdW_temp = cp.asarray((F.convolution_2d(temporary, dLdz_hat.reshape(5*channels_hidden, 1, M, N), b=None, pad=pad_constant).data).transpose(1,0,2,3)) #reshape into kernel dimensions\n",
    "\n",
    "        \n",
    "        # accumulate derivatives of weights and biases\n",
    "        dLdW += dLdW_temp \n",
    "        dLdW_g += dLdW_g_temp\n",
    "        dLdW_e += dLdW_e_temp\n",
    "        dLdW_v += dLdW_v_temp\n",
    "        dLdb_v += dLde.reshape([distance_forward])\n",
    "\n",
    "        del dLdW_temp\n",
    "        del dLdW_g_temp\n",
    "        del dLdW_v_temp\n",
    "        del dLdW_e_temp\n",
    "        del dLde\n",
    "\n",
    "        dLdI = (F.convolution_2d(cp.asnumpy(dLdz_hat).reshape(1, 5*channels_hidden, M, N), cp.asnumpy(main_kernel).transpose(1, 0, 2, 3), b=None, pad=pad_constant)[0].data) # reshape into flipped kernel dimensions\n",
    "        dLdx = cp.asarray(dLdI[0: channels_img_Decode])\n",
    "        # reinitialize what you're passing back\n",
    "        dLdh = cp.asarray(dLdI[channels_img_Decode: channels_img_Decode+channels_hidden]) + dLdI_g[channels_p: channels_p+channels_hidden] + dLdh_temp\n",
    "        dLdc_current = dLdc_previous\n",
    "\n",
    "    #delete variables from RAM \n",
    "    del prediction \n",
    "    del pre_sigmoid_prediction \n",
    "    del hidden_prediction\n",
    "    del i\n",
    "    del f\n",
    "    del a\n",
    "    del c\n",
    "    del o\n",
    "    del h\n",
    "    del n\n",
    "    del g\n",
    "    del p\n",
    "    del s\n",
    "    del e\n",
    "    del alpha\n",
    "    del xtemp\n",
    "\n",
    "    #Gradient clipping\n",
    "    if cp.linalg.norm(dLdW) > clip_threshold:\n",
    "        dLdW = dLdW*clip_threshold/cp.linalg.norm(dLdW)\n",
    "    if cp.linalg.norm(dLdW_g) > clip_threshold:\n",
    "        dLdW_g = dLdW_g*clip_threshold/cp.linalg.norm(dLdW_g)\n",
    "    if cp.linalg.norm(dLdW_e) > clip_threshold:\n",
    "        dLdW_e = dLdW_e*clip_threshold/cp.linalg.norm(dLdW_e)\n",
    "    if cp.linalg.norm(dLdW_v) > clip_threshold:\n",
    "        dLdW_v = dLdW_v*clip_threshold/cp.linalg.norm(dLdW_v)\n",
    "    if cp.linalg.norm(dLdb_c) > clip_threshold:\n",
    "        dLdb_c = dLdb_c*clip_threshold/cp.linalg.norm(dLdb_c)\n",
    "    if cp.linalg.norm(dLdb_i) > clip_threshold:\n",
    "        dLdb_i = dLdb_i*clip_threshold/cp.linalg.norm(dLdb_i)\n",
    "    if cp.linalg.norm(dLdb_f) > clip_threshold:\n",
    "        dLdb_f = dLdb_f*clip_threshold/cp.linalg.norm(dLdb_f)\n",
    "    if cp.linalg.norm(dLdb_o) > clip_threshold:\n",
    "        dLdb_o = dLdb_o*clip_threshold/cp.linalg.norm(dLdb_o)\n",
    "    if cp.linalg.norm(dLdb_n) > clip_threshold:\n",
    "        dLdb_n = dLdb_n*clip_threshold/cp.linalg.norm(dLdb_n)\n",
    "    if cp.linalg.norm(dLdb_g) > clip_threshold:\n",
    "        dLdb_g = dLdb_g*clip_threshold/cp.linalg.norm(dLdb_g)\n",
    "    if cp.linalg.norm(dLdb_e) > clip_threshold:\n",
    "        dLdb_e = dLdb_e*clip_threshold/cp.linalg.norm(dLdb_e)\n",
    "    if cp.linalg.norm(dLdb_v) > clip_threshold:\n",
    "        dLdb_v = dLdb_v*clip_threshold/cp.linalg.norm(dLdb_v)\n",
    "    \n",
    "    #---------------------update main kernel---------\n",
    "    main_kernel = main_kernel - learning_rate*dLdW\n",
    "    #---------------------update g kernel---------\n",
    "    g_kernel = g_kernel - learning_rate*dLdW_g\n",
    "    #---------------------update e kernel---------\n",
    "    e_kernel = e_kernel - learning_rate*dLdW_e\n",
    "    #---------------------update v_connected_weights---------\n",
    "    v_connected_weights = v_connected_weights - learning_rate*dLdW_v\n",
    "    #--------------------update bias c-----------------------\n",
    "    bias_c = bias_c - learning_rate*dLdb_c\n",
    "    #--------------------update bias i-----------------------\n",
    "    bias_i = bias_i - learning_rate*dLdb_i\n",
    "    #--------------------update bias f-----------------------\n",
    "    bias_f = bias_f - learning_rate*dLdb_f\n",
    "    #--------------------update bias o-----------------------\n",
    "    bias_o = bias_o - learning_rate*dLdb_o\n",
    "    #--------------------update bias n-----------------------\n",
    "    bias_n = bias_n - learning_rate*dLdb_n\n",
    "    #--------------------update bias g-----------------------\n",
    "    bias_g = bias_g - learning_rate*dLdb_g\n",
    "    #--------------------update bias e-----------------------\n",
    "    bias_e = bias_e - learning_rate*bias_e\n",
    "    #--------------------update bias v-----------------------\n",
    "    bias_v = bias_v - learning_rate*dLdb_v\n",
    "    #--------------------update connected_weights------------\n",
    "    connected_weights = connected_weights - learning_rate*dLdw_0\n",
    "    #--------------------update bias y-----------------------\n",
    "    bias_y = bias_y - learning_rate*dLdb_y\n",
    "\n",
    "    #------------------------------------------LSTM ENCODER BACKPROPAGATION CODE-----------------------------------------------\n",
    "    global connected_weightsEncode\n",
    "    global main_kernelEncode\n",
    "    global bias_iEncode\n",
    "    global bias_fEncode\n",
    "    global bias_cEncode\n",
    "    global bias_oEncode\n",
    "    global bias_yEncode\n",
    "\n",
    "    # Initialize weight matrix\n",
    "    dLdW = cp.zeros([4*channels_hidden, channels_img + channels_hidden, kernel_dimension, kernel_dimension])\n",
    "\n",
    "    # initialize biases\n",
    "    dLdb_c = cp.zeros([channels_hidden, M, N])\n",
    "    dLdb_i = cp.zeros([channels_hidden, M, N])\n",
    "    dLdb_f = cp.zeros([channels_hidden, M, N])\n",
    "    dLdb_o = cp.zeros([channels_hidden, M, N])\n",
    "\n",
    "    # Initialize cell matrix\n",
    "    #dLdc_current = cp.zeros([channels_hidden, M, N])\n",
    "\n",
    "    for t in range(local_time - 2, -1, -1):\n",
    "        dLdo = cp.multiply(dLdh, tanh(cEncode[t]))\n",
    "        dLdc_current += cp.multiply(cp.multiply(dLdh, oEncode[t]), (cp.ones((channels_hidden, M, N)) - cp.multiply(tanh(cEncode[t]), tanh(cEncode[t]))))\n",
    "        dLdi = cp.multiply(dLdc_current, aEncode[t])\n",
    "        dLda = cp.multiply(dLdc_current, iEncode[t])\n",
    "        dLdf = cp.multiply(dLdc_current, cEncode[t - 1])\n",
    "\n",
    "        dLdc_previous = cp.multiply(dLdc_current, fEncode[t])\n",
    "\n",
    "        dLda = cp.multiply(dLda, (cp.ones((channels_hidden, M, N)) - cp.multiply(aEncode[t], aEncode[t]))) #dLda_hat\n",
    "\n",
    "        dLdi = cp.multiply(cp.multiply(dLdi, iEncode[t]), cp.ones((channels_hidden, M, N)) - iEncode[t]) #dLdi_hat\n",
    "\n",
    "        dLdf = cp.multiply(cp.multiply(dLdf, fEncode[t]), cp.ones((channels_hidden, M, N)) - fEncode[t]) #dLdf_hat\n",
    "\n",
    "        dLdo = cp.multiply(cp.multiply(dLdo, oEncode[t]), cp.ones((channels_hidden, M, N)) - oEncode[t]) #dLdo_hat\n",
    "\n",
    "\n",
    "        # CONCATENATE Z IN THE RIGHT ORDER SAME ORDER AS THE WEIGHTS\n",
    "        dLdz_hat = cp.concatenate((dLdi, dLdf, dLda, dLdo), axis = 0) \n",
    "\n",
    "        #determine convolution derivatives\n",
    "        #here we will use the fact that in z = w * I, dLdW = dLdz * I\n",
    "        temporary = cp.concatenate((x[t], hEncode[t - 1]), axis=0).reshape(channels_hidden + channels_img, 1, M, N)\n",
    "        dLdI = cp.asarray(F.convolution_2d(dLdz_hat.reshape(1, 4*channels_hidden, M, N), main_kernelEncode.transpose(1, 0, 2, 3), b=None, pad=pad_constant)[0].data) # reshape into flipped kernel dimensions\n",
    "        dLdW_temp = cp.asarray((F.convolution_2d(temporary, dLdz_hat.reshape(4*channels_hidden, 1, M, N), b=None, pad=pad_constant).data).transpose(1,0,2,3)) #reshape into kernel dimensions\n",
    "\n",
    "        \n",
    "        # accumulate derivatives of weights and biases\n",
    "        dLdW += dLdW_temp \n",
    "        dLdb_c += dLda\n",
    "        dLdb_i += dLdi\n",
    "        dLdb_f += dLdf\n",
    "        dLdb_o += dLdo\n",
    "\n",
    "        # reinitialize what you're passing back\n",
    "        dLdh = dLdI[channels_img: channels_img+channels_hidden] \n",
    "        dLdc_current = dLdc_previous\n",
    "\n",
    "    #Clip all gradients again\n",
    "    if cp.linalg.norm(dLdW) > clip_threshold:\n",
    "        dLdW = dLdW*clip_threshold/cp.linalg.norm(dLdW)\n",
    "    if cp.linalg.norm(dLdb_c) > clip_threshold:\n",
    "        dLdb_c = dLdb_c*clip_threshold/cp.linalg.norm(dLdb_c)\n",
    "    if cp.linalg.norm(dLdb_i) > clip_threshold:\n",
    "        dLdb_i = dLdb_i*clip_threshold/cp.linalg.norm(dLdb_i)\n",
    "    if cp.linalg.norm(dLdb_f) > clip_threshold:\n",
    "        dLdb_f = dLdb_f*clip_threshold/cp.linalg.norm(dLdb_f)\n",
    "    if cp.linalg.norm(dLdb_o) > clip_threshold:\n",
    "        dLdb_o = dLdb_o*clip_threshold/cp.linalg.norm(dLdb_o)\n",
    "\n",
    "    #---------------------update main kernel---------\n",
    "    main_kernelEncode = main_kernelEncode - learning_rate*dLdW\n",
    "    #--------------------update bias c-----------------------\n",
    "    bias_cEncode = bias_cEncode - learning_rate*dLdb_c\n",
    "    #--------------------update bias i-----------------------\n",
    "    bias_iEncode = bias_iEncode - learning_rate*dLdb_i\n",
    "    #--------------------update bias f-----------------------\n",
    "    bias_fEncode = bias_fEncode - learning_rate*dLdb_f\n",
    "    #--------------------update bias c-----------------------\n",
    "    bias_oEncode = bias_oEncode - learning_rate*dLdb_o\n",
    "\n",
    "    # Perform forward prop\n",
    "    hidden_predictionEncode, iEncode, fEncode, aEncode, cEncode, oEncode, hEncode = encode(x, local_time-1)\n",
    "    prediction, pre_sigmoid_prediction, hidden_prediction, i, f, a, c, o, h, n, g, p, s, e, alpha, xtemp, xDecode = decode(steps_ahead, region, isFirst, timestamp + local_time - 1, satellite_name, cEncode[-2], hEncode[-2],  x[len(x)-1][0])\n",
    "\n",
    "    sumLossFinal = 0\n",
    "    for counter in range(0, steps_ahead):\n",
    "        loss = calculate_loss(prediction[counter], y[counter:counter+1, 0][0])\n",
    "        sumLossFinal += loss\n",
    "        print(\"LOSS AFTER\" + str(counter+1) + \": \" + str(loss))\n",
    "        #----------------PRINT RMSE-----------------\n",
    "        rms = rootmeansquare(unnormalize_cp(prediction[counter], ndviMean, ndviStdDev), unnormalize_cp(y[counter:counter+1, 0], ndviMean, ndviStdDev))\n",
    "        f2 = open(\"loss\" + str(counter+1) + \".txt\", \"a\")\n",
    "        f2.write(str(rms) + \"\\n\")\n",
    "    \n",
    "    if sumLossFinal > sumLossInitial:\n",
    "        print(\"-----------------------------------Close----------------------------------\")\n",
    "        #learning_rate = learning_rate*0.8\n",
    "\n",
    "    print(\"backpropagation complete\")\n",
    "\n",
    "def loadData():\n",
    "    global satellite_images\n",
    "\n",
    "    satellite_images = xr.open_dataarray('/g/data/os22/chad_tmp/NEE_modelling/data/5km/EVI_5km_monthly_2002_2021.nc').data\n",
    "\n",
    "    dataNDVI = np.zeros([14])\n",
    "    dataRain = np.zeros([10])\n",
    "\n",
    "    #totalnumber is 771\n",
    "    ndvi_images = np.zeros([711, M, N])\n",
    "    rain_images = np.zeros([711, M, N])\n",
    "\n",
    "    totalNumber = 0\n",
    "    outer = 0\n",
    "    for a in range(0,1):\n",
    "        for b in range(0, 13):\n",
    "            counter = 0\n",
    "            if os.path.isdir(\"combined_images/combine_\" + str(a) + \"_\" + str(b)):\n",
    "                filenames = []\n",
    "                for filename in os.listdir(\"combined_images/combine_\" + str(a) + \"_\" + str(b)): \n",
    "                    if filename[len(filename)-6:len(filename)-4] == \"1d\":\n",
    "                        filenames.append(filename)\n",
    "                filenames.sort()\n",
    "                for filename in filenames:\n",
    "                    arr = np.load(\"combined_images/combine_\" + str(a) + \"_\" + str(b) + \"/\" + filename)\n",
    "                    if counter<len(satellite_images[outer]):\n",
    "                        satellite_images[outer][counter][0] = normalize_np(arr[0], ndviMean, ndviStdDev)\n",
    "                        satellite_images[outer][counter][1] = arr[1]/0.7#(arr[1] - 0.35)*(0.5/0.35) #normalize_np(arr[1], rainMean, rainStdDev)\n",
    "                        print(totalNumber)\n",
    "                        ndvi_images[totalNumber] = normalize_np(arr[0], ndviMean, ndviStdDev)\n",
    "                        rain_images[totalNumber] = arr[1]/0.7 #normalize_np(arr[1], rainMean, rainStdDev)\n",
    "                    counter +=1\n",
    "                    totalNumber += 1\n",
    "                print(str(a) + \" : \" + str(b))\n",
    "\n",
    "    list1 = produceRandomImageArray()\n",
    "\n",
    "    main(list1)\n",
    "\n",
    "\n",
    "def schedule(i):\n",
    "    return 0.0002\n",
    "\n",
    "def MAPE(correct, prediction):\n",
    "    return np.sum(np.absolute(correct-prediction)/correct)/100\n",
    "    correct[correct == 0] = 0.000001\n",
    "    prediction[prediction == 0] = 0.000001\n",
    "\n",
    "def main(indexGeneralList):\n",
    "    #initiate training process etc\n",
    "    global stdev\n",
    "    global mean\n",
    "    global learning_rate\n",
    "\n",
    "    maxNdvi = 0\n",
    "    maxRain = 0\n",
    "\n",
    "    for i in range(0, T-1):\n",
    "       correct_output = satellite_images[0][i]\n",
    "       maxN = np.max(correct_output[0])\n",
    "       maxR = np.max(correct_output[1])\n",
    "       if maxN > maxNdvi: \n",
    "           maxNdvi = maxN\n",
    "       if maxR > maxRain:\n",
    "           maxRain = maxR\n",
    "       print(\"ndvi: \" + str(maxN))\n",
    "       print(\"rain: \" + str(maxR))\n",
    "    print(\"maxNdvi: \" + str(maxNdvi))\n",
    "    print(\"maxRain: \" + str(maxRain))\n",
    "\n",
    "    indexList = indexGeneralList[0:439] \n",
    "    validateList = indexGeneralList[439:564]\n",
    "    testList = indexGeneralList[564:627]\n",
    "    \n",
    "\n",
    "    for e in range(0, 10):\n",
    "        random.shuffle(indexList)\n",
    "        for i in range (0, len(indexList)):\n",
    "            #folder = random.randint(0, 8)\n",
    "            imageSatCurrent = indexList[i]\n",
    "            folder = 0\n",
    "            # (i+1) is the length of our time series data\n",
    "            print(\"testing example: -----------------------------------------\" + str(i+1))\n",
    "            print(folder)\n",
    "            print(\"LEARNING RATE: \" + str(learning_rate))\n",
    "            currentIndex = imageSatCurrent.index\n",
    "            if imageSatCurrent.satellite == \"SAME\":\n",
    "                if currentIndex + learning_window < len(satellite_images[folder]):\n",
    "                    input = satellite_images[folder][currentIndex:(currentIndex+learning_window)]\n",
    "\n",
    "                    correct_output = satellite_images[folder][currentIndex+learning_window: currentIndex + learning_window + steps_ahead]\n",
    "                    print(correct_output.shape)\n",
    "\n",
    "                    first = False\n",
    "                    if i == 0:\n",
    "                        first = True\n",
    "\n",
    "                    bptt(input, correct_output, i, learning_window, folder, first, currentIndex, \"TERRA\")\n",
    "\n",
    "            if i%50 == 0:\n",
    "                print(\"-------------------Weight Matrix----------------\")\n",
    "                np.save('5main_kernelfinal3', cp.asnumpy(main_kernel))\n",
    "                print(\"------------------connected_weights---------------------\")\n",
    "                np.save('5connected_weightsfinal3', cp.asnumpy(connected_weights))\n",
    "                print(\"-------------------g_kernel-------------------------\")\n",
    "                np.save('5g_kernelfinal3', cp.asnumpy(g_kernel))\n",
    "                print(\"-------------------------bias_g--------------------\")\n",
    "                np.save('5bias_gfinal3', cp.asnumpy(bias_g))\n",
    "                print(\"-------------------------bias_n--------------------\")\n",
    "                np.save('5bias_nfinal3', cp.asnumpy(bias_n))\n",
    "                print(\"-------------------bias_y-------------------------\")\n",
    "                np.save('5bias_yfinal3', cp.asnumpy(bias_y))\n",
    "                print(\"----------------------bias_o-----------------------\")\n",
    "                np.save('5bias_ofinal3', cp.asnumpy(bias_o))\n",
    "                print(\"-------------------bias_c-------------------------\")\n",
    "                np.save('5bias_cfinal3', cp.asnumpy(bias_c))\n",
    "                print(\"----------------------bias_f------------------\")\n",
    "                np.save('5bias_ffinal3', cp.asnumpy(bias_f))\n",
    "                print(\"-----------------------bias_i-------------------\")\n",
    "                np.save('5bias_ifinal3', cp.asnumpy(bias_i))\n",
    "                print(\"-----------------------v_connected_weights-------------------\")\n",
    "                np.save('5v_connected_weightsfinal3', cp.asnumpy(v_connected_weights))\n",
    "                print(\"----------------------bias_o-----------------------\")\n",
    "                np.save('5bias_oEncodefinal3', cp.asnumpy(bias_oEncode))\n",
    "                print(\"-------------------bias_c-------------------------\")\n",
    "                np.save('5bias_cEncodefinal3', cp.asnumpy(bias_cEncode))\n",
    "                print(\"----------------------bias_f------------------\")\n",
    "                np.save('5bias_fEncodefinal3', cp.asnumpy(bias_fEncode))\n",
    "                print(\"-----------------------bias_i-------------------\")\n",
    "                np.save('5bias_iEncodefinal3', cp.asnumpy(bias_iEncode))\n",
    "                print(\"-------------------Weight Matrix----------------\")\n",
    "                np.save('5main_kernelEncodefinal3', cp.asnumpy(main_kernelEncode))\n",
    "                        \n",
    "        validate(validateList)\n",
    "    test(testList)\n",
    "\n",
    "def produceRandomImageArray():\n",
    "    list = []\n",
    "    for i in range(55, 682):\n",
    "        list.append(make_ImageSat(i, \"SAME\"))\n",
    "\n",
    "    random.shuffle(list)        \n",
    "    return list\n",
    "\n",
    "\n",
    "def test(testList):\n",
    "    global connected_weights\n",
    "    global main_kernel\n",
    "    global bias_i\n",
    "    global bias_f\n",
    "    global bias_c\n",
    "    global bias_o\n",
    "    global bias_y\n",
    "\n",
    "    global bias_n\n",
    "    global bias_g\n",
    "    global g_kernel\n",
    "    global v_connected_weights\n",
    "\n",
    "    global bias_oEncode\n",
    "    global bias_cEncode\n",
    "    global bias_fEncode\n",
    "    global bias_iEncode\n",
    "    global main_kernelEncode\n",
    "\n",
    "\n",
    "    connected_weights = cp.asarray(np.load('5connected_weightsfinal3.npy'))\n",
    "    main_kernel = cp.asarray(np.load('5main_kernelfinal3.npy'))\n",
    "    bias_y = cp.asarray(np.load('5bias_yfinal3.npy'))\n",
    "    bias_o = cp.asarray(np.load('5bias_ofinal3.npy'))\n",
    "    bias_c = cp.asarray(np.load('5bias_cfinal3.npy'))\n",
    "    bias_f = cp.asarray(np.load('5bias_ffinal3.npy'))\n",
    "    bias_i = cp.asarray(np.load('5bias_ifinal3.npy'))\n",
    "\n",
    "    bias_n = cp.asarray(np.load('5bias_nfinal3.npy'))\n",
    "    bias_g = cp.asarray(np.load('5bias_gfinal3.npy'))\n",
    "    g_kernel = cp.asarray(np.load('5g_kernelfinal3.npy'))\n",
    "    v_connected_weights = cp.asarray(np.load('5v_connected_weightsfinal3.npy'))\n",
    "\n",
    "    bias_oEncode = cp.asarray(np.load('5bias_oEncodefinal3.npy'))\n",
    "    bias_cEncode = cp.asarray(np.load('5bias_cEncodefinal3.npy'))\n",
    "    bias_fEncode = cp.asarray(np.load('5bias_fEncodefinal3.npy'))\n",
    "    bias_iEncode = cp.asarray(np.load('5bias_iEncodefinal3.npy'))\n",
    "    main_kernelEncode = cp.asarray(np.load('5main_kernelEncodefinal3.npy'))\n",
    "\n",
    "    global learning_rate\n",
    "    global prev_validate\n",
    "\n",
    "    average = 0\n",
    "\n",
    "    sumSquareError1 = np.zeros([M,N])\n",
    "    sumSquareError2 = np.zeros([M,N])\n",
    "    sumSquareError3 = np.zeros([M,N])\n",
    "    sumSquareError4 = np.zeros([M,N])\n",
    "    \n",
    "    for i in range (0, len(testList)):\n",
    "        #folder = random.randint(0, 8)\n",
    "        imageSatCurrent = testList[i]\n",
    "        folder = 0\n",
    "        currentIndex = imageSatCurrent.index\n",
    "        if imageSatCurrent.satellite == \"SAME\":\n",
    "            if currentIndex + learning_window + steps_ahead< len(satellite_images[folder]):\n",
    "                input = satellite_images[folder][currentIndex:(currentIndex+learning_window)]\n",
    "\n",
    "                correct_output1 = satellite_images[folder][currentIndex+learning_window]\n",
    "                correct_output2 = satellite_images[folder][currentIndex+learning_window + 1]\n",
    "                correct_output3 = satellite_images[folder][currentIndex+learning_window + 2]\n",
    "                correct_output4 = satellite_images[folder][currentIndex+learning_window + 3]\n",
    "\n",
    "                print(str(np.max(correct_output1[0])) + \" max NDVI\")\n",
    "                print(str(np.min(correct_output1[1])) + \" max rain\")\n",
    "\n",
    "                roundArr = return_forecast(cp.asarray(input), learning_window, currentIndex)\n",
    "            \n",
    "                true_prediction1 = unnormalize_np(correct_output1[0], ndviMean, ndviStdDev)\n",
    "                actual_prediction1 = unnormalize_np(roundArr[0], ndviMean, ndviStdDev)\n",
    "\n",
    "                true_prediction2 = unnormalize_np(correct_output2[0], ndviMean, ndviStdDev)\n",
    "                actual_prediction2 = unnormalize_np(roundArr[1], ndviMean, ndviStdDev)\n",
    "\n",
    "                true_prediction3 = unnormalize_np(correct_output3[0], ndviMean, ndviStdDev)\n",
    "                actual_prediction3 = unnormalize_np(roundArr[2], ndviMean, ndviStdDev)\n",
    "\n",
    "                true_prediction4 = unnormalize_np(correct_output4[0], ndviMean, ndviStdDev)\n",
    "                actual_prediction4 = unnormalize_np(roundArr[3], ndviMean, ndviStdDev)\n",
    "\n",
    "                f2 = open(\"test1.txt\", \"a\")\n",
    "                f2.write(str(rootmeansquare(true_prediction1, actual_prediction1)))\n",
    "                f2.write(\"\\n\")\n",
    "\n",
    "                f2 = open(\"test2.txt\", \"a\")\n",
    "                f2.write(str(rootmeansquare(true_prediction2, actual_prediction2)))\n",
    "                f2.write(\"\\n\")\n",
    "\n",
    "                f2 = open(\"test3.txt\", \"a\")\n",
    "                f2.write(str(rootmeansquare(true_prediction3, actual_prediction3)))\n",
    "                f2.write(\"\\n\")\n",
    "\n",
    "                f2 = open(\"test4.txt\", \"a\")\n",
    "                f2.write(str(rootmeansquare(true_prediction4, actual_prediction4)))\n",
    "                f2.write(\"\\n\")                \n",
    "\n",
    "                average += rootmeansquare(true_prediction1, actual_prediction1) + rootmeansquare(true_prediction2, actual_prediction2) + rootmeansquare(true_prediction3, actual_prediction3) + rootmeansquare(true_prediction4, actual_prediction4)\n",
    "\n",
    "                sumSquareError1 = sumSquareError1 + (true_prediction1 - actual_prediction1)**2\n",
    "                sumSquareError2 = sumSquareError2 + (true_prediction2 - actual_prediction2)**2\n",
    "                sumSquareError3 = sumSquareError3 + (true_prediction3 - actual_prediction3)**2\n",
    "                sumSquareError4 = sumSquareError4 + (true_prediction4 - actual_prediction4)**2\n",
    "\n",
    "    average = average/(137*4)\n",
    "    if average>prev_validate:\n",
    "        learning_rate = learning_rate * 0.8\n",
    "\n",
    "    prev_validate = average\n",
    "\n",
    "    sumSquareError1 = np.sqrt(sumSquareError1/len(validateList))\n",
    "    finalValue1 = np.sum(sumSquareError1)/10000\n",
    "\n",
    "    sumSquareError2 = np.sqrt(sumSquareError2/len(validateList))\n",
    "    finalValue2 = np.sum(sumSquareError2)/10000\n",
    "\n",
    "    sumSquareError3 = np.sqrt(sumSquareError3/len(validateList))\n",
    "    finalValue3 = np.sum(sumSquareError3)/10000\n",
    "\n",
    "    sumSquareError4 = np.sqrt(sumSquareError4/len(validateList))\n",
    "    finalValue4 = np.sum(sumSquareError4)/10000\n",
    "\n",
    "    f2 = open(\"test21.txt\", \"a\")\n",
    "    f2.write(str(finalValue1) + \"\\n\")\n",
    "    f2.write(str(np.min(sumSquareError1)) + \"\\n\")\n",
    "    f2.write(str(np.max(sumSquareError1)) + \"\\n\")\n",
    "    f2.write(\"-----------------------------------------------END OF EPOCH-------------------------------------------\")\n",
    "    f2.write(\"\\n\")\n",
    "\n",
    "    f2 = open(\"test22.txt\", \"a\")\n",
    "    f2.write(str(finalValue2) + \"\\n\")\n",
    "    f2.write(str(np.min(sumSquareError2)) + \"\\n\")\n",
    "    f2.write(str(np.max(sumSquareError2)) + \"\\n\")\n",
    "    f2.write(\"-----------------------------------------------END OF EPOCH-------------------------------------------\")\n",
    "    f2.write(\"\\n\")\n",
    "\n",
    "    f2 = open(\"test23.txt\", \"a\")\n",
    "    f2.write(str(finalValue3) + \"\\n\")\n",
    "    f2.write(str(np.min(sumSquareError3)) + \"\\n\")\n",
    "    f2.write(str(np.max(sumSquareError3)) + \"\\n\")\n",
    "    f2.write(\"-----------------------------------------------END OF EPOCH-------------------------------------------\")\n",
    "    f2.write(\"\\n\")\n",
    "\n",
    "    f2 = open(\"test24.txt\", \"a\")\n",
    "    f2.write(str(finalValue4) + \"\\n\")\n",
    "    f2.write(str(np.min(sumSquareError4)) + \"\\n\")\n",
    "    f2.write(str(np.max(sumSquareError4)) + \"\\n\")\n",
    "    f2.write(\"-----------------------------------------------END OF EPOCH-------------------------------------------\")\n",
    "    f2.write(\"\\n\")\n",
    "\n",
    "    f2 = open(\"test1.txt\", \"a\")\n",
    "    f2.write(\"-----------------------------------------------END OF EPOCH-------------------------------------------\")\n",
    "    f2.write(\"\\n\")\n",
    "\n",
    "    f2 = open(\"test2.txt\", \"a\")\n",
    "    f2.write(\"-----------------------------------------------END OF EPOCH-------------------------------------------\")\n",
    "    f2.write(\"\\n\")\n",
    "\n",
    "    f2 = open(\"test3.txt\", \"a\")\n",
    "    f2.write(\"-----------------------------------------------END OF EPOCH-------------------------------------------\")\n",
    "    f2.write(\"\\n\")\n",
    "\n",
    "    f2 = open(\"test4.txt\", \"a\")\n",
    "    f2.write(\"-----------------------------------------------END OF EPOCH-------------------------------------------\")\n",
    "    f2.write(\"\\n\")\n",
    "\n",
    "\n",
    "def validate(validateList):\n",
    "    global connected_weights\n",
    "    global main_kernel\n",
    "    global bias_i\n",
    "    global bias_f\n",
    "    global bias_c\n",
    "    global bias_o\n",
    "    global bias_y\n",
    "\n",
    "    global bias_n\n",
    "    global bias_g\n",
    "    global g_kernel\n",
    "    global v_connected_weights\n",
    "\n",
    "    global bias_oEncode\n",
    "    global bias_cEncode\n",
    "    global bias_fEncode\n",
    "    global bias_iEncode\n",
    "    global main_kernelEncode\n",
    "\n",
    "\n",
    "    connected_weights = cp.asarray(np.load('5connected_weightsfinal3.npy'))\n",
    "    main_kernel = cp.asarray(np.load('5main_kernelfinal3.npy'))\n",
    "    bias_y = cp.asarray(np.load('5bias_yfinal3.npy'))\n",
    "    bias_o = cp.asarray(np.load('5bias_ofinal3.npy'))\n",
    "    bias_c = cp.asarray(np.load('5bias_cfinal3.npy'))\n",
    "    bias_f = cp.asarray(np.load('5bias_ffinal3.npy'))\n",
    "    bias_i = cp.asarray(np.load('5bias_ifinal3.npy'))\n",
    "\n",
    "    bias_n = cp.asarray(np.load('5bias_nfinal3.npy'))\n",
    "    bias_g = cp.asarray(np.load('5bias_gfinal3.npy'))\n",
    "    g_kernel = cp.asarray(np.load('5g_kernelfinal3.npy'))\n",
    "    v_connected_weights = cp.asarray(np.load('5v_connected_weightsfinal3.npy'))\n",
    "\n",
    "    bias_oEncode = cp.asarray(np.load('5bias_oEncodefinal3.npy'))\n",
    "    bias_cEncode = cp.asarray(np.load('5bias_cEncodefinal3.npy'))\n",
    "    bias_fEncode = cp.asarray(np.load('5bias_fEncodefinal3.npy'))\n",
    "    bias_iEncode = cp.asarray(np.load('5bias_iEncodefinal3.npy'))\n",
    "    main_kernelEncode = cp.asarray(np.load('5main_kernelEncodefinal3.npy'))\n",
    "\n",
    "    global learning_rate\n",
    "    global prev_validate\n",
    "\n",
    "    average = 0\n",
    "\n",
    "    sumSquareError1 = np.zeros([M,N])\n",
    "    sumSquareError2 = np.zeros([M,N])\n",
    "    sumSquareError3 = np.zeros([M,N])\n",
    "    sumSquareError4 = np.zeros([M,N])\n",
    "    \n",
    "    for i in range (0, len(validateList)):\n",
    "        #folder = random.randint(0, 8)\n",
    "        imageSatCurrent = validateList[i]\n",
    "        folder = 0\n",
    "        currentIndex = imageSatCurrent.index\n",
    "        if imageSatCurrent.satellite == \"SAME\":\n",
    "            if currentIndex + learning_window + 4 < len(satellite_images[folder]):\n",
    "                input = satellite_images[folder][currentIndex:(currentIndex+learning_window)]\n",
    "\n",
    "                correct_output1 = satellite_images[folder][currentIndex+learning_window]\n",
    "                correct_output2 = satellite_images[folder][currentIndex+learning_window + 1]\n",
    "                correct_output3 = satellite_images[folder][currentIndex+learning_window + 2]\n",
    "                correct_output4 = satellite_images[folder][currentIndex+learning_window + 3]\n",
    "\n",
    "                print(str(np.max(correct_output1[0])) + \" max NDVI\")\n",
    "                print(str(np.min(correct_output1[1])) + \" max rain\")\n",
    "\n",
    "                roundArr = return_forecast(cp.asarray(input), learning_window, currentIndex)\n",
    "            \n",
    "                true_prediction1 = unnormalize_np(correct_output1[0], ndviMean, ndviStdDev)\n",
    "                actual_prediction1 = unnormalize_np(roundArr[0], ndviMean, ndviStdDev)\n",
    "\n",
    "                true_prediction2 = unnormalize_np(correct_output2[0], ndviMean, ndviStdDev)\n",
    "                actual_prediction2 = unnormalize_np(roundArr[1], ndviMean, ndviStdDev)\n",
    "\n",
    "                true_prediction3 = unnormalize_np(correct_output3[0], ndviMean, ndviStdDev)\n",
    "                actual_prediction3 = unnormalize_np(roundArr[2], ndviMean, ndviStdDev)\n",
    "\n",
    "                true_prediction4 = unnormalize_np(correct_output4[0], ndviMean, ndviStdDev)\n",
    "                actual_prediction4 = unnormalize_np(roundArr[3], ndviMean, ndviStdDev)\n",
    "\n",
    "                f2 = open(\"validate1.txt\", \"a\")\n",
    "                f2.write(str(rootmeansquare(true_prediction1, actual_prediction1)))\n",
    "                f2.write(\"\\n\")\n",
    "\n",
    "                f2 = open(\"validate2.txt\", \"a\")\n",
    "                f2.write(str(rootmeansquare(true_prediction2, actual_prediction2)))\n",
    "                f2.write(\"\\n\")\n",
    "\n",
    "                f2 = open(\"validate3.txt\", \"a\")\n",
    "                f2.write(str(rootmeansquare(true_prediction3, actual_prediction3)))\n",
    "                f2.write(\"\\n\")\n",
    "\n",
    "                f2 = open(\"validate4.txt\", \"a\")\n",
    "                f2.write(str(rootmeansquare(true_prediction4, actual_prediction4)))\n",
    "                f2.write(\"\\n\")                \n",
    "\n",
    "                average += rootmeansquare(true_prediction1, actual_prediction1) + rootmeansquare(true_prediction2, actual_prediction2) + rootmeansquare(true_prediction3, actual_prediction3) + rootmeansquare(true_prediction4, actual_prediction4)\n",
    "\n",
    "                sumSquareError1 = sumSquareError1 + (true_prediction1 - actual_prediction1)**2\n",
    "                sumSquareError2 = sumSquareError2 + (true_prediction2 - actual_prediction2)**2\n",
    "                sumSquareError3 = sumSquareError3 + (true_prediction3 - actual_prediction3)**2\n",
    "                sumSquareError4 = sumSquareError4 + (true_prediction4 - actual_prediction4)**2\n",
    "\n",
    "    average = average/(137*4)\n",
    "    if average>prev_validate:\n",
    "        learning_rate = learning_rate * 0.8\n",
    "\n",
    "    prev_validate = average\n",
    "\n",
    "    sumSquareError1 = np.sqrt(sumSquareError1/len(validateList))\n",
    "    finalValue1 = np.sum(sumSquareError1)/10000\n",
    "\n",
    "    sumSquareError2 = np.sqrt(sumSquareError2/len(validateList))\n",
    "    finalValue2 = np.sum(sumSquareError2)/10000\n",
    "\n",
    "    sumSquareError3 = np.sqrt(sumSquareError3/len(validateList))\n",
    "    finalValue3 = np.sum(sumSquareError3)/10000\n",
    "\n",
    "    sumSquareError4 = np.sqrt(sumSquareError4/len(validateList))\n",
    "    finalValue4 = np.sum(sumSquareError4)/10000\n",
    "\n",
    "    f2 = open(\"validate21.txt\", \"a\")\n",
    "    f2.write(str(finalValue1) + \"\\n\")\n",
    "    f2.write(str(np.min(sumSquareError1)) + \"\\n\")\n",
    "    f2.write(str(np.max(sumSquareError1)) + \"\\n\")\n",
    "    f2.write(\"-----------------------------------------------END OF EPOCH-------------------------------------------\")\n",
    "    f2.write(\"\\n\")\n",
    "\n",
    "    f2 = open(\"validate22.txt\", \"a\")\n",
    "    f2.write(str(finalValue2) + \"\\n\")\n",
    "    f2.write(str(np.min(sumSquareError2)) + \"\\n\")\n",
    "    f2.write(str(np.max(sumSquareError2)) + \"\\n\")\n",
    "    f2.write(\"-----------------------------------------------END OF EPOCH-------------------------------------------\")\n",
    "    f2.write(\"\\n\")\n",
    "\n",
    "    f2 = open(\"validate23.txt\", \"a\")\n",
    "    f2.write(str(finalValue3) + \"\\n\")\n",
    "    f2.write(str(np.min(sumSquareError3)) + \"\\n\")\n",
    "    f2.write(str(np.max(sumSquareError3)) + \"\\n\")\n",
    "    f2.write(\"-----------------------------------------------END OF EPOCH-------------------------------------------\")\n",
    "    f2.write(\"\\n\")\n",
    "\n",
    "    f2 = open(\"validate24.txt\", \"a\")\n",
    "    f2.write(str(finalValue4) + \"\\n\")\n",
    "    f2.write(str(np.min(sumSquareError4)) + \"\\n\")\n",
    "    f2.write(str(np.max(sumSquareError4)) + \"\\n\")\n",
    "    f2.write(\"-----------------------------------------------END OF EPOCH-------------------------------------------\")\n",
    "    f2.write(\"\\n\")\n",
    "\n",
    "    f2 = open(\"validate1.txt\", \"a\")\n",
    "    f2.write(\"-----------------------------------------------END OF EPOCH-------------------------------------------\")\n",
    "    f2.write(\"\\n\")\n",
    "\n",
    "    f2 = open(\"validate2.txt\", \"a\")\n",
    "    f2.write(\"-----------------------------------------------END OF EPOCH-------------------------------------------\")\n",
    "    f2.write(\"\\n\")\n",
    "\n",
    "    f2 = open(\"validate3.txt\", \"a\")\n",
    "    f2.write(\"-----------------------------------------------END OF EPOCH-------------------------------------------\")\n",
    "    f2.write(\"\\n\")\n",
    "\n",
    "    f2 = open(\"validate4.txt\", \"a\")\n",
    "    f2.write(\"-----------------------------------------------END OF EPOCH-------------------------------------------\")\n",
    "    f2.write(\"\\n\")\n",
    "\n",
    "loadData()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee7bc9f-144f-45ea-b9b3-3d771565fa6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
