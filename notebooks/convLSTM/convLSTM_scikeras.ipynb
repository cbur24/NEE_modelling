{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bf2c6f1-ab93-4740-9050-d0cfe7eeca63",
   "metadata": {},
   "source": [
    "# Applying convLSTM to gridded data\n",
    "\n",
    "resources\n",
    "\n",
    "- https://github.com/rehaanahmad2013/ConvLSTM_NDVI_Repo\n",
    "- https://github.com/sladewinter/ConvLSTM/blob/master/Training.ipynb\n",
    "- https://sladewinter.medium.com/video-frame-prediction-using-convlstm-network-in-pytorch-b5210a6ce582\n",
    "- https://keras.io/examples/vision/conv_lstm/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38c7d647-ebfc-4938-b543-3b9881a09b93",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'chainer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Rehaan, Brian\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# The following code is a numpy (cupy for gpu) implementation of a ConvLSTM built\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# for forecasting the next image in a sequence of antecedent NDVI and rain. The\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# reconstruct the data by using its forecast from antecedent data and replace \u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# the low quality image.\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mchainer\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mchainer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'chainer'"
     ]
    }
   ],
   "source": [
    "# Rehaan, Brian\n",
    "# The following code is a numpy (cupy for gpu) implementation of a ConvLSTM built\n",
    "# for forecasting the next image in a sequence of antecedent NDVI and rain. The\n",
    "# ConvLSTM also takes in a cloudmasks, and only trained on sufficiently high \n",
    "# quality pixels. If it is a particularly cloudy day, the ConvLSTM will simply\n",
    "# reconstruct the data by using its forecast from antecedent data and replace \n",
    "# the low quality image.\n",
    "\n",
    "import math\n",
    "import chainer\n",
    "import numpy as np\n",
    "import chainer.functions as F\n",
    "import copy\n",
    "import cupy as cp\n",
    "import sys\n",
    "import random\n",
    "import matplotlib.pyplot as plt        \n",
    "import urllib\n",
    "import zipfile\n",
    "import os\n",
    "from scipy.interpolate import UnivariateSpline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf2e9f1-8a21-4a92-8505-13495c7981f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "\n",
    "# number of sets of images\n",
    "S = 10\n",
    "\n",
    "# number of images per sequence/set\n",
    "T = 410\n",
    "\n",
    "# dimensions of the image\n",
    "M = 100\n",
    "N = 100\n",
    "\n",
    "channels_img = 2  # antecedent NDVI and rain\n",
    "channels_hidden = 16\n",
    "kernel_dimension = 5\n",
    "pad_constant = 2\n",
    "\n",
    "loss_clip_constant = 12\n",
    "\n",
    "ndviMean = 0.66673688145266\n",
    "ndviStdDev = 0.16560766237944935\n",
    "rainMean = 0.19636724781555773\n",
    "rainStdDev = 0.16560766237944935\n",
    "\n",
    "# ndviMean = 0.10724276129701694\n",
    "# rainMean = 0.018842877488562778\n",
    "# ndviStdDev =  0.03155192804492544\n",
    "# rainStdDev = 0.023218907256230225\n",
    "TOTAL_DATA = 755\n",
    "usableData = 0\n",
    "\n",
    "satellite_images = np.empty([S, 711, channels_img, M, N])\n",
    "learning_window = 50\n",
    "cloud_masks = np.zeros([TOTAL_DATA, M, N])\n",
    "\n",
    "prev_validate = 100\n",
    "clip_threshold = 5\n",
    "clip_threshold_output = 1\n",
    "IMAGE_RECONSTRUCT = False\n",
    "\n",
    "#-----------------------------------GLOROT INTIALIZATION------------------------------  \n",
    "r_kernel_tanh = math.sqrt(6/((channels_hidden+channels_img)*(kernel_dimension)*(kernel_dimension) + channels_hidden))\n",
    "r_kernel_sigmoid = math.sqrt(6/((channels_hidden+channels_img)*(kernel_dimension)*(kernel_dimension) + channels_hidden))\n",
    "r_connected_weights = .6*math.sqrt(6/(channels_hidden + 1))\n",
    "\n",
    "a_kernel = cp.random.uniform(-r_kernel_tanh, r_kernel_tanh, (channels_hidden, channels_img + channels_hidden, kernel_dimension, kernel_dimension))\n",
    "i_kernel = cp.random.uniform(-r_kernel_sigmoid, r_kernel_sigmoid, (channels_hidden, channels_img + channels_hidden, kernel_dimension, kernel_dimension))\n",
    "f_kernel = cp.random.uniform(-r_kernel_sigmoid, r_kernel_sigmoid, (channels_hidden, channels_img + channels_hidden, kernel_dimension, kernel_dimension))\n",
    "o_kernel = cp.random.uniform(-r_kernel_sigmoid, r_kernel_sigmoid, (channels_hidden, channels_img + channels_hidden, kernel_dimension, kernel_dimension))\n",
    "connected_weights = cp.random.normal(-r_connected_weights, r_connected_weights, (1, channels_hidden))\n",
    "main_kernel = cp.concatenate((i_kernel, f_kernel, a_kernel, o_kernel))\n",
    "\n",
    "\n",
    "#--------------------------------------BIAS INITIALIZATION-----------------------------\n",
    "bias_c = cp.zeros([channels_hidden, M, N])\n",
    "bias_i = cp.zeros([channels_hidden, M, N])\n",
    "bias_f = cp.ones([channels_hidden, M, N])\n",
    "bias_o = cp.zeros([channels_hidden, M, N])\n",
    "bias_y = cp.zeros([channels_img, M, N])\n",
    "\n",
    "learning_rate = 0.002\n",
    "learning_rate_counter = 0\n",
    "PRELOAD_SAVED_WEIGHTS = False\n",
    "\n",
    "#--------------File Paths--------------\n",
    "CLOUD_MASK_ROOT_FOLDER = \"cloudMasks\"\n",
    "DATA_ROOT_FOLDER = \"combineUruguay\"\n",
    "\n",
    "#--------------data structure used allowing to us to process data from both Terra and Aqua satellites--------------\n",
    "class ImageSat(object):\n",
    "    index = 0\n",
    "    satellite = \"SAME\"\n",
    "\n",
    "    def __init__(self, index, satellite):\n",
    "        self.index = index\n",
    "        self.satellite = satellite\n",
    "\n",
    "def make_ImageSat(index, satellite):\n",
    "    imageSat = ImageSat(index, satellite)\n",
    "    return imageSat\n",
    "\n",
    "#-----------helper functions for foreward prop and computing gradients in backprop----------  \n",
    "def sigmoid(k):\n",
    "    return 1 / (1 + cp.exp(-k))\n",
    "\n",
    "def sigmoid_derivative(k):\n",
    "    return sigmoid(k) * (1 - sigmoid(k))\n",
    "\n",
    "def bipolar_sigmoid(k):\n",
    "    return 2 / (1 + cp.exp(-k)) - cp.ones(k.shape)\n",
    "\n",
    "def bipolar_derivative(k):\n",
    "    return (1 - (bipolar_sigmoid(k)**2))/2\n",
    "\n",
    "def tanh(k):\n",
    "    return cp.tanh(k)\n",
    "\n",
    "def tanh_derivative(k):\n",
    "    return 1 - (tanh(k))**2\n",
    "\n",
    "def expdecay(x):\n",
    "    return distance/(1+cp.exp(-0.04*x))\n",
    "\n",
    "def rect_linear_exponential(arr):\n",
    "    arr2 = copy.deepcopy(arr)\n",
    "    arr2 = expdecay(arr2)\n",
    "    return arr2\n",
    "\n",
    "def normalize_np(arr, mean, stddev):\n",
    "    return (arr+0.3)/1.3\n",
    "    #return 1/(1+np.exp(-(arr-mean)/stddev))\n",
    "\n",
    "def unnormalize_np(arr, mean, stddev):\n",
    "    return arr*1.3 - 0.3\n",
    "    #return mean - stddev*np.log((1-arr)/arr)\n",
    "\n",
    "def normalize_cp(arr, mean, stddev):\n",
    "    return (arr+0.3)/1.3\n",
    "    #return 1/(1+cp.exp(-(arr-mean)/stddev))\n",
    "\n",
    "def unnormalize_cp(arr, mean, stddev):\n",
    "    return arr*1.3 - 0.3\n",
    "    #return mean - stddev*cp.log((1-arr)/arr)\n",
    "\n",
    "def rect_linear_exponential_derivative(arr):\n",
    "    arr2 = copy.deepcopy(arr)\n",
    "    derivatives = 0.04*115*cp.exp(-0.04*arr2)/((1+cp.exp(-0.04*arr2))**2)\n",
    "    return derivatives\n",
    "\n",
    "def rect_linear(arr):\n",
    "    newArr = copy.deepcopy(arr)\n",
    "    newArr[arr<0] = 0\n",
    "    return newArr\n",
    "\n",
    "def rect_linear_derivative(arr):\n",
    "    newArr = cp.zeros(arr.shape)\n",
    "    newArr[arr>0] = 1\n",
    "    return newArr\n",
    "\n",
    "# x[t] is the input at time t\n",
    "def forward_prop(x,local_time, currentIndex):\n",
    "    global cloud_masks\n",
    "    for t in np.arange(local_time):\n",
    "        print(np.mean(cloud_masks[currentIndex + t]))\n",
    "        if np.mean(cloud_masks[currentIndex + t]) < 0.82 and IMAGE_RECONSTRUCT == True and (currentIndex + t - learning_window) > 0:\n",
    "            print(\"-------------------------------------HIGH CLOUD DENSITY...----------------------------------\")\n",
    "            print(\"------------------------------------RECONSTRUCTING IMAGE...---------------------------------\")\n",
    "            prediction6, pre_sigmoid_prediction6, hidden_prediction6, i6, f6, a6, c6, o6, h6 = forward_prop(cp.asarray(satellite_images[0][currentIndex + t - learning_window:currentIndex + t]), local_time, currentIndex + t - learning_window)\n",
    "            x[t] = prediction6\n",
    "\n",
    "    # Input Gate\n",
    "    i = cp.empty([local_time, channels_hidden, M, N])\n",
    "\n",
    "    # Forget Gate\n",
    "    f = cp.empty([local_time, channels_hidden, M, N])\n",
    "\n",
    "    # Memory\n",
    "    a = cp.empty([local_time, channels_hidden, M, N])\n",
    "\n",
    "    # Cell Gate\n",
    "    c = cp.empty([local_time + 1, channels_hidden, M, N])\n",
    "    c[-1] = cp.zeros([channels_hidden, M, N])\n",
    "    # Output Gate\n",
    "    o = cp.empty([local_time, channels_hidden, M, N])\n",
    "\n",
    "    # Hidden Unit\n",
    "    h = cp.empty([local_time + 1, channels_hidden, M, N])\n",
    "    h[-1] = cp.zeros([channels_hidden, M, N])\n",
    "    # LSTM FORWARD PROPAGATION\n",
    "    for t in np.arange(local_time):\n",
    "        temporary = cp.concatenate((x[t], h[t - 1]), axis=0)\n",
    "        temporary = temporary.reshape(1, channels_img + channels_hidden, M, N)\n",
    "\n",
    "        i[t] = sigmoid(cp.asarray(F.convolution_2d(temporary, main_kernel[0:channels_hidden], b=None, pad=pad_constant)[0].data) + bias_i)\n",
    "\n",
    "        f[t] = sigmoid(cp.asarray(F.convolution_2d(temporary, main_kernel[channels_hidden:2*channels_hidden], b=None, pad=pad_constant)[0].data) + bias_f)\n",
    "\n",
    "        a[t] = tanh(cp.asarray(F.convolution_2d(temporary, main_kernel[2*channels_hidden:3*channels_hidden], b=None, pad=pad_constant)[0].data) + bias_c)\n",
    "\n",
    "        c[t] = cp.multiply(f[t], c[t - 1]) + cp.multiply(i[t], a[t])\n",
    "\n",
    "        o[t] = sigmoid(cp.asarray(F.convolution_2d(temporary, main_kernel[3*channels_hidden:4*channels_hidden], b=None, pad=pad_constant)[0].data) + bias_o)\n",
    "\n",
    "        h[t] = cp.multiply(o[t], tanh(c[t]))\n",
    "\n",
    "    # 1 x 1 convolution\n",
    "    #output = cp.matmul(connected_weights, h[local_time-1].reshape(channels_hidden, M * N)).reshape(M, N) + bias_y[0]\n",
    "    output = cp.asarray(F.convolution_2d(h[local_time-1].reshape(1, channels_hidden, M, N), connected_weights.reshape(1, channels_hidden, 1, 1), b = None, pad = 0)[0][0].data) + bias_y[0]\n",
    "    print(\"CONNECTED_WEIGHTS NORM: \" + str(cp.linalg.norm(connected_weights)))\n",
    "    print(\"HIDDEN_PREDICTION NORM: \" + str(cp.linalg.norm(h[local_time-1])))\n",
    "    print(\"CONNECTED_WEIGHTS MEAN: \" + str(cp.mean(cp.abs(connected_weights))))\n",
    "    print(\"HIDDEN_PREDICTION MEAN: \" + str(cp.mean(cp.abs(h[local_time-1]))))\n",
    "    true_output = sigmoid(output)\n",
    "    return true_output, output, cp.reshape(h[local_time-1], (channels_hidden, M*N)), i, f, a, c, o, h\n",
    "\n",
    "\n",
    "\n",
    "def calculate_loss2(prediction, y):\n",
    "  #  prediction[prediction<0.1] = 0.00000001\n",
    "    return -np.sum(np.multiply(y, np.log(prediction)) + np.multiply(np.ones(y.shape) - y, np.log(np.ones(y.shape) - prediction)))\n",
    "\n",
    "#root mean square error\n",
    "def rootmeansquare(prediction, y):\n",
    "    return cp.sqrt(cp.sum((prediction - y)**2)/(np.count_nonzero(cp.asnumpy(prediction))))\n",
    "\n",
    "# Calculate loss.\n",
    "#loss function is MSE, since we are comparing two images. \n",
    "def calculate_loss(prediction, y):\n",
    "    lossExpression = 0.5*cp.sum((prediction - y)**2)\n",
    "    return lossExpression\n",
    "\n",
    "def calculate_loss_modified(prediction, y):\n",
    "    prediction[prediction == 0] = 0.00000001\n",
    "    y[y == 0] = 0.00000001\n",
    "    lossExpression = -cp.sum(cp.multiply(y, cp.log(prediction)) + cp.multiply(cp.ones(y.shape) - y, cp.log(cp.ones(y.shape) - prediction)))\n",
    "    return lossExpression\n",
    "\n",
    "def return_forecast(x, local_time, currentIndex):\n",
    "    a,b,c,d,e,f,g,h,i = forward_prop(cp.asarray(x), local_time, currentIndex)\n",
    "    return cp.asnumpy(a)\n",
    "\n",
    "def loss_derivative(x, y):\n",
    "    return (x-y)\n",
    "  \n",
    "#backpropagation through time (bptt) algorithm.\n",
    "def bptt(x2, y2, iteration, local_time, currentIndex):\n",
    "    x = cp.asarray(x2)\n",
    "    y = cp.asarray(y2)\n",
    "\n",
    "    global connected_weights\n",
    "    global main_kernel\n",
    "    global bias_i\n",
    "    global bias_f\n",
    "    global bias_c\n",
    "    global bias_o\n",
    "    global bias_y\n",
    "\n",
    "    global learning_rate\n",
    "    global learning_rate_counter\n",
    "\n",
    "    # Perform forward prop\n",
    "    prediction, pre_sigmoid_prediction, hidden_prediction, i, f, a, c, o, h = forward_prop(x, local_time, currentIndex)\n",
    "\n",
    "    predictionLoss = unnormalize_cp(prediction, 0, 0)\n",
    "    outputLoss = unnormalize_cp(y[0], 0, 0)\n",
    "\n",
    "    prediction = prediction*cp.asarray(cloud_masks[currentIndex+local_time])\n",
    "    y[0] = y[0]*cp.asarray(cloud_masks[currentIndex+local_time])\n",
    "    predictionLoss = predictionLoss*cp.asarray(cloud_masks[currentIndex+local_time])\n",
    "    outputLoss = outputLoss*cp.asarray(cloud_masks[currentIndex+local_time])\n",
    "\n",
    "    loss = calculate_loss(predictionLoss, outputLoss)\n",
    "    print(\"LOSS BEFORE: \")\n",
    "    print(loss)\n",
    "\n",
    "    # Calculate loss with respect to final layer\n",
    "    dLdy_2 = loss_derivative(prediction, y[0])\n",
    "\n",
    "    f2 = open(\"runtimedata/normlossderivative.txt\", \"a\")\n",
    "    f2.write(str(cp.linalg.norm(dLdy_2)) + \"\\n\")\n",
    "    # Calculate loss with respect to pre sigmoid layer\n",
    "    dLdy_1 = cp.multiply(sigmoid_derivative(pre_sigmoid_prediction), dLdy_2)\n",
    "    \n",
    "    # Calculate loss with respect to last layer of lstm\n",
    "    dLdh = cp.asarray(F.convolution_2d(dLdy_1.reshape(1, 1, M, N), (connected_weights.reshape(1, channels_hidden, 1, 1)).transpose(1,0,2,3), b=None, pad=0)[0].data)\n",
    "    dLdw_0 = cp.asarray(F.convolution_2d(hidden_prediction.reshape(channels_hidden, 1, M, N), dLdy_1.reshape(1, 1, M, N), b=None, pad=0).data).transpose(1,0,2,3)\n",
    "    dLdb_y = dLdy_1\n",
    "    dLdw_0 = dLdw_0.reshape(1, channels_hidden)\n",
    "\n",
    "    # uncomment code below if you would like to gradient clip the output layer.\n",
    "    # if cp.linalg.norm(dLdw_0) > clip_threshold_output:\n",
    "    #    dLdw_0 = dLdw_0*clip_threshold_output/cp.linalg.norm(dLdw_0)\n",
    "    # if cp.linalg.norm(dLdb_y) > clip_threshold_output:\n",
    "    #    dLdb_y = dLdb_y*clip_threshold_output/cp.linalg.norm(dLdb_y)\n",
    "    \n",
    "    #--------------------fully connected------------------\n",
    "    bias_y = bias_y - learning_rate*dLdb_y\n",
    "    connected_weights = connected_weights - learning_rate*dLdw_0\n",
    "\n",
    "    # Initialize weight matrix\n",
    "    dLdW = cp.zeros([4*channels_hidden, channels_img + channels_hidden, kernel_dimension, kernel_dimension])\n",
    "\n",
    "    # initialize biases\n",
    "    dLdb_c = cp.zeros([channels_hidden, M, N])\n",
    "    dLdb_i = cp.zeros([channels_hidden, M, N])\n",
    "    dLdb_f = cp.zeros([channels_hidden, M, N])\n",
    "    dLdb_o = cp.zeros([channels_hidden, M, N])\n",
    "\n",
    "    # Initialize cell matrix\n",
    "    dLdc_current = cp.zeros([channels_hidden, M, N])\n",
    "\n",
    "    for t in cp.arange(local_time - 1, -1, -1):\n",
    "        dLdo = cp.multiply(dLdh, tanh(c[t]))\n",
    "        dLdc_current += cp.multiply(cp.multiply(dLdh, o[t]), (cp.ones((channels_hidden, M, N)) - cp.multiply(tanh(c[t]), tanh(c[t]))))\n",
    "        dLdi = cp.multiply(dLdc_current, a[t])\n",
    "        dLda = cp.multiply(dLdc_current, i[t])\n",
    "        dLdf = cp.multiply(dLdc_current, c[t - 1])\n",
    "\n",
    "        dLdc_previous = cp.multiply(dLdc_current, f[t])\n",
    "\n",
    "        dLda = cp.multiply(dLda, (cp.ones((channels_hidden, M, N)) - cp.multiply(a[t], a[t]))) #dLda_hat\n",
    "\n",
    "        dLdi = cp.multiply(cp.multiply(dLdi, i[t]), cp.ones((channels_hidden, M, N)) - i[t]) #dLdi_hat\n",
    "\n",
    "        dLdf = cp.multiply(cp.multiply(dLdf, f[t]), cp.ones((channels_hidden, M, N)) - f[t]) #dLdf_hat\n",
    "\n",
    "        dLdo = cp.multiply(cp.multiply(dLdo, o[t]), cp.ones((channels_hidden, M, N)) - o[t]) #dLdo_hat\n",
    "\n",
    "\n",
    "        # CONCATENATE Z IN THE RIGHT ORDER SAME ORDER AS THE WEIGHTS\n",
    "        dLdz_hat = cp.concatenate((dLdi, dLdf, dLda, dLdo), axis = 0) \n",
    "\n",
    "        #determine convolution derivatives\n",
    "        #here we will use the fact that in z = w * I, dLdW = dLdz * I\n",
    "        temporary = cp.concatenate((x[t], h[t - 1]), axis=0).reshape(channels_hidden + channels_img, 1, M, N)\n",
    "        dLdI = cp.asarray(F.convolution_2d(dLdz_hat.reshape(1, 4*channels_hidden, M, N), main_kernel.transpose(1, 0, 2, 3), b=None, pad=pad_constant)[0].data) # reshape into flipped kernel dimensions\n",
    "        dLdW_temp = cp.asarray((F.convolution_2d(temporary, dLdz_hat.reshape(4*channels_hidden, 1, M, N), b=None, pad=pad_constant).data).transpose(1,0,2,3)) #reshape into kernel dimensions\n",
    " \n",
    "        # accumulate derivatives of weights and biases\n",
    "        dLdW += dLdW_temp \n",
    "        dLdb_c += dLda\n",
    "        dLdb_i += dLdi\n",
    "        dLdb_f += dLdf\n",
    "        dLdb_o += dLdo\n",
    "\n",
    "        # reinitialize what you're passing back\n",
    "        dLdh = dLdI[channels_img: channels_img+channels_hidden] \n",
    "        dLdc_current = dLdc_previous\n",
    "\n",
    "    # #Clip all gradients again\n",
    "    if cp.linalg.norm(dLdW) > clip_threshold:\n",
    "        dLdW = dLdW*clip_threshold/cp.linalg.norm(dLdW)\n",
    "    if cp.linalg.norm(dLdb_c) > clip_threshold:\n",
    "        dLdb_c = dLdb_c*clip_threshold/cp.linalg.norm(dLdb_c)\n",
    "    if cp.linalg.norm(dLdb_i) > clip_threshold:\n",
    "        dLdb_i = dLdb_i*clip_threshold/cp.linalg.norm(dLdb_i)\n",
    "    if cp.linalg.norm(dLdb_f) > clip_threshold:\n",
    "        dLdb_f = dLdb_f*clip_threshold/cp.linalg.norm(dLdb_f)\n",
    "    if cp.linalg.norm(dLdb_o) > clip_threshold:\n",
    "        dLdb_o = dLdb_o*clip_threshold/cp.linalg.norm(dLdb_o)\n",
    "\n",
    "    #---------------------update main kernel---------\n",
    "    main_kernel = main_kernel - learning_rate*dLdW\n",
    "    #--------------------update bias c-----------------------\n",
    "    bias_c = bias_c - learning_rate*dLdb_c\n",
    "    #--------------------update bias i-----------------------\n",
    "    bias_i = bias_i - learning_rate*dLdb_i\n",
    "    #--------------------update bias f-----------------------\n",
    "    bias_f = bias_f - learning_rate*dLdb_f\n",
    "    #--------------------update bias c-----------------------\n",
    "    bias_o = bias_o - learning_rate*dLdb_o\n",
    "\n",
    "    prediction2, pre_sigmoid_prediction2, hidden_prediction2, i2, f2, a2, c2, o2, h2 = forward_prop(x, local_time, currentIndex)\n",
    "    prediction3 = prediction2*cp.asarray(cloud_masks[currentIndex + local_time])\n",
    "    loss2 = calculate_loss(prediction3, y[0])\n",
    "\n",
    "    prediction2 = unnormalize_cp(prediction2, ndviMean, ndviStdDev)\n",
    "    prediction2 = prediction2*cp.asarray(cloud_masks[currentIndex + local_time])\n",
    "    outputArr = unnormalize_cp(y[0], ndviMean, ndviStdDev)\n",
    "    outputArr = outputArr*cp.asarray(cloud_masks[currentIndex + local_time])\n",
    "    rms3 = rootmeansquare(prediction2, outputArr)\n",
    "\n",
    "    f2 = open(\"runtimedata/loss.txt\", \"a\")\n",
    "    f2.write(str(rms3) + \"\\n\")\n",
    "    \n",
    "\n",
    "    if loss2 > loss:\n",
    "        #sys.exit(\"what\")\n",
    "        f2 = open(\"runtimedata/closeResults.txt\", \"a\")\n",
    "        f2.write(str(iteration))\n",
    "        f2.write(\"\\n\")\n",
    "        learning_rate_counter += 1\n",
    "        if learning_rate_counter == 1:\n",
    "            learning_rate_counter = 0\n",
    "            #learning_rate = learning_rate*0.9\n",
    "        print(\"----------------close------------------------------\")\n",
    "\n",
    "\n",
    "    print(\"backpropagation complete\")\n",
    "\n",
    "def generateCloudMask():\n",
    "    global cloud_masks\n",
    "    totalNumber = 0\n",
    "    for a in range(0, 1):\n",
    "        for b in range(0, 1):\n",
    "            counter = 0\n",
    "            if os.path.isdir(CLOUD_MASK_ROOT_FOLDER):\n",
    "                filenames = []\n",
    "                for filename in os.listdir(CLOUD_MASK_ROOT_FOLDER): \n",
    "                    filenames.append(filename)\n",
    "                filenames.sort()\n",
    "                for filename in filenames:\n",
    "                    arr = np.load(CLOUD_MASK_ROOT_FOLDER + \"/\" + filename)\n",
    "                    if counter<satellite_images.shape[1]:\n",
    "                        #Make 0,1,2,3 cloud mask into a 0-1 cloud mask. \n",
    "                        arr[arr == 3] = 4\n",
    "                        arr[arr == 2] = 4\n",
    "                        arr[arr == 1] = 5\n",
    "                        arr[arr == 0] = 5\n",
    "                        arr[arr == -1] = 0\n",
    "                        arr[arr == 4] = 0\n",
    "                        arr[arr == 5] = 1\n",
    "                        cloud_masks[totalNumber] = arr[0:100, 0:100]\n",
    "                        print(np.mean(cloud_masks[totalNumber]))\n",
    "                    counter += 1\n",
    "                    totalNumber += 1\n",
    "\n",
    "\n",
    "def loadData():\n",
    "    generateCloudMask()\n",
    "\n",
    "    global satellite_images\n",
    "\n",
    "    totalNumber = 0\n",
    "    for a in range(0, 1):\n",
    "        for b in range(0, 1):\n",
    "            counter = 0\n",
    "            if os.path.isdir(DATA_ROOT_FOLDER):\n",
    "                filenames = []\n",
    "                for filename in os.listdir(DATA_ROOT_FOLDER): \n",
    "                    filenames.append(filename)\n",
    "                filenames.sort()\n",
    "                for filename in filenames:\n",
    "                    arr = np.load(DATA_ROOT_FOLDER + \"/\" + filename)\n",
    "                    if counter<satellite_images.shape[1]:\n",
    "                        satellite_images[0][counter][0] = normalize_np(arr[0][0:100, 0:100], ndviMean, ndviStdDev)\n",
    "                        satellite_images[0][counter][1] = arr[1][0:100, 0:100]/214\n",
    "                        counter += 1\n",
    "\n",
    "                    totalNumber += 1\n",
    "            \n",
    "    list1 = produceRandomImageArray()\n",
    "\n",
    "    main(list1)\n",
    "\n",
    "def MAPE(correct, prediction):\n",
    "    return np.sum(np.absolute(correct-prediction)/correct)/100\n",
    "    \n",
    "def main(indexGeneralList):\n",
    "    #initiate training process etc\n",
    "    global stdev\n",
    "    global mean\n",
    "    global learning_rate\n",
    "\n",
    "    global connected_weights\n",
    "    global main_kernel\n",
    "    global bias_i\n",
    "    global bias_f\n",
    "    global bias_c\n",
    "    global bias_o\n",
    "    global bias_y\n",
    "\n",
    "    if PRELOAD_SAVED_WEIGHTS == True:\n",
    "        connected_weights = cp.asarray(np.load('runtimedata/epoch18/5connected_weightsfinal3.npy'))\n",
    "        main_kernel = cp.asarray(np.load('runtimedata/epoch18/5main_kernelfinal3.npy'))\n",
    "        bias_y = cp.asarray(np.load('runtimedata/epoch18/5bias_yfinal3.npy'))\n",
    "        bias_o = cp.asarray(np.load('runtimedata/epoch18/5bias_ofinal3.npy'))\n",
    "        bias_c = cp.asarray(np.load('runtimedata/epoch18/5bias_cfinal3.npy'))\n",
    "        bias_f = cp.asarray(np.load('runtimedata/epoch18/5bias_ffinal3.npy'))\n",
    "        bias_i = cp.asarray(np.load('runtimedata/epoch18/5bias_ifinal3.npy'))\n",
    "\n",
    "    global usableData\n",
    "    usableData = len(indexGeneralList)\n",
    "\n",
    "    indexList = indexGeneralList[0:int(0.7*usableData)] \n",
    "    validateList = indexGeneralList[(int(0.7*usableData)+1):int(0.9*usableData)]\n",
    "    testList = indexGeneralList[(int(0.9*usableData)+1):usableData]\n",
    "    \n",
    "    f2 = open(\"runtimedata/indexListNumbers.txt\", \"a\")\n",
    "    for k in range(0, len(indexList)):\n",
    "        f2.write(str(indexList[k].index) + \" : \" + str(indexList[k].satellite))\n",
    "        f2.write(\"\\n\")\n",
    "\n",
    "    f2 = open(\"runtimedata/validateListNumbers.txt\", \"a\")\n",
    "    for k in range(0, len(validateList)):\n",
    "        f2.write(str(validateList[k].index) + \" : \" + str(validateList[k].satellite))\n",
    "        f2.write(\"\\n\")\n",
    "\n",
    "    f2 = open(\"runtimedata/testListNumbers.txt\", \"a\")\n",
    "    for k in range(0, len(testList)):\n",
    "        f2.write(str(testList[k].index) + \" : \" + str(testList[k].satellite))\n",
    "        f2.write(\"\\n\")\n",
    "\n",
    "    for e in range(0, 20):\n",
    "        random.shuffle(indexList)\n",
    "        os.makedirs(\"C:/Users/Rehaan/Desktop/UruguayData/runtimedata/epoch\" + str(e+19))\n",
    "        for i in range (0, len(indexList)):\n",
    "            #folder = random.randint(0, 8)\n",
    "            imageSatCurrent = indexList[i]\n",
    "            folder = 0\n",
    "            # (i+1) is the length of our time series data\n",
    "            print(\"testing example: -----------------------------------------\" + str(i+1))\n",
    "            print(folder)\n",
    "            print(\"LEARNING RATE: \" + str(learning_rate))\n",
    "            f2 = open(\"runtimedata/learning_rate.txt\", \"a\")\n",
    "            f2.write(str(learning_rate) + \"\\n\")\n",
    "            currentIndex = imageSatCurrent.index\n",
    "            if \"SAME\" == \"SAME\":\n",
    "                if currentIndex + learning_window < len(satellite_images[folder]):\n",
    "                    input = satellite_images[folder][currentIndex:(currentIndex+learning_window)]\n",
    "                    correct_output = satellite_images[folder][currentIndex+learning_window]\n",
    "\n",
    "                    first = False\n",
    "                    if i == 0:\n",
    "                        first = True\n",
    "\n",
    "                    bptt(input, correct_output, 350*e + i, learning_window, currentIndex)\n",
    "\n",
    "            if i%50 == 0 or i == len(indexList) - 1:\n",
    "                print(\"-------------------Weight Matrix----------------\")\n",
    "                np.save('runtimedata/epoch' + str(e) + '/5main_kernelfinal3', cp.asnumpy(main_kernel))\n",
    "                print(\"------------------connected_weights---------------------\")\n",
    "                np.save('runtimedata/epoch' + str(e) + '/5connected_weightsfinal3', cp.asnumpy(connected_weights))\n",
    "                print(\"-------------------bias_y-------------------------\")\n",
    "                np.save('runtimedata/epoch' + str(e) + '/5bias_yfinal3', cp.asnumpy(bias_y))\n",
    "                print(\"----------------------bias_o-----------------------\")\n",
    "                np.save('runtimedata/epoch' + str(e) + '/5bias_ofinal3', cp.asnumpy(bias_o))\n",
    "                print(\"-------------------bias_c-------------------------\")\n",
    "                np.save('runtimedata/epoch' + str(e) + '/5bias_cfinal3', cp.asnumpy(bias_c))\n",
    "                print(\"----------------------bias_f------------------\")\n",
    "                np.save('runtimedata/epoch' + str(e) + '/5bias_ffinal3', cp.asnumpy(bias_f))\n",
    "                print(\"-----------------------bias_i-------------------\")\n",
    "                np.save('runtimedata/epoch' + str(e) + '/5bias_ifinal3', cp.asnumpy(bias_i))\n",
    "\n",
    "        validate(validateList, e)\n",
    "    test(validateList)\n",
    "\n",
    "    \n",
    "\n",
    "def produceRandomImageArray():\n",
    "    global usableData\n",
    "    list = []\n",
    "    print(\"got here 2\")\n",
    "    for i in range(0, TOTAL_DATA - learning_window):\n",
    "        list.append(make_ImageSat(i, \"SAME\"))\n",
    "        print(\"----------------------adding-----------------------------\")\n",
    "        usableData += 1\n",
    "\n",
    "    random.shuffle(list)        \n",
    "    return list\n",
    "\n",
    "def test(testList):\n",
    "    global connected_weights\n",
    "    global main_kernel\n",
    "    global bias_i\n",
    "    global bias_f\n",
    "    global bias_c\n",
    "    global bias_o\n",
    "    global bias_y\n",
    "\n",
    "    connected_weights = cp.asarray(np.load('5connected_weightsfinal3.npy'))\n",
    "    main_kernel = cp.asarray(np.load('5main_kernelfinal3.npy'))\n",
    "    bias_y = cp.asarray(np.load('5bias_yfinal3.npy'))\n",
    "    bias_o = cp.asarray(np.load('5bias_ofinal3.npy'))\n",
    "    bias_c = cp.asarray(np.load('5bias_cfinal3.npy'))\n",
    "    bias_f = cp.asarray(np.load('5bias_ffinal3.npy'))\n",
    "    bias_i = cp.asarray(np.load('5bias_ifinal3.npy'))\n",
    "\n",
    "\n",
    "    sumSquareError = np.zeros([M,N])\n",
    "\n",
    "    for i in range (0, len(testList)):\n",
    "        #folder = random.randint(0, 8)\n",
    "        imageSatCurrent = testList[i]\n",
    "        folder = 0\n",
    "        currentIndex = imageSatCurrent.index\n",
    "        print(\"---------------------WHAT-----------------------\")\n",
    "        print(str(currentIndex))\n",
    "        if imageSatCurrent.satellite == \"SAME\":\n",
    "            if currentIndex + learning_window + 2 < len(satellite_images[folder]):\n",
    "                input = satellite_images[folder][currentIndex:(currentIndex+learning_window)]\n",
    "\n",
    "                correct_output = satellite_images[folder][currentIndex+learning_window]\n",
    "                \n",
    "                roundArr = return_forecast(input, learning_window, currentIndex)\n",
    "\n",
    "                true_prediction = unnormalize_np(correct_output[0], ndviMean, ndviStdDev)\n",
    "                actual_prediction = unnormalize_np(roundArr, ndviMean, ndviStdDev)\n",
    "                \n",
    "                true_prediction = true_prediction*cloud_masks[currentIndex + learning_window]\n",
    "                actual_prediction = actual_prediction*cloud_masks[currentIndex + learning_window]\n",
    "\n",
    "                print(\"RMSE\")\n",
    "                print(rootmeansquare(true_prediction, actual_prediction))\n",
    "\n",
    "                f2 = open(\"runtimedata/testResults.txt\", \"a\")\n",
    "                f2.write(str(rootmeansquare(true_prediction, actual_prediction)))\n",
    "                f2.write(\"\\n\")\n",
    "\n",
    "                sumSquareError = sumSquareError + (true_prediction - actual_prediction)**2\n",
    "\n",
    "    \n",
    "    sumSquareError = np.sqrt(sumSquareError/len(testList))\n",
    "    finalValue = np.sum(sumSquareError)/10000\n",
    "    print(str(finalValue))\n",
    "    print(str(np.min(sumSquareError)))\n",
    "    print(str(np.max(sumSquareError)))\n",
    "\n",
    "\n",
    "def validate(validateList, e):\n",
    "    global connected_weights\n",
    "    global main_kernel\n",
    "    global bias_i\n",
    "    global bias_f\n",
    "    global bias_c\n",
    "    global bias_o\n",
    "    global bias_y\n",
    "\n",
    "    connected_weights = cp.asarray(np.load('runtimedata/epoch' + str(e) + '/5connected_weightsfinal3.npy'))\n",
    "    main_kernel = cp.asarray(np.load('runtimedata/epoch' + str(e) + '/5main_kernelfinal3.npy'))\n",
    "    bias_y = cp.asarray(np.load('runtimedata/epoch' + str(e) + '/5bias_yfinal3.npy'))\n",
    "    bias_o = cp.asarray(np.load('runtimedata/epoch' + str(e) + '/5bias_ofinal3.npy'))\n",
    "    bias_c = cp.asarray(np.load('runtimedata/epoch' + str(e) + '/5bias_cfinal3.npy'))\n",
    "    bias_f = cp.asarray(np.load('runtimedata/epoch' + str(e) + '/5bias_ffinal3.npy'))\n",
    "    bias_i = cp.asarray(np.load('runtimedata/epoch' + str(e) + '/5bias_ifinal3.npy'))\n",
    "\n",
    "    global learning_rate\n",
    "    global prev_validate\n",
    "\n",
    "    average = 0\n",
    "\n",
    "    sumSquareError = np.zeros([M,N])\n",
    "    for i in range (0, len(validateList)):\n",
    "        #folder = random.randint(0, 8)\n",
    "        imageSatCurrent = validateList[i]\n",
    "        folder = 0\n",
    "        currentIndex = imageSatCurrent.index\n",
    "        if imageSatCurrent.satellite == \"SAME\":\n",
    "            if currentIndex + learning_window < len(satellite_images[folder]):\n",
    "                input = satellite_images[folder][currentIndex:(currentIndex+learning_window)]\n",
    "\n",
    "                correct_output = satellite_images[folder][currentIndex+learning_window]\n",
    "                print(str(np.max(correct_output[0])) + \" max NDVI\")\n",
    "                print(str(np.min(correct_output[1])) + \" max rain\")\n",
    "\n",
    "                roundArr = return_forecast(input, learning_window, currentIndex)\n",
    "            \n",
    "                true_prediction = unnormalize_np(correct_output[0], ndviMean, ndviStdDev)\n",
    "                actual_prediction = unnormalize_np(roundArr, ndviMean, ndviStdDev)\n",
    "\n",
    "                true_prediction = true_prediction*cloud_masks[currentIndex + learning_window]\n",
    "                actual_prediction = actual_prediction*cloud_masks[currentIndex + learning_window]\n",
    "                \n",
    "                f2 = open(\"runtimedata/validate1.txt\", \"a\")\n",
    "                f2.write(str(rootmeansquare(true_prediction, actual_prediction)))\n",
    "                f2.write(\"\\n\")\n",
    "\n",
    "\n",
    "                print(\"ROOT MEAN SQUARE: \")\n",
    "                print(rootmeansquare(true_prediction, actual_prediction))\n",
    "\n",
    "                # if rootmeansquare(true_prediction, actual_prediction) < 0.06:\n",
    "                #     np.save(\"goodimage\", true_prediction)\n",
    "                #     np.save(\"predictimage\", actual_prediction)\n",
    "\n",
    "                average += rootmeansquare(true_prediction, actual_prediction)\n",
    "\n",
    "                sumSquareError = sumSquareError + (true_prediction - actual_prediction)**2\n",
    "\n",
    "\n",
    "    average = average/137\n",
    "    if average>prev_validate:\n",
    "        learning_rate = learning_rate * 0.8\n",
    "\n",
    "    prev_validate = average\n",
    "    sumSquareError = np.sqrt(sumSquareError/len(validateList))\n",
    "    finalValue = np.sum(sumSquareError)/10000\n",
    "    f2 = open(\"runtimedata/validate2.txt\", \"a\")\n",
    "    f2.write(str(finalValue) + \"\\n\")\n",
    "    f2.write(str(np.min(sumSquareError)) + \"\\n\")\n",
    "    f2.write(str(np.max(sumSquareError)) + \"\\n\")\n",
    "    f2.write(\"-----------------------------------------------END OF EPOCH-------------------------------------------\")\n",
    "    f2.write(\"\\n\")\n",
    "\n",
    "    f2 = open(\"runtimedata/validate1.txt\", \"a\")\n",
    "    f2.write(\"-----------------------------------------------END OF EPOCH-------------------------------------------\")\n",
    "    f2.write(\"\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3860a43d-7d9e-45b2-b23e-a16b56599d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "loadData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cf90f6-bc7d-49cb-ad5f-f3b7938bf14f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830b0319-5997-4e46-9b67-5997665b4fad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
