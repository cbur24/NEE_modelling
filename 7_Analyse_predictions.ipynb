{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse modelled fluxes\n",
    "\n",
    "- Should also compare results of gridded NEE predictions (correlations etc.) with EC point data to see if they follow the same trends. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from datacube.utils.dask import start_local_dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = start_local_dask(mem_safety_margin='2Gb')\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = 'GPP'\n",
    "results_name = 'GPP_2003_2021_5km_LGBM.nc'\n",
    "data_path = '/g/data/os22/chad_tmp/NEE_modelling/results/prediction_data/data_5km.nc'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open predictor data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = xr.open_dataset(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataarray('/g/data/os22/chad_tmp/NEE_modelling/results/predictions/'+results_name,\n",
    "                       chunks=dict(x=250,y=250, time=-1))#.sel(time=slice('2003','2018'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import xarray as xr\n",
    "import dask.array as da\n",
    "from dask.delayed import delayed\n",
    "from  scipy import stats\n",
    "\n",
    "def _calc_slope(y):\n",
    "    \"\"\"return linear regression statistical variables\"\"\"\n",
    "    mask = np.isfinite(y)\n",
    "    x = np.arange(len(y))\n",
    "    return stats.linregress(x[mask], y[mask])\n",
    "\n",
    "# regression function defition\n",
    "def regression(y):\n",
    "    \"\"\"apply linear regression function along time axis\"\"\"\n",
    "    axis_num = y.get_axis_num('time')\n",
    "    return da.apply_along_axis(_calc_slope, axis_num, y)\n",
    "\n",
    "# fill pixels that are all-NaNs\n",
    "allnans = ds.isnull().all('time').compute()\n",
    "ds = ds.where(~allnans, other=0)\n",
    "\n",
    "# regression analysis\n",
    "delayed_objs = delayed(regression)(ds).persist()\n",
    "\n",
    "# transforms dask.delayed to dask.array\n",
    "results = da.from_delayed(delayed_objs, shape=(5, ds.shape[1:][0], ds.shape[1:][1]), dtype=np.float32)\n",
    "results = results.compute()\n",
    "results = results.compute() #need this twice haven't figured out why\n",
    "\n",
    "# statistical variables definition\n",
    "variables = ['slope', 'intercept', 'r_value', 'p_value', 'std_err']\n",
    "\n",
    "# coordination definition\n",
    "coords = {'y': ds.y, 'x': ds.x}\n",
    "\n",
    "# output xarray.Dataset definition\n",
    "ds_out = xr.Dataset(\n",
    "    data_vars=dict(slope=([\"y\", \"x\"], results[0]),\n",
    "                   intercept=([\"y\", \"x\"], results[1]),\n",
    "                   r_value=([\"y\", \"x\"], results[2]),\n",
    "                   p_value=([\"y\", \"x\"], results[3]),\n",
    "                   std_err=([\"y\", \"x\"], results[4]),\n",
    "                  ),\n",
    "    coords = coords)\n",
    "\n",
    "#remask all-NaN pixel\n",
    "ds_out = ds_out.where(~allnans)\n",
    "ds_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mask with Evergreen Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lc = xr.open_dataset('/g/data/os22/chad_tmp/NEE_modelling/data/Landcover_merged_5km.nc').isel(time=1)\n",
    "lc['latitude'] = lc.latitude.astype('float32')\n",
    "lc['longitude'] = lc.longitude.astype('float32')\n",
    "lc = lc.rename({'latitude':'y','longitude':'x'})\n",
    "\n",
    "trees = lc.PFT == 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trees.plot.imshow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_out.slope.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import folium\n",
    "# import odc.geo\n",
    "# import folium\n",
    "# from odc.geo.xr import assign_crs\n",
    "\n",
    "# # Create folium Map (ipyleaflet is also supported)\n",
    "# m = folium.Map(tiles='openstreetmap')\n",
    "\n",
    "# # Plot each sample image with different colormap\n",
    "# ds_out.slope.where(trees).odc.add_to(m, cmap='BrBG', vmax=0.2,vmin=-0.2, opacity=1.0)\n",
    "\n",
    "# # Zoom map to Australia\n",
    "# m.fit_bounds(ds_out.odc.map_bounds())\n",
    "\n",
    "# # tile = folium.TileLayer(\n",
    "# #         tiles = 'https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}',\n",
    "# #         attr = 'Esri',\n",
    "# #         name = 'Esri Satellite',\n",
    "# #         overlay = True,\n",
    "# #         control = True\n",
    "# #        ).add_to(m)\n",
    "\n",
    "# folium.LayerControl().add_to(m)\n",
    "# display(m)\n",
    "\n",
    "\n",
    "# ds_out.slope.where(trees).plot.imshow(size=10, robust=True, cmap='BrBG')\n",
    "# plt.title('Linear Trend in Evergreen Forest GPP 2003-2018');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_out.slope.where(trees).plot.imshow(size=10, robust=True, cmap='BrBG')\n",
    "plt.title('Linear Trend in Evergreen Forest GPP 2003-2018');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_out.slope.plot.imshow(size=10, robust=True, cmap='BrBG')\n",
    "plt.title('Linear Trend in Evergreen Forest GPP 2003-2018');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlations\n",
    "\n",
    "TODO: compute p-values and plot hashes over areas with significance > 0.05\n",
    "https://xskillscore.readthedocs.io/en/stable/api/xskillscore.pearson_r_eff_p_value.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_clim_mean = ds.groupby('time.month').mean()\n",
    "ds_anom = (ds.groupby('time.month') - ds_clim_mean).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precip_corr = xr.corr(ds_anom, data['rain_anom'], dim='time').compute()\n",
    "vpd_corr = xr.corr(ds_anom, data['vpd'], dim='time').compute()\n",
    "srad_corr = xr.corr(ds_anom, data['srad_anom'], dim='time').compute()\n",
    "tavg_corr = xr.corr(ds_anom, data['tavg_anom'], dim='time').compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(2,2, figsize=(18,15), sharey=True, sharex=True)\n",
    "precip_corr.plot.imshow(vmin=-1, vmax=1, cmap='RdBu', ax=ax[0,0], add_colorbar=False)\n",
    "ax[0,0].set_title(var+' & Rain');\n",
    "\n",
    "tavg_corr.plot.imshow(vmin=-1, vmax=1, cmap='RdBu', ax=ax[0,1], add_colorbar=False)\n",
    "ax[0,1].set_title(var+' & TAVG');\n",
    "\n",
    "srad_corr.plot.imshow(vmin=-1, vmax=1, cmap='RdBu', ax=ax[1,0], add_colorbar=False)\n",
    "ax[1,0].set_title(var+' & SRAD');\n",
    "\n",
    "im = vpd_corr.plot.imshow(vmin=-1, vmax=1, cmap='RdBu', ax=ax[1,1], add_colorbar=False)\n",
    "ax[1,1].set_title(var+' & VPD')\n",
    "\n",
    "cbar = fig.colorbar(im, spacing='uniform', ax=ax.ravel().tolist(), orientation='vertical', shrink=0.4);\n",
    "# plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable with highest correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs = xr.merge([\n",
    "    np.abs(precip_corr.rename('precip_anom')),\n",
    "    np.abs(vpd_corr.rename('vpd')),\n",
    "    np.abs(srad_corr.rename('srad_anom')),\n",
    "    np.abs(tavg_corr.rename('tavg_anom'))\n",
    "])\n",
    "\n",
    "max_corrs = corrs.to_array(\"variable\").idxmax(\"variable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_corrs = xr.where(max_corrs == 'precip_anom', 1, max_corrs)\n",
    "max_corrs = xr.where(max_corrs == 'vpd', 2, max_corrs)\n",
    "max_corrs = xr.where(max_corrs == 'srad_anom', 3, max_corrs)\n",
    "max_corrs = xr.where(max_corrs == 'tavg_anom', 4, max_corrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_corrs = max_corrs.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1, figsize=(10,7))\n",
    "im = max_corrs.plot.imshow(add_colorbar=False, ax=ax)\n",
    "cbar = fig.colorbar(im, spacing='uniform', ax=ax, orientation='vertical', shrink=0.4)\n",
    "cbar.set_ticks([1,2,3,4])\n",
    "cbar.set_ticklabels(['Rain', 'VPD', 'SRAD', 'TAVG'], fontsize=10)\n",
    "plt.title('Variable with Maximum Absolute Correlation with '+var);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot relationships between NEE, GPP, ER and environmental variables (P, T, SM etc)\n",
    "\n",
    "Following Lui et al. (2018) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpp = xr.open_dataarray('/g/data/os22/chad_tmp/NEE_modelling/results/predictions/GPP_2003_2021_5km_LGBM.nc',\n",
    "                       chunks=dict(x=250,y=250, time=-1))#.sel(time=slice('2003','2018'))\n",
    "\n",
    "nee = xr.open_dataarray('/g/data/os22/chad_tmp/NEE_modelling/results/predictions/NEE_2003_2021_5km_LGBM.nc',\n",
    "                       chunks=dict(x=250,y=250, time=-1))#.sel(time=slice('2003','2018'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def s_cor(x,y, pthres = 0.05, direction = True):\n",
    "    \"\"\"\n",
    "    Uses the scipy stats module to calculate a Kendall correlation test\n",
    "    :x vector: Input pixel vector to run tests on\n",
    "    :y vector: The date input vector\n",
    "    :pthres: Significance of the underlying test\n",
    "    :direction: output only direction as output (-1 & 1)\n",
    "    \"\"\"\n",
    "    # Check NA values\n",
    "    co = np.count_nonzero(~np.isnan(x))\n",
    "    if co < 4: # If fewer than 4 observations return -9999\n",
    "        return np.nan\n",
    "    # Run the kendalltau test\n",
    "    r, p_value = stats.spearmanr(x, y, nan_policy='omit')\n",
    "\n",
    "    # Criterium to return results in case of Significance\n",
    "    if p_value > pthres:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return r \n",
    "\n",
    "def spearman_correlation(x,y,dim='year'):\n",
    "    return xr.apply_ufunc(\n",
    "        s_cor, x , y,\n",
    "        input_core_dims=[[dim], [dim]],\n",
    "        vectorize=True,\n",
    "        dask='parallelized',\n",
    "        output_dtypes=[np.float32]\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = spearman_correlation(gpp, nee ,'time').compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.plot.imshow(size=6, vmin=-1, vmax=1, cmap='RdBu')\n",
    "plt.title('Signficant (p<0.05) Temporal Spearman Correlations: GPP & NEE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Causality\n",
    "\n",
    "###  Granger casaulity tests?\n",
    "\n",
    "### Bayesian structure learning?\n",
    "https://towardsdatascience.com/a-step-by-step-guide-in-detecting-causal-relationships-using-bayesian-structure-learning-in-python-c20c6b31cee5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "\n",
    "def grangers_causation_matrix(data, variables, maxlag=12, test='ssr_chi2test', verbose=False):    \n",
    "    \"\"\"Check Granger Causality of all possible combinations of the Time series.\n",
    "    The rows are the response variable, columns are predictors. The values in the table \n",
    "    are the P-Values. P-Values lesser than the significance level (0.05), implies \n",
    "    the Null Hypothesis that the coefficients of the corresponding past values is \n",
    "    zero, that is, the X does not cause Y can be rejected.\n",
    "\n",
    "    data      : pandas dataframe containing the time series variables\n",
    "    variables : list containing names of the time series variables.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(np.zeros((len(variables), len(variables))), columns=variables, index=variables)\n",
    "    for c in df.columns:\n",
    "        for r in df.index:\n",
    "            test_result = grangercausalitytests(data[[r, c]], maxlag=maxlag, verbose=False)\n",
    "            p_values = [round(test_result[i+1][0][test][1],4) for i in range(maxlag)]\n",
    "            if verbose: print(f'Y = {r}, X = {c}, P Values = {p_values}')\n",
    "            min_p_value = np.min(p_values)\n",
    "            df.loc[r, c] = min_p_value\n",
    "    df.columns = [var + '_x' for var in variables]\n",
    "    df.index = [var + '_y' for var in variables]\n",
    "    return df\n",
    "\n",
    "grangers_causation_matrix(ndvi, variables = ndvi.columns)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
