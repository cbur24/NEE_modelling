{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse modelled fluxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import scipy as sp\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from odc.algo import xr_reproject\n",
    "# from odc.geo.geobox import zoom_out\n",
    "from matplotlib import pyplot as plt\n",
    "from datacube.utils.dask import start_local_dask\n",
    "\n",
    "sys.path.append('/g/data/os22/chad_tmp/dea-notebooks/Tools/')\n",
    "from dea_tools.spatial import xr_rasterize\n",
    "\n",
    "sys.path.append('/g/data/os22/chad_tmp/NEE_modelling/')\n",
    "from collect_prediction_data import round_coords, collect_prediction_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = start_local_dask(mem_safety_margin='2Gb')\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = 'NEE'\n",
    "suffix='20230320'\n",
    "results_name = var+'_2003_2022_1km_quantiles_'+suffix+'.nc'\n",
    "chunks = {'x':1100,'y':1100, 'time':-1}\n",
    "# mask_path = '/g/data/os22/chad_tmp/NEE_modelling/results/prediction_data/mask_5km.nc'\n",
    "# data_path = '/g/data/os22/chad_tmp/NEE_modelling/results/prediction_data/data_5km.nc'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataarray('/g/data/os22/chad_tmp/NEE_modelling/results/predictions/'+results_name,\n",
    "                       chunks=chunks).sel(quantile=0.5).drop('quantile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid = zoom_out(ds.odc.geobox, 2)\n",
    "# ds = xr_reproject(ds, geobox=grid.compat, resampling='average').compute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds = ds.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open predictor data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = collect_prediction_data(time_start='2003',\n",
    "                             time_end='2022',\n",
    "                             verbose=False,\n",
    "                             export=False,\n",
    "                             chunks=chunks\n",
    "                             )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_var='vpd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# var_clim_mean = data[data_var].groupby('time.month').mean()\n",
    "# var_anom = (data[data_var].groupby('time.month') - var_clim_mean).compute()\n",
    "\n",
    "# var_clim_mean = var_clim_mean.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_clim_mean = ds.groupby('time.month').mean().compute()\n",
    "ds_anom = (ds.groupby('time.month') - ds_clim_mean).compute()\n",
    "\n",
    "# ds_clim_mean = ds_clim_mean.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create table of correlations per bioclimatic region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.read_file('/g/data/os22/chad_tmp/NEE_modelling/data/bioclimatic_regions.geojson')\n",
    "gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clim_vars = ['rain_anom', 'rain_cml3_anom', 'rain_cml6_anom','rain_cml12_anom', 'tavg_anom', 'srad_anom', 'kNDVI_anom']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outer = {}\n",
    "for index, row in gdf.iterrows():\n",
    "    print(row['region_name'])\n",
    "    mask = xr_rasterize(gdf.iloc[[index]], ds_anom.isel(time=1))\n",
    "    mask = round_coords(mask)\n",
    "    mask = mask.rename({'latitude':'y', 'longitude':'x'})\n",
    "    inner = {}\n",
    "    for v in clim_vars:\n",
    "        var_anom_region = data[v].where(mask).compute()\n",
    "        ds_anom_region = ds_anom.where(mask)\n",
    "        r2 = xr.corr(ds_anom_region.chunk(chunks),\n",
    "                     var_anom_region.chunk(chunks),\n",
    "                     dim='time').compute()\n",
    "        r2 = r2.mean(['x', 'y'])\n",
    "        print('  ', v, r2.values)\n",
    "        inner[v] = r2.values\n",
    "    outer[row['region_name']] = inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(outer)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.to_csv('/g/data/os22/chad_tmp/NEE_modelling/results/'+var+'_anomaly_bioregion_correlations.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Over all of Aus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2 = xr.corr(ds_anom_region, var_anom_region, dim='time').compute()\n",
    "r2 = r2.mean(['x', 'y'])\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax2_ylim = -100,100\n",
    "ax_ylim = -1,1\n",
    "\n",
    "fig,ax=plt.subplots(1,2, figsize=(18,5), gridspec_kw={'width_ratios': [3, 1]})\n",
    "ax2 = ax[0].twinx()\n",
    "var_anom.mean(['x','y']).rolling(time=3).mean().plot(ax=ax2, label=data_var, c='orange')\n",
    "ds_anom.sum(['x','y']).rolling(time=3).mean().plot(ax=ax[0], label=var)\n",
    "ax[0].legend(loc=(0.80,0.925))\n",
    "ax2.legend(loc=(0.80,0.85))\n",
    "ax2.set_ylabel(data_var+' Anomaly', fontsize=15)\n",
    "ax[0].set_xlabel('')\n",
    "ax2.set_ylim(ax2_ylim)\n",
    "ax[0].set_ylim(ax_ylim)\n",
    "ax[0].text(.05, .90, 'r={:.2f}'.format(r2[0]),\n",
    "            transform=ax[0].transAxes, fontsize=15)\n",
    "ax[0].set_ylabel(var+' Anomalies (PgC y⁻¹)', fontsize=15)\n",
    "ax[0].tick_params(axis='x', labelsize=14)\n",
    "ax[0].tick_params(axis='y', labelsize=14)\n",
    "ax2.tick_params(axis='y', labelsize=14)\n",
    "ax[1].tick_params(axis='x', labelsize=14)\n",
    "ax[1].tick_params(axis='y', labelsize=14)\n",
    "\n",
    "ax[0].axhline(0, c='grey', linestyle='--')\n",
    "\n",
    "ax3 = ax[1].twinx()\n",
    "\n",
    "var_clim_mean.mean(['x','y']).plot(ax=ax3, label=data_var, c='orange')\n",
    "ds_clim_mean.mean(['x','y']).plot(ax=ax[1], label='NEE')\n",
    "ax3.set_ylabel(data_var)\n",
    "ax3.set_ylabel(data_var, fontsize=15)\n",
    "ax[1].set_ylabel(var+' (PgC y⁻¹)', fontsize=15)\n",
    "ax[1].set_xticks(range(1,13))\n",
    "ax[1].set_xticklabels([\"J\",\"F\",\"M\",\"A\",\"M\",\"J\",\"J\",\"A\",\"S\",\"O\",\"N\",\"D\"]) \n",
    "ax[1].set_xlabel('')\n",
    "ax3.tick_params(axis='y', labelsize=14)\n",
    "ax[0].set_title(None)\n",
    "ax[1].set_title(None)\n",
    "ax2.set_title(None)\n",
    "ax3.set_title(None)\n",
    "plt.tight_layout();\n",
    "plt.savefig('/g/data/os22/chad_tmp/NEE_modelling/results/figs/'+var+'_Aus_'+data_var+'_correlations.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots by bioregion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax2_ylim = -0.11,0.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to save results \n",
    "results = {}\n",
    "for index, row in gdf.iterrows():\n",
    "\n",
    "    # Generate a polygon mask to keep only data within the polygon\n",
    "    mask = xr_rasterize(gdf.iloc[[index]], var_anom.isel(time=1))\n",
    "    mask['latitude'] = mask.latitude.astype('float32')\n",
    "    mask['longitude'] = mask.longitude.astype('float32')  \n",
    "    mask['latitude'] = np.array([round(i,4) for i in mask.latitude.values])\n",
    "    mask['longitude'] = np.array([round(i,4) for i in mask.longitude.values])\n",
    "    mask = mask.rename({'latitude':'y', 'longitude':'x'})\n",
    "    \n",
    "    # Mask dataset to set pixels outside the polygon to `NaN`\n",
    "    var_anom_region = var_anom.where(mask)\n",
    "    ds_anom_region = ds_anom.where(mask)\n",
    "    \n",
    "    var_clim_mean_region = var_clim_mean.where(mask)\n",
    "    ds_clim_mean_region = ds_clim_mean.where(mask)\n",
    "    \n",
    "    r2 = xr.corr(ds_anom_region, var_anom_region, dim='time').compute()\n",
    "    r2 = r2.mean(['x', 'y'])\n",
    "    print(row['region_name'], r2)\n",
    "\n",
    "    fig,ax=plt.subplots(1,2, figsize=(18,5), gridspec_kw={'width_ratios': [3, 1]})\n",
    "    ax2 = ax[0].twinx()\n",
    "    var_anom_region.mean(['x','y']).rolling(time=3).mean().plot(ax=ax2, label=data_var, c='orange')\n",
    "    ds_anom_region.mean(['x','y']).rolling(time=3).mean().plot(ax=ax[0], label=var)\n",
    "    \n",
    "    ax[0].legend(loc=(0.80,0.925))\n",
    "    ax2.legend(loc=(0.80,0.85))\n",
    "    ax2.set_ylabel(data_var+' Anomaly', fontsize=15)\n",
    "    ax[0].set_xlabel('')\n",
    "    ax2.set_ylim(ax2_ylim)\n",
    "    ax[0].set_ylim(ax_ylim)\n",
    "    ax[0].text(.05, .90, 'r={:.2f}'.format(r2[0]),\n",
    "                transform=ax[0].transAxes, fontsize=15)\n",
    "    ax[0].set_ylabel(var+' Anomalies (gC m\\N{SUPERSCRIPT TWO} m⁻¹)', fontsize=15)\n",
    "    ax[0].tick_params(axis='x', labelsize=14)\n",
    "    ax[0].tick_params(axis='y', labelsize=14)\n",
    "    ax2.tick_params(axis='y', labelsize=14)\n",
    "    ax[1].tick_params(axis='x', labelsize=14)\n",
    "    ax[1].tick_params(axis='y', labelsize=14)\n",
    "\n",
    "    ax[0].axhline(0, c='grey', linestyle='--')\n",
    "\n",
    "    ax3 = ax[1].twinx()\n",
    "\n",
    "    var_clim_mean_region.mean(['x','y']).plot(ax=ax3, label=data_var, c='orange')\n",
    "    ds_clim_mean_region.mean(['x','y']).plot(ax=ax[1], label='NEE')\n",
    "    \n",
    "    ax3.set_ylabel(data_var)\n",
    "    ax3.set_ylabel(data_var, fontsize=15)\n",
    "    ax[1].set_ylabel(var+' (gC m\\N{SUPERSCRIPT TWO} m⁻¹)', fontsize=15)\n",
    "    ax[1].set_xticks(range(1,13))\n",
    "    ax[1].set_xticklabels([\"J\",\"F\",\"M\",\"A\",\"M\",\"J\",\"J\",\"A\",\"S\",\"O\",\"N\",\"D\"]) \n",
    "    ax[1].set_xlabel('')\n",
    "    ax3.tick_params(axis='y', labelsize=14)\n",
    "    ax[0].set_title(None)\n",
    "    ax[1].set_title(None)\n",
    "    ax2.set_title(None)\n",
    "    ax3.set_title(None)\n",
    "    \n",
    "    plt.suptitle(row['region_name'], fontsize=18)\n",
    "    plt.tight_layout();\n",
    "    plt.savefig('/g/data/os22/chad_tmp/NEE_modelling/results/figs/'+var+'_'+row['region_name']+'_'+data_var+'_correlations.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per-pixel correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kndvi_corr = xr.corr(ds.chunk(chunks), data['kNDVI'], dim='time').compute()\n",
    "rain_corr = xr.corr(ds.chunk(chunks), data['rain'], dim='time').compute()\n",
    "vpd_corr = xr.corr(ds.chunk(chunks), data['vpd'], dim='time').compute()\n",
    "srad_corr = xr.corr(ds.chunk(chunks), data['srad'], dim='time').compute()\n",
    "tavg_corr = xr.corr(ds.chunk(chunks), data['tavg'], dim='time').compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kndvi_corr.plot.imshow()\n",
    "# plt.title('kNDVI correlation with ER')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlations with anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kNDVI_anom_corr = xr.corr(ds_anom.chunk(chunks), data['kNDVI_anom'], dim='time').compute()\n",
    "rain_anom_corr = xr.corr(ds_anom.chunk(chunks), data['rain_anom'], dim='time').compute()\n",
    "# vpd_anom_corr = xr.corr(ds_anom.chunk(chunks), data['vpd'], dim='time').compute()\n",
    "srad_anom_corr = xr.corr(ds_anom.chunk(chunks), data['srad_anom'], dim='time').compute()\n",
    "tavg_anom_corr = xr.corr(ds_anom.chunk(chunks), data['tavg_anom'], dim='time').compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlations with climatology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precip_clim_corr = xr.corr(ds_clim_mean.chunk({'x':1100,'y':1100}), data['rain'].groupby('time.month').mean(), dim='month').compute()\n",
    "srad_clim_corr = xr.corr(ds_clim_mean.chunk({'x':1100,'y':1100}), data['srad'].groupby('time.month').mean(), dim='month').compute()\n",
    "tavg_clim_corr = xr.corr(ds_clim_mean.chunk({'x':1100,'y':1100}), data['tavg'].groupby('time.month').mean(), dim='month').compute()\n",
    "kNDVI_clim_corr = xr.corr(ds_clim_mean.chunk({'x':1100,'y':1100}), data['kNDVI'].groupby('time.month').mean(), dim='month').compute()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot correlations with anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anom_data = [rain_anom_corr,  tavg_anom_corr, srad_anom_corr, kNDVI_anom_corr]\n",
    "clim_vars = ['Rainfall', 'Air Temp.','Solar Rad.', 'kNDVI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes = plt.subplots(1,4, figsize=(24,7), sharey=True, sharex=True)\n",
    "\n",
    "for ax, ds, clim in zip(axes.ravel(), anom_data, clim_vars):\n",
    "\n",
    "    im = ds.plot.imshow(vmin=-0.8, vmax=0.8, cmap='RdBu_r', ax=ax, add_colorbar=False)\n",
    "    ax.set_title(var+' Anomalies & '+clim+' Anomalies',  fontsize=20);\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_ylabel('')\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_xticklabels([])\n",
    "\n",
    "plt.tight_layout();\n",
    "fig.savefig('/g/data/os22/chad_tmp/NEE_modelling/results/figs/'+var+'_anomalies_perpixel_climate_correlations.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get handles and labels for reuse\n",
    "# label_params = ax[1,1].get_legend_handles_labels() \n",
    "cbar = fig.colorbar(im, spacing='uniform', ax=ax, orientation='horizontal', shrink=0.4);\n",
    "\n",
    "# cbar\n",
    "figl, axl = plt.subplots(figsize=(11,4))\n",
    "axl.axis(False)\n",
    "cbar = plt.colorbar(im, spacing='uniform', ax=axl, orientation='horizontal')\n",
    "cbar.ax.tick_params(labelsize=20)\n",
    "cbar.set_label(\"Pearson's Correlation\",size=20)\n",
    "figl.savefig('/g/data/os22/chad_tmp/NEE_modelling/results/figs/correlation_legend.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot correlations with climatology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(2,2, figsize=(12,12), sharey=True, sharex=True)\n",
    "precip_clim_corr.plot.imshow(vmin=-0.8, vmax=0.8, cmap='RdBu_r', ax=ax[0,0], add_colorbar=False)\n",
    "ax[0,0].set_title(var+' Climatology & Rain Climatology',  fontsize=18);\n",
    "ax[0,0].set_yticklabels([])\n",
    "ax[0,0].set_ylabel('')\n",
    "ax[0,0].set_xlabel('')\n",
    "ax[0,0].set_xticklabels([])\n",
    "\n",
    "tavg_clim_corr.plot.imshow(vmin=-0.8, vmax=0.8, cmap='RdBu_r', ax=ax[0,1], add_colorbar=False)\n",
    "ax[0,1].set_title(var+' Climatology & TAVG Climatology',  fontsize=18);\n",
    "ax[0,1].set_yticklabels([])\n",
    "ax[0,1].set_ylabel('')\n",
    "ax[0,1].set_xlabel('')\n",
    "ax[0,1].set_xticklabels([])\n",
    "\n",
    "srad_clim_corr.plot.imshow(vmin=-0.8, vmax=0.8, cmap='RdBu_r', ax=ax[1,0], add_colorbar=False)\n",
    "ax[1,0].set_title(var+' Climatology & SRAD Climatology',  fontsize=18);\n",
    "ax[1,0].set_yticklabels([])\n",
    "ax[1,0].set_ylabel('')\n",
    "ax[1,0].set_xlabel('')\n",
    "ax[1,0].set_xticklabels([])\n",
    "\n",
    "im = kNDVI_clim_corr.plot.imshow(vmin=-0.8, vmax=0.8, cmap='RdBu_r', ax=ax[1,1], add_colorbar=False)\n",
    "ax[1,1].set_title(var+' Climatology & kNDVI Climatology', fontsize=18)\n",
    "ax[1,1].set_yticklabels([])\n",
    "ax[1,1].set_ylabel('')\n",
    "ax[1,1].set_xlabel('')\n",
    "ax[1,1].set_xticklabels([])\n",
    "plt.tight_layout();\n",
    "fig.savefig('/g/data/os22/chad_tmp/NEE_modelling/results/figs/'+var+'_climatology_perpixel_climate_correlations.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable with highest correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs = xr.merge([\n",
    "    np.abs(rain_corr.rename('rain')),\n",
    "    np.abs(vpd_corr.rename('vpd')),\n",
    "    np.abs(srad_corr.rename('srad')),\n",
    "    np.abs(tavg_corr.rename('tavg'))\n",
    "])\n",
    "\n",
    "max_corrs = corrs.to_array(\"variable\").idxmax(\"variable\")\n",
    "max_corrs = xr.where(max_corrs == 'rain', 1, max_corrs)\n",
    "max_corrs = xr.where(max_corrs == 'vpd', 2, max_corrs)\n",
    "max_corrs = xr.where(max_corrs == 'srad', 3, max_corrs)\n",
    "max_corrs = xr.where(max_corrs == 'tavg', 4, max_corrs)\n",
    "\n",
    "max_corrs = max_corrs.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rain_corrs = xr.merge([\n",
    "    np.abs(precip_corr.rename('precip_anom')),\n",
    "    np.abs(precip_3_corr.rename('precip_3_anom')),\n",
    "    np.abs(precip_6_corr.rename('precip_6_anom')),\n",
    "    np.abs(precip_12_corr.rename('precip_12_anom'))\n",
    "])\n",
    "\n",
    "rain_max_corrs = rain_corrs.to_array(\"variable\").idxmax(\"variable\")\n",
    "\n",
    "rain_max_corrs = xr.where(rain_max_corrs == 'precip_anom', 1, rain_max_corrs)\n",
    "rain_max_corrs = xr.where(rain_max_corrs == 'precip_3_anom', 2, rain_max_corrs)\n",
    "rain_max_corrs = xr.where(rain_max_corrs == 'precip_6_anom', 3, rain_max_corrs)\n",
    "rain_max_corrs = xr.where(rain_max_corrs == 'precip_12_anom', 4, rain_max_corrs)\n",
    "\n",
    "rain_max_corrs = rain_max_corrs.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1, figsize=(10,7))\n",
    "im = max_corrs.plot.imshow(add_colorbar=False, ax=ax)\n",
    "cbar = fig.colorbar(im, spacing='uniform', ax=ax, orientation='vertical', shrink=0.4)\n",
    "cbar.set_ticks([1,2,3,4])\n",
    "cbar.set_ticklabels(['Rain', 'VPD', 'SRAD', 'TAVG'], fontsize=10)\n",
    "plt.title('Climate Variable with Maximum Absolute Correlation with '+var);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1, figsize=(10,7))\n",
    "im = rain_max_corrs.plot.imshow(add_colorbar=False, ax=ax)\n",
    "cbar = fig.colorbar(im, spacing='uniform', ax=ax, orientation='vertical', shrink=0.4)\n",
    "cbar.set_ticks([1,2,3,4])\n",
    "cbar.set_ticklabels(['Rain', 'Rain-3', 'Rain-6', 'Rain-12'], fontsize=10)\n",
    "plt.title('Rainfall Variable with Maximum Absolute Correlation with '+var);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import xarray as xr\n",
    "import dask.array as da\n",
    "from dask.delayed import delayed\n",
    "from  scipy import stats\n",
    "\n",
    "def _calc_slope(y):\n",
    "    \"\"\"return linear regression statistical variables\"\"\"\n",
    "    mask = np.isfinite(y)\n",
    "    x = np.arange(len(y))\n",
    "    return stats.linregress(x[mask], y[mask])\n",
    "\n",
    "# regression function defition\n",
    "def regression(y):\n",
    "    \"\"\"apply linear regression function along time axis\"\"\"\n",
    "    axis_num = y.get_axis_num('time')\n",
    "    return da.apply_along_axis(_calc_slope, axis_num, y)\n",
    "\n",
    "# fill pixels that are all-NaNs\n",
    "allnans = ds.isnull().all('time').compute()\n",
    "ds = ds.where(~allnans, other=0)\n",
    "\n",
    "# regression analysis\n",
    "delayed_objs = delayed(regression)(ds).persist()\n",
    "\n",
    "# transforms dask.delayed to dask.array\n",
    "results = da.from_delayed(delayed_objs, shape=(5, ds.shape[1:][0], ds.shape[1:][1]), dtype=np.float32)\n",
    "results = results.compute()\n",
    "results = results.compute() #need this twice haven't figured out why\n",
    "\n",
    "# statistical variables definition\n",
    "variables = ['slope', 'intercept', 'r_value', 'p_value', 'std_err']\n",
    "\n",
    "# coordination definition\n",
    "coords = {'y': ds.y, 'x': ds.x}\n",
    "\n",
    "# output xarray.Dataset definition\n",
    "ds_out = xr.Dataset(\n",
    "    data_vars=dict(slope=([\"y\", \"x\"], results[0]),\n",
    "                   intercept=([\"y\", \"x\"], results[1]),\n",
    "                   r_value=([\"y\", \"x\"], results[2]),\n",
    "                   p_value=([\"y\", \"x\"], results[3]),\n",
    "                   std_err=([\"y\", \"x\"], results[4]),\n",
    "                  ),\n",
    "    coords = coords)\n",
    "\n",
    "#remask all-NaN pixel\n",
    "ds_out = ds_out.where(~allnans)\n",
    "ds_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mask with Evergreen Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lc = xr.open_dataset('/g/data/os22/chad_tmp/NEE_modelling/data/Landcover_merged_5km.nc').isel(time=1)\n",
    "lc['latitude'] = lc.latitude.astype('float32')\n",
    "lc['longitude'] = lc.longitude.astype('float32')\n",
    "lc = lc.rename({'latitude':'y','longitude':'x'})\n",
    "\n",
    "trees = lc.PFT == 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trees.plot.imshow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_out.slope.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import folium\n",
    "# import odc.geo\n",
    "# import folium\n",
    "# from odc.geo.xr import assign_crs\n",
    "\n",
    "# # Create folium Map (ipyleaflet is also supported)\n",
    "# m = folium.Map(tiles='openstreetmap')\n",
    "\n",
    "# # Plot each sample image with different colormap\n",
    "# ds_out.slope.where(trees).odc.add_to(m, cmap='BrBG', vmax=0.2,vmin=-0.2, opacity=1.0)\n",
    "\n",
    "# # Zoom map to Australia\n",
    "# m.fit_bounds(ds_out.odc.map_bounds())\n",
    "\n",
    "# # tile = folium.TileLayer(\n",
    "# #         tiles = 'https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}',\n",
    "# #         attr = 'Esri',\n",
    "# #         name = 'Esri Satellite',\n",
    "# #         overlay = True,\n",
    "# #         control = True\n",
    "# #        ).add_to(m)\n",
    "\n",
    "# folium.LayerControl().add_to(m)\n",
    "# display(m)\n",
    "\n",
    "\n",
    "# ds_out.slope.where(trees).plot.imshow(size=10, robust=True, cmap='BrBG')\n",
    "# plt.title('Linear Trend in Evergreen Forest GPP 2003-2018');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_out.slope.where(trees).plot.imshow(size=10, robust=True, cmap='BrBG')\n",
    "plt.title('Linear Trend in Evergreen Forest GPP 2003-2018');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_out.slope.plot.imshow(size=10, robust=True, cmap='BrBG')\n",
    "plt.title('Linear Trend in Evergreen Forest GPP 2003-2018');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot relationships between NEE, GPP, ER and environmental variables (P, T, SM etc)\n",
    "\n",
    "Following Lui et al. (2018) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpp = xr.open_dataarray('/g/data/os22/chad_tmp/NEE_modelling/results/predictions/GPP_2003_2021_5km_LGBM.nc',\n",
    "                       chunks=dict(x=250,y=250, time=-1))#.sel(time=slice('2003','2018'))\n",
    "\n",
    "nee = xr.open_dataarray('/g/data/os22/chad_tmp/NEE_modelling/results/predictions/NEE_2003_2021_5km_LGBM.nc',\n",
    "                       chunks=dict(x=250,y=250, time=-1))#.sel(time=slice('2003','2018'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def s_cor(x,y, pthres = 0.05, direction = True):\n",
    "    \"\"\"\n",
    "    Uses the scipy stats module to calculate a Kendall correlation test\n",
    "    :x vector: Input pixel vector to run tests on\n",
    "    :y vector: The date input vector\n",
    "    :pthres: Significance of the underlying test\n",
    "    :direction: output only direction as output (-1 & 1)\n",
    "    \"\"\"\n",
    "    # Check NA values\n",
    "    co = np.count_nonzero(~np.isnan(x))\n",
    "    if co < 4: # If fewer than 4 observations return -9999\n",
    "        return np.nan\n",
    "    # Run the kendalltau test\n",
    "    r, p_value = stats.spearmanr(x, y, nan_policy='omit')\n",
    "\n",
    "    # Criterium to return results in case of Significance\n",
    "    if p_value > pthres:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return r \n",
    "\n",
    "def spearman_correlation(x,y,dim='year'):\n",
    "    return xr.apply_ufunc(\n",
    "        s_cor, x , y,\n",
    "        input_core_dims=[[dim], [dim]],\n",
    "        vectorize=True,\n",
    "        dask='parallelized',\n",
    "        output_dtypes=[np.float32]\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = spearman_correlation(gpp, nee ,'time').compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.plot.imshow(size=6, vmin=-1, vmax=1, cmap='RdBu')\n",
    "plt.title('Signficant (p<0.05) Temporal Spearman Correlations: GPP & NEE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Causality\n",
    "\n",
    "###  Granger casaulity tests?\n",
    "\n",
    "### Bayesian structure learning?\n",
    "https://towardsdatascience.com/a-step-by-step-guide-in-detecting-causal-relationships-using-bayesian-structure-learning-in-python-c20c6b31cee5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "\n",
    "def grangers_causation_matrix(data, variables, maxlag=12, test='ssr_chi2test', verbose=False):    \n",
    "    \"\"\"Check Granger Causality of all possible combinations of the Time series.\n",
    "    The rows are the response variable, columns are predictors. The values in the table \n",
    "    are the P-Values. P-Values lesser than the significance level (0.05), implies \n",
    "    the Null Hypothesis that the coefficients of the corresponding past values is \n",
    "    zero, that is, the X does not cause Y can be rejected.\n",
    "\n",
    "    data      : pandas dataframe containing the time series variables\n",
    "    variables : list containing names of the time series variables.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(np.zeros((len(variables), len(variables))), columns=variables, index=variables)\n",
    "    for c in df.columns:\n",
    "        for r in df.index:\n",
    "            test_result = grangercausalitytests(data[[r, c]], maxlag=maxlag, verbose=False)\n",
    "            p_values = [round(test_result[i+1][0][test][1],4) for i in range(maxlag)]\n",
    "            if verbose: print(f'Y = {r}, X = {c}, P Values = {p_values}')\n",
    "            min_p_value = np.min(p_values)\n",
    "            df.loc[r, c] = min_p_value\n",
    "    df.columns = [var + '_x' for var in variables]\n",
    "    df.index = [var + '_y' for var in variables]\n",
    "    return df\n",
    "\n",
    "grangers_causation_matrix(ndvi, variables = ndvi.columns)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
