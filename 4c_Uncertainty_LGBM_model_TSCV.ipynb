{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling uncertainty in ML predictions\n",
    "\n",
    "Quantile regression in this context doesn't work as an estimate of Aus-wide uncertainty.\n",
    "\n",
    "Instead, we will attempt to model the uncertainty that comes from the training data and optimization algorithm.  To do this, we will generate 30 models. For each iteration, two site's entire time-series will be removed from the training data and both a  LGBM model will be fit on the remaining data.  This will result in 29 models that later we can use to make predictions with. The envelope of our predictions will inform our uncertainity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import dump\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import lightgbm as lgbm\n",
    "from sklearn.model_selection import TimeSeriesSplit, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_var = 'NEE'\n",
    "features_list = '/g/data/os22/chad_tmp/NEE_modelling/results/feature_select_'+model_var+'.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = '/g/data/os22/chad_tmp/NEE_modelling/results/training_data/'\n",
    "sites = os.listdir('/g/data/os22/chad_tmp/NEE_modelling/results/training_data/')\n",
    "\n",
    "td = []\n",
    "for site in sites:\n",
    "    if '.csv' in site:\n",
    "        xx = pd.read_csv(base+site, index_col='time', parse_dates=True)\n",
    "        xx['site'] = site[0:5]\n",
    "        td.append(xx)\n",
    "\n",
    "ts = pd.concat(td).dropna() #we'll use this later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = [\n",
    "            #'LAI_anom_RS',\n",
    "             'kNDVI_anom_RS',\n",
    "             'FPAR_RS',\n",
    "             'LST_RS',\n",
    "             'tree_cover_RS',\n",
    "             'nontree_cover_RS',\n",
    "             'nonveg_cover_RS',\n",
    "             'LST-Tair_RS',\n",
    "             'TWI_RS',\n",
    "             'NDWI_RS',\n",
    "             'rain_anom_RS',\n",
    "             'rain_cml3_anom_RS',\n",
    "             'rain_cml6_anom_RS',\n",
    "             'rain_cml12_anom_RS',\n",
    "             'srad_anom_RS',\n",
    "             'vpd_RS',\n",
    "             'tavg_anom_RS',\n",
    "             'SOC_RS',\n",
    "             'C4_percent_RS',\n",
    "             'elevation_RS',\n",
    "             'VegH_RS',\n",
    "             'site'\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2744, 21)\n"
     ]
    }
   ],
   "source": [
    "xx = []\n",
    "yy = []\n",
    "\n",
    "for t in td:\n",
    "    # t = t.drop(['Fluxcom_RS-Meteo_NEE', 'Fluxcom_RS_NEE', 'ThisStudy_NEE', 'Cable_NEE',\n",
    "    #    'Fluxcom_RS_GPP', 'Fluxcom_RS-meteo_GPP', 'ThisStudy_GPP', 'Cable_GPP',\n",
    "    #    'MODIS_GPP', 'GOSIF_GPP'], axis=1)  \n",
    "    \n",
    "    t = t.dropna()  # remove NaNS\n",
    "    df = t.drop(['NEE_SOLO_EC','GPP_SOLO_EC','ER_SOLO_EC'], axis=1) # seperate carbon fluxes\n",
    "    \n",
    "    #df = df.filter(regex='RS') # only use remote sensing variables   \n",
    "    df = df[variables]\n",
    "    \n",
    "    if model_var == 'ET':\n",
    "        df_var=t[model_var+'_EC']\n",
    "    else:\n",
    "        df_var=t[[model_var+'_SOLO_EC', 'site']] # seperate out the variable we're modelling\n",
    "    \n",
    "    x = df.reset_index(drop=True)#.to_numpy()\n",
    "    y = df_var.reset_index(drop=True)#.to_numpy()\n",
    "    xx.append(x)\n",
    "    yy.append(y)\n",
    "\n",
    "x = pd.concat(xx)\n",
    "y = pd.concat(yy)\n",
    "\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove features not selected in earlier fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vars = list(pd.read_csv(features_list))[0:-1]\n",
    "train_vars=[i[:-3] for i in train_vars]\n",
    "train_vars = [i+'_RS' for i in train_vars]\n",
    "train_vars.append('site')\n",
    "\n",
    "x = x[train_vars]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate 30 iterations of models\n",
    "\n",
    "For two regression methods (RF and LigtGBM), we remove two randomly selected sites from the training data\n",
    "\n",
    "Then, we do the per site TSCV: For each site, grab a sequential set of test samples (time-series-split methods), the remaining points (either side of test samples) go into training.  A single K-fold contains test and training samples from every site.\n",
    "\n",
    "A model is built and saved that is trained of 15 iterations of site removal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 001/015\n",
      "  Model: _lgbm_\n",
      "  r2 score  0.59\n",
      "  Model: _rf_\n",
      "  r2 score  0.61\n",
      " 002/015\n",
      "  Model: _lgbm_\n",
      "  r2 score  0.57\n",
      "  Model: _rf_\n",
      "  r2 score  0.57\n",
      " 003/015\n",
      "  Model: _lgbm_\n",
      "  r2 score  0.58\n",
      "  Model: _rf_\n",
      "  r2 score  0.59\n",
      " 004/015\n",
      "  Model: _lgbm_\n",
      "  r2 score  0.6\n",
      "  Model: _rf_\n",
      "  r2 score  0.6\n",
      " 005/015\n",
      "  Model: _lgbm_\n",
      "  r2 score  0.6\n",
      "  Model: _rf_\n",
      "  r2 score  0.6\n",
      " 006/015\n",
      "  Model: _lgbm_\n",
      "  r2 score  0.62\n",
      "  Model: _rf_\n",
      "  r2 score  0.62\n",
      " 007/015\n",
      "  Model: _lgbm_\n",
      "  r2 score  0.62\n",
      "  Model: _rf_\n",
      "  r2 score  0.63\n",
      " 008/015\n",
      "  Model: _lgbm_\n",
      "  r2 score  0.54\n",
      "  Model: _rf_\n",
      "  r2 score  0.53\n",
      " 009/015\n",
      "  Model: _lgbm_\n",
      "  r2 score  0.61\n",
      "  Model: _rf_\n",
      "  r2 score  0.62\n",
      " 010/015\n",
      "  Model: _lgbm_\n",
      "  r2 score  0.62\n",
      "  Model: _rf_\n",
      "  r2 score  0.62\n",
      " 011/015\n",
      "  Model: _lgbm_\n",
      "  r2 score  0.58\n",
      "  Model: _rf_\n",
      "  r2 score  0.58\n",
      " 012/015\n",
      "  Model: _lgbm_\n",
      "  r2 score  0.55\n",
      "  Model: _rf_\n",
      "  r2 score  0.54\n",
      " 013/015\n",
      "  Model: _lgbm_\n",
      "  r2 score  0.62\n",
      "  Model: _rf_\n",
      "  r2 score  0.61\n",
      " 014/015\n",
      "  Model: _lgbm_\n",
      "  r2 score  0.62\n",
      "  Model: _rf_\n",
      "  r2 score  0.62\n",
      " 015/015\n",
      "  Model: _lgbm_\n",
      "  r2 score  0.62\n",
      "  Model: _rf_\n",
      "  r2 score  0.62\n",
      "CPU times: user 13min 29s, sys: 31.1 s, total: 14min\n",
      "Wall time: 1h 52min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "i=0\n",
    "for m in range(1,16): # 15 iterations for each model\n",
    "    print(\" {:03}/{:03}\\r\".format(m, len(range(1,16))))\n",
    "          \n",
    "    #randomly select two sites to remove from dataset\n",
    "    subset=np.random.choice(x['site'].unique(), size=2)\n",
    "    x_n = x[~x.site.isin(subset)]\n",
    "    y_n = y[~y.site.isin(subset)]\n",
    "\n",
    "    sites_n = x_n['site'].unique()\n",
    "    x_n['original_index'] = [i for i in range(0,len(x_n))]\n",
    "    \n",
    "    #build TSCV splits across all remaining sites\n",
    "    train_1=[]\n",
    "    train_2=[]\n",
    "    train_3=[]\n",
    "    train_4=[]\n",
    "    train_5=[]\n",
    "\n",
    "    test_1=[]\n",
    "    test_2=[]\n",
    "    test_3=[]\n",
    "    test_4=[]\n",
    "    test_5=[]\n",
    "\n",
    "    for site_n in sites_n:\n",
    "        df = x_n.loc[x_n['site'] == site_n]\n",
    "        tscv = TimeSeriesSplit(n_splits=5)\n",
    "        i=1\n",
    "        for train, test in tscv.split(df):\n",
    "            all_indices=np.concatenate([train,test])\n",
    "            left_over = df.loc[~df.index.isin(all_indices)].index.values\n",
    "            train = np.concatenate([train, left_over])\n",
    "            if i==1:\n",
    "                train_1.append(df.iloc[train]['original_index'].values)\n",
    "                test_1.append(df.iloc[test]['original_index'].values)\n",
    "            if i==2:\n",
    "                train_2.append(df.iloc[train]['original_index'].values)\n",
    "                test_2.append(df.iloc[test]['original_index'].values)\n",
    "            if i==3:\n",
    "                train_3.append(df.iloc[train]['original_index'].values)\n",
    "                test_3.append(df.iloc[test]['original_index'].values)\n",
    "            if i==4:\n",
    "                train_4.append(df.iloc[train]['original_index'].values)\n",
    "                test_4.append(df.iloc[test]['original_index'].values)\n",
    "            if i==4:\n",
    "                train_5.append(df.iloc[train]['original_index'].values)\n",
    "                test_5.append(df.iloc[test]['original_index'].values)\n",
    "            i+=1\n",
    "\n",
    "    train_1 = np.concatenate(train_1)\n",
    "    train_2 = np.concatenate(train_2)\n",
    "    train_3 = np.concatenate(train_3)\n",
    "    train_4 = np.concatenate(train_4)\n",
    "    train_5 = np.concatenate(train_5)\n",
    "\n",
    "    test_1 = np.concatenate(test_1)\n",
    "    test_2 = np.concatenate(test_2)\n",
    "    test_3 = np.concatenate(test_3)\n",
    "    test_4 = np.concatenate(test_4)\n",
    "    test_5 = np.concatenate(test_5)\n",
    "\n",
    "    train = [train_1, train_2, train_3, train_4, train_5]\n",
    "    test = [test_1, test_2, test_3, test_4, test_5]\n",
    "\n",
    "    #check there are no train indices in the test indices\n",
    "    for i,j in zip(train, test):\n",
    "        assert (np.sum(np.isin(i,j)) == 0)\n",
    "\n",
    "    #remove the columns we no longer need\n",
    "    x_n = x_n.drop(['site', 'original_index'], axis=1)\n",
    "    y_n = y_n.drop('site', axis=1)\n",
    "\n",
    "    #loop through the two regression methods\n",
    "    for regressor in [LGBMRegressor,\n",
    "                      RandomForestRegressor]:\n",
    "\n",
    "        if isinstance(regressor(), lgbm.sklearn.LGBMRegressor):\n",
    "            m_name='_lgbm_'\n",
    "\n",
    "            param_grid = {\n",
    "                'num_leaves': stats.randint(5,40),\n",
    "                'min_child_samples':stats.randint(10,30),\n",
    "                'boosting_type': ['gbdt', 'dart'],\n",
    "                'max_depth': stats.randint(5,25),\n",
    "                'n_estimators': [300, 400, 500],\n",
    "            }\n",
    "\n",
    "        else:\n",
    "            m_name='_rf_'\n",
    "\n",
    "            param_grid = {\n",
    "                'max_depth': stats.randint(5,25),\n",
    "                'max_features': ['log2', None, \"sqrt\"],\n",
    "                'n_estimators': [300,400,500]}\n",
    "\n",
    "        print('  Model:', m_name)\n",
    "        \n",
    "        clf = RandomizedSearchCV(\n",
    "               regressor(random_state=0, n_jobs=-1),\n",
    "               param_grid,\n",
    "               scoring='r2',\n",
    "               verbose=0,\n",
    "               n_iter=200,\n",
    "               n_jobs=-1,\n",
    "               cv=zip(train, test)\n",
    "              )\n",
    "        \n",
    "        if m_name=='_rf_':\n",
    "            clf.fit(x_n, y_n.values.ravel())\n",
    "        \n",
    "        else:\n",
    "            clf.fit(x_n, y_n)\n",
    "        \n",
    "        print('  r2 score ', round(clf.best_score_, 2))\n",
    "\n",
    "        #fit model and save\n",
    "        model = regressor(**clf.best_params_)\n",
    "        \n",
    "        if m_name=='_rf_':\n",
    "            model.fit(x_n, y_n.values.ravel())\n",
    "        \n",
    "        else:\n",
    "            model.fit(x_n, y_n)\n",
    "\n",
    "        dump(model, '/g/data/os22/chad_tmp/NEE_modelling/results/models_uncertainty/'+model_var+m_name+str(m)+'.joblib')\n",
    "\n",
    "    i+=1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # if os.path.exists('/g/data/os22/chad_tmp/NEE_modelling/results/models_uncertainty/'+model_var+m_name+str(i)+'.joblib'):\n",
    "    #     print('skipping iteration as model already exists')\n",
    "    #     continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
