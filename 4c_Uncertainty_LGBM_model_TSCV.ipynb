{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling uncertainty in LGBM predictions\n",
    "\n",
    "Quantile regression in this context doesn't work as an estimate of Aus-wide uncertainty.\n",
    "\n",
    "Instead, we will attempt to model the uncertainty that comes from the training data.  To do this, we will generate _n_ models where _n_ is equal to the number of unique EC flux tower sites that make up the training data (29 in this case). For each iteration, one site's entire time-series will be removed from the training data and a LGBM model will be fit on the remaining data.  This will result in 29 models that later we can use to make predictions with. The envelope of our predictions will inform our uncertainity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import dump\n",
    "import multiprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "from lightgbm import LGBMRegressor\n",
    "import lightgbm as lgbm\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'AUS'\n",
    "model_var = 'NEE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ncpus = 16\n"
     ]
    }
   ],
   "source": [
    "ncpus=multiprocessing.cpu_count()\n",
    "print('ncpus = '+str(ncpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = '/g/data/os22/chad_tmp/NEE_modelling/results/training_data/'\n",
    "sites = os.listdir('/g/data/os22/chad_tmp/NEE_modelling/results/training_data/')\n",
    "\n",
    "td = []\n",
    "for site in sites:\n",
    "    if '.csv' in site:\n",
    "        xx = pd.read_csv(base+site, index_col='time', parse_dates=True)\n",
    "        xx['site'] = site[0:5]\n",
    "        td.append(xx)\n",
    "\n",
    "ts = pd.concat(td).dropna() #we'll use this later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = [\n",
    "            #'LAI_anom_RS',\n",
    "             'kNDVI_anom_RS',\n",
    "             'FPAR_RS',\n",
    "             'LST_RS',\n",
    "             'tree_cover_RS',\n",
    "             'nontree_cover_RS',\n",
    "             'nonveg_cover_RS',\n",
    "             'LST-Tair_RS',\n",
    "             'TWI_RS',\n",
    "             'NDWI_RS',\n",
    "             'rain_anom_RS',\n",
    "             'rain_cml3_anom_RS',\n",
    "             'rain_cml6_anom_RS',\n",
    "             'rain_cml12_anom_RS',\n",
    "             'srad_anom_RS',\n",
    "             'vpd_RS',\n",
    "             'tavg_anom_RS',\n",
    "             'SOC_RS',\n",
    "             #'CO2_RS',\n",
    "             'site'\n",
    "            ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2744, 18)\n"
     ]
    }
   ],
   "source": [
    "xx = []\n",
    "yy = []\n",
    "\n",
    "for t in td:\n",
    "    t = t.drop(['Fluxcom_RS-Meteo_NEE', 'Fluxcom_RS_NEE', 'ThisStudy_NEE', 'Cable_NEE',\n",
    "       'Fluxcom_RS_GPP', 'Fluxcom_RS-meteo_GPP', 'ThisStudy_GPP', 'Cable_GPP',\n",
    "       'MODIS_GPP', 'GOSIF_GPP'], axis=1)  \n",
    "    \n",
    "    t = t.dropna()  # remove NaNS\n",
    "    df = t.drop(['NEE_SOLO_EC','GPP_SOLO_EC','ER_SOLO_EC'], axis=1) # seperate carbon fluxes\n",
    "    \n",
    "    #df = df.filter(regex='RS') # only use remote sensing variables   \n",
    "    df = df[variables]\n",
    "    \n",
    "    if model_var == 'ET':\n",
    "        df_var=t[model_var+'_EC']\n",
    "    else:\n",
    "        df_var=t[[model_var+'_SOLO_EC', 'site']] # seperate out the variable we're modelling\n",
    "    \n",
    "    x = df.reset_index(drop=True)#.to_numpy()\n",
    "    y = df_var.reset_index(drop=True)#.to_numpy()\n",
    "    xx.append(x)\n",
    "    yy.append(y)\n",
    "\n",
    "x = pd.concat(xx)\n",
    "y = pd.concat(yy)\n",
    "\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model robustness with time-series K-fold cross validation\n",
    "\n",
    "* If you set boosting as RF then the lightgbm algorithm behaves as random forest. According to the documentation, to use RF you must use bagging_fraction and feature_fraction smaller than 1\n",
    "\n",
    "\n",
    "<img src=\"results/figs/cross_validation.png\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate five sets of train-test indices\n",
    "\n",
    "First we remove one site's time-series from the full training dataset.\n",
    "\n",
    "Then,we do the per site TSCV: For each site, grab a sequential set of test samples (time-series-split methods), the remaining points (either side of test samples) go into training.  A single K-fold contains test and training samples from every site.\n",
    "\n",
    "A model is built and saved that is trained of n-1 sites\n",
    "\n",
    "We can **label the models by the site that is removed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping Colli\n",
      "skipping Tumba\n",
      "skipping CapeT\n",
      "skipping Boyag\n",
      "skipping Litch\n",
      "skipping Silve\n",
      "skipping Emera\n",
      "skipping Whroo\n",
      "skipping Great\n",
      "skipping Otway\n",
      "skipping Sturt\n",
      "skipping FoggD\n",
      "skipping Gingi\n",
      "skipping Adela\n",
      "skipping DalyU\n",
      "starting Riggs\n",
      "Riggs: r2 score  0.57\n",
      "starting Longr\n",
      "Longr: r2 score  0.58\n",
      "starting Samfo\n",
      "Samfo: r2 score  0.58\n",
      "starting Walla\n",
      "Walla: r2 score  0.62\n",
      "starting Robso\n",
      "Robso: r2 score  0.59\n",
      "starting Warra\n",
      "Warra: r2 score  0.61\n",
      "starting Womba\n",
      "Womba: r2 score  0.54\n",
      "starting Calpe\n",
      "Calpe: r2 score  0.58\n",
      "starting Yanco\n",
      "Yanco: r2 score  0.57\n",
      "starting Alice\n",
      "Alice: r2 score  0.58\n",
      "starting Ridge\n",
      "Ridge: r2 score  0.57\n",
      "starting DryRi\n",
      "DryRi: r2 score  0.58\n",
      "starting Cumbe\n",
      "Cumbe: r2 score  0.59\n",
      "starting CowBa\n",
      "CowBa: r2 score  0.6\n"
     ]
    }
   ],
   "source": [
    "for site in x['site'].unique():\n",
    "    \n",
    "    if os.path.exists('/g/data/os22/chad_tmp/NEE_modelling/results/models_uncertainty/'+model_var+'_LGBM_'+site+'-rm.joblib'):\n",
    "        print('skipping '+site)\n",
    "        continue\n",
    "    else:\n",
    "        print('starting ' +site)\n",
    "        x_n = x[x.site != site]\n",
    "        y_n = y[y.site != site]\n",
    "\n",
    "        sites_n = x_n['site'].unique()\n",
    "        x_n['original_index'] = [i for i in range(0,len(x_n))]\n",
    "\n",
    "        train_1=[]\n",
    "        train_2=[]\n",
    "        train_3=[]\n",
    "        train_4=[]\n",
    "        train_5=[]\n",
    "\n",
    "        test_1=[]\n",
    "        test_2=[]\n",
    "        test_3=[]\n",
    "        test_4=[]\n",
    "        test_5=[]\n",
    "\n",
    "        for site_n in sites_n:\n",
    "            df = x_n.loc[x_n['site'] == site_n]\n",
    "            tscv = TimeSeriesSplit(n_splits=5)\n",
    "            i=1\n",
    "            for train, test in tscv.split(df):\n",
    "                all_indices=np.concatenate([train,test])\n",
    "                left_over = df.loc[~df.index.isin(all_indices)].index.values\n",
    "                train = np.concatenate([train, left_over])\n",
    "                if i==1:\n",
    "                    train_1.append(df.iloc[train]['original_index'].values)\n",
    "                    test_1.append(df.iloc[test]['original_index'].values)\n",
    "                if i==2:\n",
    "                    train_2.append(df.iloc[train]['original_index'].values)\n",
    "                    test_2.append(df.iloc[test]['original_index'].values)\n",
    "                if i==3:\n",
    "                    train_3.append(df.iloc[train]['original_index'].values)\n",
    "                    test_3.append(df.iloc[test]['original_index'].values)\n",
    "                if i==4:\n",
    "                    train_4.append(df.iloc[train]['original_index'].values)\n",
    "                    test_4.append(df.iloc[test]['original_index'].values)\n",
    "                if i==4:\n",
    "                    train_5.append(df.iloc[train]['original_index'].values)\n",
    "                    test_5.append(df.iloc[test]['original_index'].values)\n",
    "                i+=1\n",
    "\n",
    "        train_1 = np.concatenate(train_1)\n",
    "        train_2 = np.concatenate(train_2)\n",
    "        train_3 = np.concatenate(train_3)\n",
    "        train_4 = np.concatenate(train_4)\n",
    "        train_5 = np.concatenate(train_5)\n",
    "\n",
    "        test_1 = np.concatenate(test_1)\n",
    "        test_2 = np.concatenate(test_2)\n",
    "        test_3 = np.concatenate(test_3)\n",
    "        test_4 = np.concatenate(test_4)\n",
    "        test_5 = np.concatenate(test_5)\n",
    "\n",
    "        train = [train_1, train_2, train_3, train_4, train_5]\n",
    "        test = [test_1, test_2, test_3, test_4, test_5]\n",
    "\n",
    "        #check there are no train indices in the test indices\n",
    "        for i,j in zip(train, test):\n",
    "            assert (np.sum(np.isin(i,j)) == 0)\n",
    "\n",
    "        #remove the columns we no longer need\n",
    "        x_n = x_n.drop(['site', 'original_index'], axis=1)\n",
    "        y_n = y_n.drop('site', axis=1)\n",
    "\n",
    "        #optimize hyperparamters\n",
    "        param_grid = {\n",
    "            'num_leaves': [7, 14, 21, 28, 31],\n",
    "            'min_child_samples':[15, 20, 30],\n",
    "            'boosting_type': ['gbdt', 'dart'],\n",
    "            'max_depth': [5, 10, 15, 20],\n",
    "            'n_estimators': [300, 400, 500],\n",
    "            'early_stopping_round' : [10]\n",
    "            }\n",
    "\n",
    "        clf = GridSearchCV(LGBMRegressor(verbose=-1),\n",
    "                           param_grid,\n",
    "                           scoring='r2',\n",
    "                           verbose=0,\n",
    "                           cv=zip(train, test), #using timeseries custom splits here\n",
    "                          )\n",
    "\n",
    "        clf.fit(x_n, y_n, callbacks=None)\n",
    "        print(site+ \": r2 score \", round(clf.best_score_, 2))\n",
    "\n",
    "        #fit model and save\n",
    "        model = LGBMRegressor(**clf.best_params_)\n",
    "        model.fit(x_n,y_n)\n",
    "\n",
    "        dump(model, '/g/data/os22/chad_tmp/NEE_modelling/results/models_uncertainty/'+model_var+'_LGBM_'+site+'-rm.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
