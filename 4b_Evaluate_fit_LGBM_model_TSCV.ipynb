{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate and fit a ML model on the EC flux tower data \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "from joblib import dump\n",
    "import multiprocessing\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "from lightgbm import LGBMRegressor\n",
    "import lightgbm as lgbm\n",
    "import shap\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV, KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "import sys\n",
    "sys.path.append('/g/data/os22/chad_tmp/dea-notebooks/Tools')\n",
    "from dea_tools.classification import spatial_clusters\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'AUS'\n",
    "model_var = 'NEE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncpus=multiprocessing.cpu_count()\n",
    "print('ncpus = '+str(ncpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = '/g/data/os22/chad_tmp/NEE_modelling/results/training_data/'\n",
    "sites = os.listdir('/g/data/os22/chad_tmp/NEE_modelling/results/training_data/')\n",
    "\n",
    "td = []\n",
    "for site in sites:\n",
    "    if '.csv' in site:\n",
    "        xx = pd.read_csv(base+site, index_col='time', parse_dates=True)\n",
    "        xx['site'] = site[0:5]\n",
    "        td.append(xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = [\n",
    "            #'LAI_anom_RS',\n",
    "             'kNDVI_anom_RS',\n",
    "             'FPAR_RS',\n",
    "             'LST_RS',\n",
    "             'tree_cover_RS',\n",
    "             'nontree_cover_RS',\n",
    "             'nonveg_cover_RS',\n",
    "             'LST-Tair_RS',\n",
    "             'TWI_RS',\n",
    "             'NDWI_RS',\n",
    "             'rain_anom_RS',\n",
    "             'rain_cml3_anom_RS',\n",
    "             'rain_cml6_anom_RS',\n",
    "             'rain_cml12_anom_RS',\n",
    "             'srad_anom_RS',\n",
    "             'vpd_RS',\n",
    "             'tavg_anom_RS',\n",
    "             'SOC_RS',\n",
    "             #'CO2_RS',\n",
    "             'site'\n",
    "            ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = []\n",
    "yy = []\n",
    "\n",
    "for t in td:\n",
    "    #t = t.drop('PFT_RS', axis=1)  \n",
    "    t = t.dropna()  # remove NaNS\n",
    "    df = t.drop(['NEE_SOLO_EC','GPP_SOLO_EC','ER_SOLO_EC'], axis=1) # seperate carbon fluxes\n",
    "    \n",
    "    #df = df.filter(regex='RS') # only use remote sensing variables   \n",
    "    df = df[variables]\n",
    "    \n",
    "    if model_var == 'ET':\n",
    "        df_var=t[model_var+'_EC']\n",
    "    else:\n",
    "        df_var=t[model_var+'_SOLO_EC'] # seperate out the variable we're modelling\n",
    "    \n",
    "    x = df.reset_index(drop=True)#.to_numpy()\n",
    "    y = df_var.reset_index(drop=True)#.to_numpy()\n",
    "    xx.append(x)\n",
    "    yy.append(y)\n",
    "\n",
    "x = pd.concat(xx)\n",
    "y = pd.concat(yy)\n",
    "\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify monotonic constraints for CO2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m_con= ['1' if col == 'CO2_RS' else '0' for col in x.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "\n",
    "http://rasbt.github.io/mlxtend/user_guide/feature_selection/SequentialFeatureSelector/#sequentialfeatureselector-the-popular-forward-and-backward-feature-selection-approaches-including-floating-variants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model robustness with time-series K-fold cross validation\n",
    "\n",
    "* Need to understand parameters of LightGBM better to define a sensible `param_grid`, and prevent overfitting\n",
    "* If you set boosting as RF then the lightgbm algorithm behaves as random forest. According to the documentation, to use RF you must use bagging_fraction and feature_fraction smaller than 1\n",
    "\n",
    "\n",
    "<img src=\"results/figs/cross_validation.png\" width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate five sets of train-test indices\n",
    "\n",
    "For each site, grab a sequential set of test samples (time-series-split methods), the remaining points (either side of test samples) go into training.  A single K-fold contains test and training samples from every site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sites = x['site'].unique()\n",
    "x['original_index'] = [i for i in range(0,len(x))]\n",
    "\n",
    "train_1=[]\n",
    "train_2=[]\n",
    "train_3=[]\n",
    "train_4=[]\n",
    "train_5=[]\n",
    "\n",
    "test_1=[]\n",
    "test_2=[]\n",
    "test_3=[]\n",
    "test_4=[]\n",
    "test_5=[]\n",
    "\n",
    "for site in sites:\n",
    "    df = x.loc[x['site'] == site]\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    i=1\n",
    "    for train, test in tscv.split(df):\n",
    "        all_indices=np.concatenate([train,test])\n",
    "        left_over = df.loc[~df.index.isin(all_indices)].index.values\n",
    "        train = np.concatenate([train, left_over])\n",
    "        if i==1:\n",
    "            train_1.append(df.iloc[train]['original_index'].values)\n",
    "            test_1.append(df.iloc[test]['original_index'].values)\n",
    "        if i==2:\n",
    "            train_2.append(df.iloc[train]['original_index'].values)\n",
    "            test_2.append(df.iloc[test]['original_index'].values)\n",
    "        if i==3:\n",
    "            train_3.append(df.iloc[train]['original_index'].values)\n",
    "            test_3.append(df.iloc[test]['original_index'].values)\n",
    "        if i==4:\n",
    "            train_4.append(df.iloc[train]['original_index'].values)\n",
    "            test_4.append(df.iloc[test]['original_index'].values)\n",
    "        if i==4:\n",
    "            train_5.append(df.iloc[train]['original_index'].values)\n",
    "            test_5.append(df.iloc[test]['original_index'].values)\n",
    "        i+=1\n",
    "\n",
    "train_1 = np.concatenate(train_1)\n",
    "train_2 = np.concatenate(train_2)\n",
    "train_3 = np.concatenate(train_3)\n",
    "train_4 = np.concatenate(train_4)\n",
    "train_5 = np.concatenate(train_5)\n",
    "\n",
    "test_1 = np.concatenate(test_1)\n",
    "test_2 = np.concatenate(test_2)\n",
    "test_3 = np.concatenate(test_3)\n",
    "test_4 = np.concatenate(test_4)\n",
    "test_5 = np.concatenate(test_5)\n",
    "\n",
    "train = [train_1, train_2, train_3, train_4, train_5]\n",
    "test = [test_1, test_2, test_3, test_4, test_5]\n",
    "\n",
    "#check there are no train indices in the test indices\n",
    "for i,j in zip(train, test):\n",
    "    assert (np.sum(np.isin(i,j)) == 0)\n",
    "\n",
    "#remove the columns we no longer need\n",
    "x = x.drop(['site', 'original_index'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write out predictor variables to text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textfile = open(\"/g/data/os22/chad_tmp/NEE_modelling/results/variables.txt\", \"w\")\n",
    "for element in x.columns:\n",
    "    textfile.write(element + \",\")\n",
    "textfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the parameter grid based on the results of random search \n",
    "# param_grid = {\n",
    "#     'num_leaves': [7, 14, 21, 28, 31, 50, 70],\n",
    "#     'boosting_type ': ['gbdt','dart'],\n",
    "#     'max_depth': [-1, 3, 5, 10, 20],\n",
    "#     'n_estimators': [100, 200, 300],\n",
    "# }\n",
    "\n",
    "# Create the parameter grid based on the results of random search \n",
    "param_grid = {\n",
    "    # 'num_leaves': [7,21,31,50],\n",
    "    #'boosting_type ': ['gbdt', 'dart'],\n",
    "    # 'max_depth': [-1, 5, 10, 20],\n",
    "    'n_estimators': [100,\n",
    "                     # 200,\n",
    "                     # 300\n",
    "                    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lists to store results of CV testing\n",
    "acc = []\n",
    "rmse=[]\n",
    "r2=[]\n",
    "\n",
    "i = 1\n",
    "for train_index, test_index in zip(train, test):\n",
    "    print(f\"Working on {i}/{len(train)} outer cv split\", end='\\r')\n",
    "    model = LGBMRegressor(random_state=1,\n",
    "                          n_jobs=ncpus,\n",
    "                          # monotone_constraints=m_con,\n",
    "                          # monotone_constraints_method='intermediate'\n",
    "                          )\n",
    "\n",
    "    # index training, testing\n",
    "    X_tr, X_tt = x.iloc[train_index, :], x.iloc[test_index, :]\n",
    "    y_tr, y_tt = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    # def index_gen(listTrain, listTest):\n",
    "    #     yield listTrain, listTest\n",
    "    # inner split on data within outer split\n",
    "    inner_cv = KFold(n_splits=2,\n",
    "                     shuffle=True,\n",
    "                     random_state=0)\n",
    "    \n",
    "    clf = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grid,\n",
    "        scoring='r2',\n",
    "        n_jobs=ncpus,\n",
    "        refit=True,\n",
    "        cv=inner_cv.split(X_tr, y_tr),\n",
    "    )\n",
    "\n",
    "    clf.fit(X_tr, y_tr)\n",
    "    # predict using the best model\n",
    "    best_model = clf.best_estimator_\n",
    "    pred = best_model.predict(X_tt)\n",
    "\n",
    "    # evaluate model w/ multiple metrics\n",
    "    # r2\n",
    "    r2_ = r2_score(y_tt, pred)\n",
    "    r2.append(r2_)\n",
    "    # Overall accuracy\n",
    "    ac = mean_absolute_error(y_tt, pred)\n",
    "    acc.append(ac)\n",
    "    # F1 scores\n",
    "    rmse_ = np.sqrt(mean_squared_error(y_tt, pred))\n",
    "    rmse.append(rmse_)\n",
    "    \n",
    "    #1:1 plots in a plot\n",
    "    fig,ax = plt.subplots(1,1, figsize=(6,6))\n",
    "    sb.scatterplot(x=y_tt,y=pred,color=\"#338844\", edgecolor=\"white\", s=50, lw=1, alpha=0.5, ax=ax)\n",
    "    sb.regplot(x=y_tt, y=pred, scatter=False, color='m', ax=ax)\n",
    "    sb.regplot(x=y_tt, y=y_tt, color='black', scatter=False, line_kws={'linestyle':'dashed'}, ax=ax);\n",
    "    \n",
    "    plt.xlabel('True')\n",
    "    plt.ylabel('Prediction');\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(\"/g/data/os22/chad_tmp/NEE_modelling/results/\"+str(i)+\"_\"+model_var+\"_lgbm.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean MAE % accuracy: \"+ str(round(np.mean(acc), 2)))\n",
    "print(\"Std dev of MAE % accuracy: \"+ str(round(np.std(acc), 2)))\n",
    "print('\\n')\n",
    "print(\"Mean RMSE: \"+ str(round(np.mean(rmse), 2)))\n",
    "print(\"Std dev RMSE: \"+ str(round(np.std(rmse), 2)))\n",
    "print('\\n')\n",
    "print(\"Mean r2: \"+ str(round(np.mean(r2), 2)))\n",
    "print(\"Std dev r2: \"+ str(round(np.std(r2), 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'num_leaves': [7, 14, 21, 28, 31, 50],\n",
    "    'min_data_in_leaf':[15, 20, 30],\n",
    "    #'boosting_type ': ['gbdt', 'dart'],\n",
    "    'max_depth': [3, 5, 10, 20],\n",
    "    'n_estimators': [100, 200, 300, 400],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate n_splits of train-test_split\n",
    "#rs = ShuffleSplit(n_splits=outer_cv_splits, test_size=test_size, random_state=1)\n",
    "\n",
    "#instatiate a gridsearchCV\n",
    "clf = GridSearchCV(LGBMRegressor(\n",
    "                                 # boosting_type='rf',\n",
    "                                 # bagging_freq=1, \n",
    "                                 # bagging_fraction=0.8\n",
    "                                ),\n",
    "                   param_grid,\n",
    "                   scoring='r2',\n",
    "                   verbose=0,\n",
    "                   cv=zip(train, test),\n",
    "                   n_jobs=ncpus)\n",
    "\n",
    "clf.fit(x, y, callbacks=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The most accurate combination of tested parameters is: \")\n",
    "pprint(clf.best_params_)\n",
    "print('\\n')\n",
    "print(\"The r2 score using these parameters is: \")\n",
    "print(round(clf.best_score_, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit on all data using best params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LGBMRegressor(**clf.best_params_,\n",
    "                     # monotone_constraints=m_con,\n",
    "                     # monotone_constraints_method='intermediate'\n",
    "                     )\n",
    "\n",
    "model.fit(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(model, '/g/data/os22/chad_tmp/NEE_modelling/results/models/'+model_name+'_'+model_var+'_LGBM_model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x)\n",
    "\n",
    "rmse = float(format(np.sqrt(mean_squared_error(y, y_pred)), '.3f'))\n",
    "print(\"RMSE:\", rmse);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.scatterplot(x=y,y=y_pred,color=\"#338844\", edgecolor=\"white\", s=50, lw=1, alpha=0.5)\n",
    "sb.regplot(x=y, y=y_pred, scatter=False, color='m')\n",
    "sb.regplot(x=y, y=y, color='black', scatter=False, line_kws={'linestyle':'dashed'});\n",
    "plt.xlabel('True')\n",
    "plt.ylabel('Prediction');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine feature importance\n",
    "\n",
    "SHAP (SHapley Additive exPlanations) is a game theoretic approach to explain the output of any machine learning model\n",
    "\n",
    "https://github.com/slundberg/shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explain the model's predictions using SHAP\n",
    "explainer = shap.Explainer(model)\n",
    "shap_values = explainer(x)\n",
    "\n",
    "# visualize the first prediction's explanation\n",
    "# shap.plots.waterfall(shap_values[0])\n",
    "# shap.plots.beeswarm(shap_values)\n",
    "shap.plots.bar(shap_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
