{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate and fit a ML model on the EC flux tower data \n",
    "\n",
    "Need to better consider categorical Landcover data. Currrently just passing it as integers in a single column. I can use one-hot-encoding but this may be a poor way of handling categorical variables in this case:\n",
    "* https://notebook.community/roaminsight/roamresearch/BlogPosts/Categorical_variables_in_tree_models/categorical_variables_post\n",
    "* https://notebook.community/marcotcr/lime/doc/notebooks/Tutorial_H2O_continuous_and_cat\n",
    "\n",
    "* I should consider using `h2o` which handles categorical data much better https://www.kaggle.com/code/nanomathias/h2o-distributed-random-forest-starter/script\n",
    "* R's randomforest algos do a better job too\n",
    "\n",
    "* `LightGBM` handles categorical variables well: https://lightgbm.readthedocs.io/en/latest/Features.html#optimal-split-for-categorical-features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "from joblib import dump\n",
    "import multiprocessing\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "from lightgbm import LGBMRegressor\n",
    "import lightgbm as lgbm\n",
    "import shap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV, ShuffleSplit, KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'AUS'\n",
    "model_var = 'GPP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncpus=multiprocessing.cpu_count()\n",
    "print('ncpus = '+str(ncpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = '/g/data/os22/chad_tmp/NEE_modelling/results/training_data/'\n",
    "sites = os.listdir('/g/data/os22/chad_tmp/NEE_modelling/results/training_data/')\n",
    "\n",
    "td = []\n",
    "for site in sites:\n",
    "    if '.csv' in site:\n",
    "        xx = pd.read_csv(base+site, index_col='time', parse_dates=True)\n",
    "        td.append(xx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = []\n",
    "yy = []\n",
    "for t in td:\n",
    "    #t = t.drop('soil_moisture_RS', axis=1) # get rid of soil moisture \n",
    "    t = t.dropna()  # remove NaNS\n",
    "    df = t.drop(['NEE_SOLO_EC','GPP_SOLO_EC','ER_SOLO_EC'], axis=1) # seperate carbon fluxes\n",
    "    df = df.filter(regex='RS') # only use remote sensing variables   \n",
    "    \n",
    "    # Write out predictior variables to text file\n",
    "    textfile = open(\"/g/data/os22/chad_tmp/NEE_modelling/results/variables.txt\", \"w\")\n",
    "    for element in df.columns:\n",
    "        textfile.write(element + \",\")\n",
    "    textfile.close()\n",
    "    \n",
    "    df_var=t[model_var+'_SOLO_EC'] # seperate out the variable we're modelling\n",
    "\n",
    "    x = df.reset_index(drop=True)#.to_numpy()\n",
    "    y = df_var.reset_index(drop=True)#.to_numpy()\n",
    "    xx.append(x)\n",
    "    yy.append(y)\n",
    "\n",
    "x = pd.concat(xx)\n",
    "y = pd.concat(yy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reclassify LC numbers to string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x['PFT_RS'] = x['PFT_RS'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x['PFT_RS'] = np.where(x['PFT_RS']==10.0, 'Trees', df_xx['PFT_RS'])\n",
    "# x['PFT_RS'] = np.where(x['PFT_RS']=='20.0', 'Shrubs', df_xx['PFT_RS'])\n",
    "# x['PFT_RS'] = np.where(x['PFT_RS']=='30.0', 'Grass', df_xx['PFT_RS'])\n",
    "# x['PFT_RS'] = np.where(x['PFT_RS']=='40.0', 'Crops', df_xx['PFT_RS'])\n",
    "\n",
    "# x = df_xx.to_numpy()\n",
    "# y = df_yy.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model robustness with nested K-fold cross validation\n",
    "\n",
    "* Need to understand parameters of LightGBM better to define a sensible `param_grid`, and prevent overfitting\n",
    "* If you set boosting as RF then the lightgbm algorithm behaves as random forest. According to the documentation, to use RF you must use bagging_fraction and feature_fraction smaller than 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_cv_splits = 5\n",
    "\n",
    "outer_cv_splits = 5\n",
    "\n",
    "test_size = 0.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the parameter grid based on the results of random search \n",
    "# param_grid = {\n",
    "#     'num_leaves': [7, 14, 21, 28, 31, 50, 70],\n",
    "#     'boosting_type ': ['gbdt','dart'],\n",
    "#     'max_depth': [-1, 3, 5, 10, 20],\n",
    "#     'n_estimators': [100, 200, 300],\n",
    "# }\n",
    "\n",
    "# Create the parameter grid based on the results of random search \n",
    "param_grid = {\n",
    "    'num_leaves': [7, 14, 21, 28, 31, 50, 70],\n",
    "    'boosting_type ': ['gbdt', 'dart'],\n",
    "    'max_depth': [-1, 3, 5, 10, 20],\n",
    "    'n_estimators': [100, 200, 300, 400],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outer_cv = KFold(n_splits=outer_cv_splits, shuffle=True,\n",
    "                        random_state=0)\n",
    "\n",
    "# lists to store results of CV testing\n",
    "acc = []\n",
    "rmse=[]\n",
    "r2=[]\n",
    "i = 1\n",
    "for train_index, test_index in outer_cv.split(x, y):\n",
    "    print(f\"Working on {i}/5 outer cv split\", end='\\r')\n",
    "    model = LGBMRegressor(random_state=1, n_jobs=ncpus)\n",
    "    \n",
    "    # index training, testing, and coordinate data\n",
    "    X_tr, X_tt = x.iloc[train_index, :], x.iloc[test_index, :]\n",
    "    y_tr, y_tt = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    # inner split on data within outer split\n",
    "    inner_cv = KFold(n_splits=inner_cv_splits,\n",
    "                     shuffle=True,\n",
    "                     random_state=0)\n",
    "    \n",
    "    clf = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grid,\n",
    "        scoring='r2',\n",
    "        n_jobs=ncpus,\n",
    "        refit=True,\n",
    "        cv=inner_cv.split(X_tr, y_tr),\n",
    "    )\n",
    "\n",
    "    clf.fit(X_tr, y_tr,  categorical_feature=['PFT_RS'])\n",
    "    # predict using the best model\n",
    "    best_model = clf.best_estimator_\n",
    "    pred = best_model.predict(X_tt)\n",
    "\n",
    "    # evaluate model w/ multiple metrics\n",
    "    # r2\n",
    "    r2_ = r2_score(y_tt, pred)\n",
    "    r2.append(r2_)\n",
    "    # Overall accuracy\n",
    "    ac = mean_absolute_error(y_tt, pred)\n",
    "    acc.append(ac)\n",
    "    # F1 scores\n",
    "    rmse_ = np.sqrt(mean_squared_error(y_tt, pred))\n",
    "    rmse.append(rmse_)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean MAE % accuracy: \"+ str(round(np.mean(acc), 2)))\n",
    "print(\"Std dev of MAE % accuracy: \"+ str(round(np.std(acc), 2)))\n",
    "print('\\n')\n",
    "print(\"Mean RMSE: \"+ str(round(np.mean(rmse), 2)))\n",
    "print(\"Std dev RMSE: \"+ str(round(np.std(rmse), 2)))\n",
    "print('\\n')\n",
    "print(\"Mean r2: \"+ str(round(np.mean(r2), 2)))\n",
    "print(\"Std dev r2: \"+ str(round(np.std(r2), 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate n_splits of train-test_split\n",
    "rs = ShuffleSplit(n_splits=outer_cv_splits, test_size=test_size, random_state=1)\n",
    "\n",
    "#instatiate a gridsearchCV\n",
    "clf = GridSearchCV(LGBMRegressor(verbose_eval=-1,                          # boosting_type='rf',\n",
    "                                 # bagging_freq=1, \n",
    "                                 # bagging_fraction=0.8\n",
    "                                ),\n",
    "                   param_grid,\n",
    "                   scoring='r2',\n",
    "                   verbose=1,\n",
    "                   cv=rs.split(x, y),\n",
    "                   n_jobs=ncpus)\n",
    "\n",
    "\n",
    "clf.fit(x, y, categorical_feature=['PFT_RS'], callbacks=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The most accurate combination of tested parameters is: \")\n",
    "pprint(clf.best_params_)\n",
    "print('\\n')\n",
    "print(\"The r2 score using these parameters is: \")\n",
    "print(round(clf.best_score_, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit on all data using best params\n",
    "\n",
    "Feature importance using SHAP https://github.com/slundberg/shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LGBMRegressor(**clf.best_params_)\n",
    "model.fit(x, y, categorical_feature=['PFT_RS'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine feature importance\n",
    "\n",
    "SHAP (SHapley Additive exPlanations) is a game theoretic approach to explain the output of any machine learning model\n",
    "\n",
    "https://github.com/slundberg/shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explain the model's predictions using SHAP\n",
    "explainer = shap.Explainer(model)\n",
    "shap_values = explainer(x)\n",
    "\n",
    "# visualize the first prediction's explanation\n",
    "# shap.plots.waterfall(shap_values[0])\n",
    "# shap.plots.beeswarm(shap_values)\n",
    "shap.plots.bar(shap_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(model, '/g/data/os22/chad_tmp/NEE_modelling/results/models/'+model_name+'_'+model_var+'_LGBM_model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x)\n",
    "\n",
    "rmse = float(format(np.sqrt(mean_squared_error(y, y_pred)), '.3f'))\n",
    "print(\"RMSE:\", rmse);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.scatterplot(x=y,y=y_pred,color=\"#338844\", edgecolor=\"white\", s=50, lw=1, alpha=0.5)\n",
    "sb.regplot(x=y, y=y_pred, scatter=False, color='m')\n",
    "sb.regplot(x=y, y=y, color='black', scatter=False, line_kws={'linestyle':'dashed'});\n",
    "plt.xlabel('True')\n",
    "plt.ylabel('Prediction');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
