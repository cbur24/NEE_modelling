{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling uncertainty in ML predictions\n",
    "\n",
    "Quantile regression in this context doesn't work as an estimate of Aus-wide uncertainty.\n",
    "\n",
    "Instead, we will attempt to model the uncertainty that comes from the training data and optimization algorithm.  To do this, we will generate 30 models. For each iteration, two site's entire time-series will be removed from the training data and both a  LGBM model will be fit on the remaining data.  This will result in 29 models that later we can use to make predictions with. The envelope of our predictions will inform our uncertainity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import seaborn as sb\n",
    "import pandas as pd\n",
    "from joblib import dump\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "from scipy.stats import gaussian_kde\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import lightgbm as lgbm\n",
    "from sklearn.model_selection import TimeSeriesSplit, RandomizedSearchCV, KFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_var = 'NEE'\n",
    "suffix = '20230109'\n",
    "features_list = '/g/data/os22/chad_tmp/NEE_modelling/results/variables_'+suffix+'.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = '/g/data/os22/chad_tmp/NEE_modelling/results/training_data/'\n",
    "sites = os.listdir('/g/data/os22/chad_tmp/NEE_modelling/results/training_data/')\n",
    "\n",
    "td = []\n",
    "for site in sites:\n",
    "    if '.csv' in site:\n",
    "        xx = pd.read_csv(base+site, index_col='time', parse_dates=True)\n",
    "        xx['site'] = site[0:5]\n",
    "        td.append(xx)\n",
    "\n",
    "ts = pd.concat(td).dropna() #we'll use this later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kNDVI_RS',\n",
       " 'kNDVI_anom_RS',\n",
       " 'LST_RS',\n",
       " 'tree_cover_RS',\n",
       " 'nontree_cover_RS',\n",
       " 'nonveg_cover_RS',\n",
       " 'LST-Tair_RS',\n",
       " 'TWI_RS',\n",
       " 'NDWI_RS',\n",
       " 'rain_RS',\n",
       " 'rain_cml3_anom_RS',\n",
       " 'rain_cml6_anom_RS',\n",
       " 'rain_cml12_anom_RS',\n",
       " 'srad_RS',\n",
       " 'srad_anom_RS',\n",
       " 'vpd_RS',\n",
       " 'tavg_RS',\n",
       " 'tavg_anom_RS',\n",
       " 'SOC_RS',\n",
       " 'C4_percent_RS',\n",
       " 'site']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vars = list(pd.read_csv(features_list))[0:-1]\n",
    "train_vars=[i[:-3] for i in train_vars]\n",
    "train_vars = [i+'_RS' for i in train_vars]\n",
    "train_vars.append('site')\n",
    "train_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2735, 21)\n"
     ]
    }
   ],
   "source": [
    "xx = []\n",
    "yy = []\n",
    "\n",
    "for t in td:\n",
    "    # t = t.drop(['Fluxcom_RS-Meteo_NEE', 'Fluxcom_RS_NEE', 'ThisStudy_NEE', 'Cable_NEE',\n",
    "    #    'Fluxcom_RS_GPP', 'Fluxcom_RS-meteo_GPP', 'ThisStudy_GPP', 'Cable_GPP',\n",
    "    #    'MODIS_GPP', 'GOSIF_GPP'], axis=1)  \n",
    "    \n",
    "    t = t.dropna()  # remove NaNS\n",
    "    df = t.drop(['NEE_SOLO_EC','GPP_SOLO_EC','ER_SOLO_EC'], axis=1) # seperate carbon fluxes\n",
    "    \n",
    "    #df = df.filter(regex='RS') # only use remote sensing variables   \n",
    "    df = df[train_vars]\n",
    "    \n",
    "    if model_var == 'ET':\n",
    "        df_var=t[model_var+'_EC']\n",
    "    else:\n",
    "        df_var=t[[model_var+'_SOLO_EC', 'site']] # seperate out the variable we're modelling\n",
    "    \n",
    "    x = df.reset_index(drop=True)#.to_numpy()\n",
    "    y = df_var.reset_index(drop=True)#.to_numpy()\n",
    "    xx.append(x)\n",
    "    yy.append(y)\n",
    "\n",
    "x = pd.concat(xx)\n",
    "y = pd.concat(yy)\n",
    "\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate 30 iterations of models\n",
    "\n",
    "For two regression methods (RF and LigtGBM), we remove two randomly selected sites from the training data\n",
    "\n",
    "Then, we do the per site TSCV: For each site, grab a sequential set of test samples (time-series-split methods), the remaining points (either side of test samples) go into training.  A single K-fold contains test and training samples from every site.\n",
    "\n",
    "A model is built and saved that is trained of 15 iterations of site removal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 001/015\n",
      "  Model: _lgbm_\n",
      "    1/5 outer cv split\n",
      "    2/5 outer cv split\n",
      "    3/5 outer cv split\n",
      "    4/5 outer cv split\n",
      "    5/5 outer cv split\n",
      "  Model: _rf_\n",
      "    1/5 outer cv split\n",
      "    2/5 outer cv split\n",
      "    3/5 outer cv split\n",
      "    4/5 outer cv split\n",
      "    5/5 outer cv split\n",
      " 002/015\n",
      "  Model: _lgbm_\n",
      "    1/5 outer cv split\n",
      "    2/5 outer cv split\n",
      "    3/5 outer cv split\n",
      "    4/5 outer cv split\n",
      "    5/5 outer cv split\n",
      "  Model: _rf_\n",
      "    1/5 outer cv split\n",
      "    2/5 outer cv split\n",
      "    3/5 outer cv split\n",
      "    4/5 outer cv split\n",
      "    5/5 outer cv split\n",
      " 003/015\n",
      "  Model: _lgbm_\n",
      "    1/5 outer cv split\n",
      "    2/5 outer cv split\n",
      "    3/5 outer cv split\n",
      "    4/5 outer cv split\n",
      "    5/5 outer cv split\n",
      "  Model: _rf_\n",
      "    1/5 outer cv split\n",
      "    2/5 outer cv split\n",
      "    3/5 outer cv split\n",
      "    4/5 outer cv split\n",
      "    5/5 outer cv split\n",
      " 004/015\n",
      "  Model: _lgbm_\n",
      "    1/5 outer cv split\n",
      "    2/5 outer cv split\n",
      "    3/5 outer cv split\n",
      "    4/5 outer cv split\n",
      "    5/5 outer cv split\n",
      "  Model: _rf_\n",
      "    1/5 outer cv split\n",
      "    2/5 outer cv split\n",
      "    3/5 outer cv split\n",
      "    4/5 outer cv split\n",
      "    5/5 outer cv split\n",
      " 005/015\n",
      "  Model: _lgbm_\n",
      "    1/5 outer cv split\n",
      "    2/5 outer cv split\n",
      "    3/5 outer cv split\n",
      "    4/5 outer cv split\n",
      "    5/5 outer cv split\n",
      "  Model: _rf_\n",
      "    1/5 outer cv split\n",
      "    2/5 outer cv split\n",
      "    3/5 outer cv split\n",
      "    4/5 outer cv split\n",
      "    5/5 outer cv split\n",
      " 006/015\n",
      "  Model: _lgbm_\n",
      "    1/5 outer cv split\n",
      "    2/5 outer cv split\n",
      "    3/5 outer cv split\n",
      "    4/5 outer cv split\n",
      "    5/5 outer cv split\n",
      "  Model: _rf_\n",
      "    1/5 outer cv split\n",
      "    2/5 outer cv split\n",
      "    3/5 outer cv split\n",
      "    4/5 outer cv split\n",
      "    5/5 outer cv split\n",
      " 007/015\n",
      "  Model: _lgbm_\n",
      "    1/5 outer cv split\n",
      "    2/5 outer cv split\n",
      "    3/5 outer cv split\n",
      "    4/5 outer cv split\n",
      "    5/5 outer cv split\n",
      "  Model: _rf_\n",
      "    1/5 outer cv split\n",
      "    2/5 outer cv split\n",
      "    3/5 outer cv split\n",
      "    4/5 outer cv split\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "i=0\n",
    "for m in range(1,16): # 15 iterations for each model\n",
    "    print(\" {:03}/{:03}\\r\".format(m, len(range(1,16))))\n",
    "          \n",
    "    #randomly select two sites to remove from dataset\n",
    "    subset=np.random.choice(x['site'].unique(), size=2)\n",
    "    x_n = x[~x.site.isin(subset)]\n",
    "    y_n = y[~y.site.isin(subset)]\n",
    "\n",
    "    sites_n = x_n['site'].unique()\n",
    "    x_n['original_index'] = [i for i in range(0,len(x_n))]\n",
    "    \n",
    "    #build TSCV splits across all remaining sites\n",
    "    train_1=[]\n",
    "    train_2=[]\n",
    "    train_3=[]\n",
    "    train_4=[]\n",
    "    train_5=[]\n",
    "\n",
    "    test_1=[]\n",
    "    test_2=[]\n",
    "    test_3=[]\n",
    "    test_4=[]\n",
    "    test_5=[]\n",
    "\n",
    "    for site_n in sites_n:\n",
    "        df = x_n.loc[x_n['site'] == site_n]\n",
    "        tscv = TimeSeriesSplit(n_splits=5)\n",
    "        i=1\n",
    "        for train, test in tscv.split(df):\n",
    "            all_indices=np.concatenate([train,test])\n",
    "            left_over = df.loc[~df.index.isin(all_indices)].index.values\n",
    "            train = np.concatenate([train, left_over])\n",
    "            if i==1:\n",
    "                train_1.append(df.iloc[train]['original_index'].values)\n",
    "                test_1.append(df.iloc[test]['original_index'].values)\n",
    "            if i==2:\n",
    "                train_2.append(df.iloc[train]['original_index'].values)\n",
    "                test_2.append(df.iloc[test]['original_index'].values)\n",
    "            if i==3:\n",
    "                train_3.append(df.iloc[train]['original_index'].values)\n",
    "                test_3.append(df.iloc[test]['original_index'].values)\n",
    "            if i==4:\n",
    "                train_4.append(df.iloc[train]['original_index'].values)\n",
    "                test_4.append(df.iloc[test]['original_index'].values)\n",
    "            if i==4:\n",
    "                train_5.append(df.iloc[train]['original_index'].values)\n",
    "                test_5.append(df.iloc[test]['original_index'].values)\n",
    "            i+=1\n",
    "\n",
    "    train_1 = np.concatenate(train_1)\n",
    "    train_2 = np.concatenate(train_2)\n",
    "    train_3 = np.concatenate(train_3)\n",
    "    train_4 = np.concatenate(train_4)\n",
    "    train_5 = np.concatenate(train_5)\n",
    "\n",
    "    test_1 = np.concatenate(test_1)\n",
    "    test_2 = np.concatenate(test_2)\n",
    "    test_3 = np.concatenate(test_3)\n",
    "    test_4 = np.concatenate(test_4)\n",
    "    test_5 = np.concatenate(test_5)\n",
    "\n",
    "    train = [train_1, train_2, train_3, train_4, train_5]\n",
    "    test = [test_1, test_2, test_3, test_4, test_5]\n",
    "\n",
    "    #check there are no train indices in the test indices\n",
    "    for i,j in zip(train, test):\n",
    "        assert (np.sum(np.isin(i,j)) == 0)\n",
    "\n",
    "    #remove the columns we no longer need\n",
    "    x_n = x_n.drop(['site', 'original_index'], axis=1)\n",
    "    y_n = y_n.drop('site', axis=1)\n",
    "\n",
    "    #loop through the two regression methods\n",
    "    for regressor in [LGBMRegressor,\n",
    "                      RandomForestRegressor]:\n",
    "\n",
    "        if isinstance(regressor(), lgbm.sklearn.LGBMRegressor):\n",
    "            m_name='_lgbm_'\n",
    "\n",
    "            param_grid = {\n",
    "                'num_leaves': stats.randint(5,40),\n",
    "                'min_child_samples':stats.randint(10,30),\n",
    "                'boosting_type': ['gbdt', 'dart'],\n",
    "                'max_depth': stats.randint(5,25),\n",
    "                'n_estimators': [300, 400, 500],\n",
    "            }\n",
    "\n",
    "        else:\n",
    "            m_name='_rf_'\n",
    "\n",
    "            param_grid = {\n",
    "                'max_depth': stats.randint(5,35),\n",
    "                'max_features': ['log2', None, \"sqrt\"],\n",
    "                'n_estimators': [200,300,400,500]}\n",
    "\n",
    "        print('  Model:', m_name)\n",
    "        \n",
    "        #-----Nested CV to test accuracy-----------------------------------------------\n",
    "        # results are saved as a .csv \n",
    "        j=1\n",
    "        for train_index, test_index in zip(train, test):\n",
    "            print(f\"    {j}/{len(train)} outer cv split\")\n",
    "            \n",
    "            if os.path.exists(\"/g/data/os22/chad_tmp/NEE_modelling/results/cross_validation/\"+model_var+\"_ensemble/CV_\"+str(j)+\"_\"+model_var+m_name+str(m)+\".csv\"):\n",
    "                j+=1\n",
    "                continue\n",
    "\n",
    "            # index training, testing\n",
    "            X_tr, X_tt = x_n.iloc[train_index, :], x_n.iloc[test_index, :]\n",
    "            y_tr, y_tt = y_n.iloc[train_index], y_n.iloc[test_index]\n",
    "\n",
    "            #simple random split on inner fold\n",
    "            inner_cv = KFold(n_splits=5,\n",
    "                             shuffle=True,\n",
    "                             random_state=0)\n",
    "\n",
    "            if m_name=='_rf_':\n",
    "                model = regressor(random_state=1, verbose=0, n_jobs=-1)\n",
    "                clf = RandomizedSearchCV(\n",
    "                               model,\n",
    "                               param_grid,\n",
    "                               verbose=0,\n",
    "                               n_iter=250,\n",
    "                               n_jobs=-1,\n",
    "                               cv=inner_cv.split(X_tr, y_tr),\n",
    "                              )\n",
    "\n",
    "                clf.fit(X_tr, y_tr.values.ravel())\n",
    "\n",
    "            else:\n",
    "                model = regressor(random_state=1, verbose=-1, n_jobs=-1)\n",
    "                clf = RandomizedSearchCV(\n",
    "                               model,\n",
    "                               param_grid,\n",
    "                               verbose=-1,\n",
    "                               n_iter=250,\n",
    "                               n_jobs=-1,\n",
    "                               cv=inner_cv.split(X_tr, y_tr),\n",
    "                              )\n",
    "                clf.fit(X_tr, y_tr, callbacks=None)\n",
    "\n",
    "\n",
    "            # predict using the best model\n",
    "            best_model = clf.best_estimator_\n",
    "            pred = best_model.predict(X_tt)\n",
    "            dff = pd.DataFrame({'Test':y_tt.values.squeeze(), 'Pred':pred}).reset_index(drop=True)\n",
    "            dff.to_csv(\"/g/data/os22/chad_tmp/NEE_modelling/results/cross_validation/\"+model_var+\"_ensemble/CV_\"+str(j)+\"_\"+model_var+m_name+str(m)+\".csv\")\n",
    "\n",
    "            j+=1\n",
    "        #-----End of Nested CV ---------------------------------------------------\n",
    "        \n",
    "        #Now conduct a hyperparameter test on all the data\n",
    "        # (minus the two removed sites) and fit a model\n",
    "#         clf = RandomizedSearchCV(\n",
    "#                regressor(random_state=0, n_jobs=-1),\n",
    "#                param_grid,\n",
    "#                verbose=0,\n",
    "#                n_iter=500,\n",
    "#                n_jobs=-1,\n",
    "#                cv=zip(train, test)\n",
    "#               )\n",
    "    \n",
    "#         #fit model and save\n",
    "#         model = regressor(**clf.best_params_)\n",
    "        \n",
    "#         if m_name=='_rf_':\n",
    "#             model.fit(x_n, y_n.values.ravel())\n",
    "        \n",
    "#         else:\n",
    "#             model.fit(x_n, y_n)\n",
    "\n",
    "        #dump(model, '/g/data/os22/chad_tmp/NEE_modelling/results/models_uncertainty/'+model_var+'/'+m_name+str(m)+'.joblib')\n",
    "\n",
    "    i+=1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create 1:1 plots and statistics\n",
    "\n",
    "Here we loop through all the cross-validation datasets we've output in the previous step.  For each of the CV splits (1 through to 5, remember the CV splits are the same for each model iteration) we will take the median of all the model estimates (30 in total) for each of the cross validation data points. This means we end up with ~2000 cross-validation points to plot, where each point represents the median of all 30 model estimates.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base='/g/data/os22/chad_tmp/NEE_modelling/results/cross_validation/'+model_var+'_ensemble/'\n",
    "\n",
    "df_list=[]\n",
    "\n",
    "#loop through the CV splots\n",
    "for j in ['CV_'+str(i) for i in range(1,6)]:\n",
    "    \n",
    "    #get the list of cvs corresponding with a given CV split\n",
    "    csvs = [i for i in os.listdir(base) if j in i]\n",
    "    \n",
    "    dffs=[]\n",
    "    #loop through the csvs and load the data\n",
    "    for i in csvs:\n",
    "        df = pd.read_csv(base+i, usecols=['Test', 'Pred'])\n",
    "        #assign 'name' of the model iteration to each test/pred column\n",
    "        df = df.rename({'Test': 'Test_'+i[-9:-4], 'Pred': 'Pred_'+i[-9:-4]}, axis=1)\n",
    "        dffs.append(df)\n",
    "    \n",
    "    #concantenate all the data  for the given CV split\n",
    "    cross_df = pd.concat(dffs, axis=1, join=\"inner\")\n",
    "    cross_df = cross_df.sort_index(axis=1) #sort the columns\n",
    "    #find the median of the model estimates for each point\n",
    "    cross_df['Pred_mean'] = cross_df[cross_df.columns[0:30]].median(axis=1)\n",
    "    cross_df['Test_mean'] = cross_df[cross_df.columns[30:]].median(axis=1)\n",
    "    \n",
    "    #clean up\n",
    "    cross_df = cross_df[['Pred_mean', 'Test_mean']]\n",
    "    cross_df = cross_df.rename({'Pred_mean':'Pred', 'Test_mean':'Test'}, axis=1)\n",
    "    df_list.append(cross_df)\n",
    "\n",
    "#join the summarised data\n",
    "cross_df = pd.concat(df_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs,pred = cross_df['Test'].values, cross_df['Pred'].values\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(obs,pred)\n",
    "r2 = r_value**2\n",
    "ac = mean_absolute_error(obs, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1, figsize=(7,6))\n",
    "\n",
    "xy = np.vstack([cross_df['Test'],cross_df['Pred']])\n",
    "z = gaussian_kde(xy)(xy)\n",
    "\n",
    "sb.scatterplot(data=cross_df, x='Test',y='Pred',c=z, s=50, lw=1, alpha=0.5, ax=ax)\n",
    "sb.regplot(data=cross_df, x='Test',y='Pred', scatter=False, color='red', ax=ax)\n",
    "sb.regplot(data=cross_df, x='Test',y='Test', color='black', scatter=False, line_kws={'linestyle':'dashed'}, ax=ax);\n",
    "\n",
    "plt.xlabel('Observation '+ model_var + ' (gC m\\N{SUPERSCRIPT TWO} m⁻¹)', fontsize=20)\n",
    "plt.ylabel('Prediction ' + model_var+ ' (gC m\\N{SUPERSCRIPT TWO} m⁻¹)', fontsize=20)\n",
    "ax.text(.05, .95, 'r\\N{SUPERSCRIPT TWO}={:.2f}'.format(np.mean(r2)),\n",
    "            transform=ax.transAxes, fontsize=20)\n",
    "ax.text(.05, .9, 'MAE={:.3g}'.format(np.mean(ac)),\n",
    "            transform=ax.transAxes, fontsize=20)\n",
    "ax.tick_params(axis='x', labelsize=20)\n",
    "ax.tick_params(axis='y', labelsize=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# fig.savefig(\"/g/data/os22/chad_tmp/NEE_modelling/results/cross_validation/cross_val_\"+model_var+\"_MEDIAN_\"+suffix+\".png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
